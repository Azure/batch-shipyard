{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V - Automatic Model Selection from Parametric Sweep using Task Dependencies\n",
    "In this notebook we will be taking the example from the [Parametric Sweep](04_Parameter_Sweep.ipynb) notebook and automating the entire chain using task dependencies in a single Azure Batch job.\n",
    "\n",
    "* [Setup](#section1)\n",
    "* [Configure job](#section2)\n",
    "* [Submit job](#section3)\n",
    "* [Download best model](#section4)\n",
    "* [Delete job](#section5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a simple alias for Batch Shipyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%alias shipyard SHIPYARD_CONFIGDIR=config python $HOME/batch-shipyard/shipyard.py %l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that everything is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shipyard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the account information we saved earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_json(filename):\n",
    "    with open(filename, 'r') as infile:\n",
    "        return json.load(infile)\n",
    "    \n",
    "account_info = read_json('account_information.json')\n",
    "\n",
    "storage_account_key = account_info['storage_account_key']\n",
    "storage_account_name = account_info['storage_account_name']\n",
    "IMAGE_NAME = account_info['IMAGE_NAME']\n",
    "STORAGE_ALIAS = account_info['STORAGE_ALIAS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous job we ran on a single node we will be running the job on GPU enabled nodes. We will be repeating the parametric search we did in the previous notebook but this time instead of downloading all the results and evaluating the model performance we will get a final task to do that for us using task dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def write_json_to_file(json_dict, filename):\n",
    "    \"\"\" Simple function to write JSON dictionaries to files\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as outfile:\n",
    "        json.dump(json_dict, outfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the `jobs.json` configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "JOB_ID = 'cntk-ps-as-job'\n",
    "\n",
    "jobs = {\n",
    "    \"job_specifications\": [\n",
    "        {\n",
    "            \"id\": JOB_ID,\n",
    "            \"tasks\": [\n",
    "                        {\n",
    "                            \"image\": IMAGE_NAME,\n",
    "                            \"task_factory\": {\n",
    "                                \"parametric_sweep\": {\n",
    "                                    \"product\": [\n",
    "                                        {#num_convolution_layers\n",
    "                                            \"start\": 2,\n",
    "                                            \"stop\": 4,\n",
    "                                            \"step\": 1\n",
    "                                        },\n",
    "                                        {#minibatch_size\n",
    "                                            \"start\": 32,\n",
    "                                            \"stop\": 96,\n",
    "                                            \"step\": 32\n",
    "                                        }\n",
    "                                    ]\n",
    "                                }\n",
    "                            },\n",
    "                            \"command\": \"bash -c \\\"source /cntk/activate-cntk; python -u ConvNet_CIFAR10.py --datadir $AZ_BATCH_NODE_SHARED_DIR/data --num_convolution_layers {0} --minibatch_size {1} --max_epochs 30\\\"\",\n",
    "                            \"remove_container_after_exit\": True,\n",
    "                            \"gpu\": True,\n",
    "                            \"resource_files\": [\n",
    "                                    {\n",
    "                                        \"file_path\": \"ConvNet_CIFAR10.py\",\n",
    "                                        \"blob_source\": \"https://batchshipyardexamples.blob.core.windows.net/code/ConvNet_CIFAR10.py\",\n",
    "                                        \"file_mode\":'0777'\n",
    "                                    }\n",
    "                            ],\n",
    "                            \"output_data\": {\n",
    "                                \"azure_storage\": [\n",
    "                                    {\n",
    "                                        \"storage_account_settings\": STORAGE_ALIAS,\n",
    "                                        \"container\": \"output\",\n",
    "                                        \"source\": \"$AZ_BATCH_TASK_DIR/wd/Models\"\n",
    "                                    },\n",
    "                                ]\n",
    "                            },\n",
    "                        }\n",
    "            ]  \n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "num_parameter_sweep_tasks = 4\n",
    "print('number of tasks for parametric sweep {}: {}'.format(JOB_ID, num_parameter_sweep_tasks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create the Python program to run that performs the best model selection. Note that this code is nearly similar to the code for selecting the best model locally in the [Parameter sweep notebook](04_Parameter_Sweep.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile autoselect.py\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def read_json(filename):\n",
    "    with open(filename, 'r') as infile:\n",
    "        return json.load(infile)\n",
    "\n",
    "def scandir(basedir):\n",
    "    for root, dirs, files in os.walk(basedir):\n",
    "        for f in files:\n",
    "            yield os.path.join(root, f) \n",
    "\n",
    "MODELS_DIR = os.path.join('wd', 'Models')\n",
    "            \n",
    "results_dict = {}\n",
    "for model in scandir(MODELS_DIR):\n",
    "    if not model.endswith('.json'):\n",
    "        continue\n",
    "    key = model.split(os.sep)[2]  # due to MODELS_DIR path change\n",
    "    results_dict[key] = read_json(model)\n",
    "\n",
    "# use items() instead of iteritems() as this will be run in python3\n",
    "tuple_min_error = min(results_dict.items(), key=lambda x: x[1]['test_metric'])\n",
    "configuration_with_min_error = tuple_min_error[0]\n",
    "print('task with smallest error: {} ({})'.format(configuration_with_min_error, tuple_min_error[1]['test_metric']))\n",
    "\n",
    "# copy best model to wd\n",
    "MODEL_NAME = 'ConvNet_CIFAR10_model.dnn'\n",
    "shutil.copy(os.path.join(MODELS_DIR, configuration_with_min_error, MODEL_NAME), '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to prepare the file to be uploaded to the Azure Storage account to be referenced in the task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "INPUT_CONTAINER = 'input-autoselect'\n",
    "OUTPUT_CONTAINER = 'output-autoselect'\n",
    "UPLOAD_DIR = 'autoselect_upload'\n",
    "\n",
    "!rm -rf $UPLOAD_DIR\n",
    "!mkdir -p $UPLOAD_DIR\n",
    "!mv autoselect.py $UPLOAD_DIR\n",
    "!ls -alF $UPLOAD_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alias `blobxfer` and upload it to `INPUT_CONTAINER`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%alias blobxfer python -m blobxfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blobxfer $storage_account_name $INPUT_CONTAINER $UPLOAD_DIR --upload --storageaccountkey $storage_account_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll append the `auto-model-selection` task which depends on the prior training tasks. The important properties here are `depends_on_range` which specifies a range of task ids the `auto-model-selection` task depends on. Additionally, this task requires data from the prior run task which is specified in `input_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_input_data_spec(job_id, task_id):\n",
    "    return {\n",
    "        \"job_id\": job_id,\n",
    "        \"task_id\": task_id,\n",
    "        \"include\": [\"wd/Models/*_{}_{}/*\".format(task_id, job_id)]\n",
    "    }\n",
    "\n",
    "input_data = []\n",
    "for x in range(0, num_parameter_sweep_tasks):\n",
    "    input_data.append(generate_input_data_spec(JOB_ID, '{}'.format(x)))\n",
    "\n",
    "model_selection_task = {\n",
    "    \"id\": \"auto-model-selection\",\n",
    "    \"command\": 'bash -c \"source /cntk/activate-cntk; python -u autoselect.py\"',\n",
    "    \"depends_on_range\": [0, num_parameter_sweep_tasks - 1],\n",
    "    \"image\": IMAGE_NAME,\n",
    "    \"remove_container_after_exit\": True,\n",
    "    \"input_data\": {\n",
    "        \"azure_batch\": input_data,\n",
    "        \"azure_storage\": [\n",
    "            {\n",
    "                \"storage_account_settings\": STORAGE_ALIAS,\n",
    "                \"container\": INPUT_CONTAINER\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"output_data\": {\n",
    "        \"azure_storage\": [\n",
    "            {\n",
    "                \"storage_account_settings\": STORAGE_ALIAS,\n",
    "                \"container\": OUTPUT_CONTAINER,\n",
    "                \"include\": [\"*wd/ConvNet_CIFAR10_model.dnn\"],\n",
    "                \"blobxfer_extra_options\": \"--delete --strip-components 2\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# append auto-model-selection task to jobs\n",
    "jobs['job_specifications'][0]['tasks'].append(model_selection_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "write_json_to_file(jobs, os.path.join('config', 'jobs.json'))\n",
    "print(json.dumps(jobs, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit job\n",
    "Check that everything is ok with our pool before we submit our jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shipyard pool list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have confirmed everything is working we can execute our job using the command below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "shipyard jobs add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the command below we can check the status of our jobs. Once all jobs have an exit code we can continue. You can also view the **heatmap** of this pool on [Azure Portal](https://portal.azure.com) to monitor the progress of this job on the compute nodes under your Batch account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "shipyard jobs listtasks --jobid $JOB_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download best model\n",
    "The best performing model from the parametric sweep job should now be saved to our `OUTPUT_CONTAINER` container by the `auto-model-selection` task. Let's save this model in `MODELS_DIR`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MODELS_DIR = 'auto-selected-model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the best performing model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blobxfer $storage_account_name $OUTPUT_CONTAINER $MODELS_DIR --remoteresource . --download --storageaccountkey $storage_account_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model file (`ConvNet_CIFAR10_model.dnn`) is now ready for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!ls -alF $MODELS_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To delete the job use the command below. Just be aware that this will get rid of all the files created by the job and tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shipyard jobs del -y --termtasks --wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
