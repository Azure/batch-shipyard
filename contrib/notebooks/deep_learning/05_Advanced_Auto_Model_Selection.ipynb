{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V - Automatic Model Selection from Parametric Sweep using Task Dependencies\n",
    "In this notebook we will be taking the example from the [Parametric Sweep](04_Parameter_Sweep.ipynb) notebook and automating the entire chain using task dependencies in a single Azure Batch job.\n",
    "\n",
    "* [Setup](#section1)\n",
    "* [Configure job](#section2)\n",
    "* [Submit job](#section3)\n",
    "* [Download best model](#section4)\n",
    "* [Delete job](#section5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a simple alias for Batch Shipyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%alias shipyard SHIPYARD_CONFIGDIR=config python $HOME/batch-shipyard/shipyard.py %l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that everything is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: shipyard.py [OPTIONS] COMMAND [ARGS]...\r\n",
      "\r\n",
      "  Batch Shipyard: Provision and Execute Docker Workloads on Azure Batch\r\n",
      "\r\n",
      "Options:\r\n",
      "  --version   Show the version and exit.\r\n",
      "  -h, --help  Show this message and exit.\r\n",
      "\r\n",
      "Commands:\r\n",
      "  cert      Certificate actions\r\n",
      "  data      Data actions\r\n",
      "  fs        Filesystem in Azure actions\r\n",
      "  jobs      Jobs actions\r\n",
      "  keyvault  KeyVault actions\r\n",
      "  misc      Miscellaneous actions\r\n",
      "  pool      Pool actions\r\n",
      "  storage   Storage actions\r\n"
     ]
    }
   ],
   "source": [
    "shipyard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the account information we saved earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_json(filename):\n",
    "    with open(filename, 'r') as infile:\n",
    "        return json.load(infile)\n",
    "    \n",
    "account_info = read_json('account_information.json')\n",
    "\n",
    "storage_account_key = account_info['storage_account_key']\n",
    "storage_account_name = account_info['storage_account_name']\n",
    "IMAGE_NAME = account_info['IMAGE_NAME']\n",
    "STORAGE_ALIAS = account_info['STORAGE_ALIAS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous job we ran on a single node we will be running the job on GPU enabled nodes. We will be repeating the parametric search we did in the previous notebook but this time instead of downloading all the results and evaluating the model performance we will get a final task to do that for us using task dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def write_json_to_file(json_dict, filename):\n",
    "    \"\"\" Simple function to write JSON dictionaries to files\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as outfile:\n",
    "        json.dump(json_dict, outfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the `jobs.json` configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tasks for parametric sweep cntk-ps-as-job: 4\n"
     ]
    }
   ],
   "source": [
    "JOB_ID = 'cntk-ps-as-job'\n",
    "\n",
    "jobs = {\n",
    "    \"job_specifications\": [\n",
    "        {\n",
    "            \"id\": JOB_ID,\n",
    "            \"tasks\": [\n",
    "                        {\n",
    "                            \"image\": IMAGE_NAME,\n",
    "                            \"task_factory\": {\n",
    "                                \"parametric_sweep\": {\n",
    "                                    \"product\": [\n",
    "                                        {#num_convolution_layers\n",
    "                                            \"start\": 2,\n",
    "                                            \"stop\": 4,\n",
    "                                            \"step\": 1\n",
    "                                        },\n",
    "                                        {#minibatch_size\n",
    "                                            \"start\": 32,\n",
    "                                            \"stop\": 96,\n",
    "                                            \"step\": 32\n",
    "                                        }\n",
    "                                    ]\n",
    "                                }\n",
    "                            },\n",
    "                            \"command\": \"bash -c \\\"source /cntk/activate-cntk; python -u ConvNet_CIFAR10.py --datadir $AZ_BATCH_NODE_SHARED_DIR/data --num_convolution_layers {0} --minibatch_size {1} --max_epochs 30\\\"\",\n",
    "                            \"remove_container_after_exit\": True,\n",
    "                            \"gpu\": True,\n",
    "                            \"resource_files\": [\n",
    "                                    {\n",
    "                                        \"file_path\": \"ConvNet_CIFAR10.py\",\n",
    "                                        \"blob_source\": \"https://batchshipyardexamples.blob.core.windows.net/code/ConvNet_CIFAR10.py\",\n",
    "                                        \"file_mode\":'0777'\n",
    "                                    }\n",
    "                            ],\n",
    "                            \"output_data\": {\n",
    "                                \"azure_storage\": [\n",
    "                                    {\n",
    "                                        \"storage_account_settings\": STORAGE_ALIAS,\n",
    "                                        \"container\": \"output\",\n",
    "                                        \"source\": \"$AZ_BATCH_TASK_DIR/wd/Models\"\n",
    "                                    },\n",
    "                                ]\n",
    "                            },\n",
    "                        }\n",
    "            ]  \n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "num_parameter_sweep_tasks = 4\n",
    "print('number of tasks for parametric sweep {}: {}'.format(JOB_ID, num_parameter_sweep_tasks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create the Python program to run that performs the best model selection. Note that this code is nearly similar to the code for selecting the best model locally in the [Parameter sweep notebook](04_Parameter_Sweep.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing autoselect.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile autoselect.py\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def read_json(filename):\n",
    "    with open(filename, 'r') as infile:\n",
    "        return json.load(infile)\n",
    "\n",
    "def scandir(basedir):\n",
    "    for root, dirs, files in os.walk(basedir):\n",
    "        for f in files:\n",
    "            yield os.path.join(root, f) \n",
    "\n",
    "MODELS_DIR = os.path.join('wd', 'Models')\n",
    "            \n",
    "results_dict = {}\n",
    "for model in scandir(MODELS_DIR):\n",
    "    if not model.endswith('.json'):\n",
    "        continue\n",
    "    key = model.split(os.sep)[2]  # due to MODELS_DIR path change\n",
    "    results_dict[key] = read_json(model)\n",
    "\n",
    "# use items() instead of iteritems() as this will be run in python3\n",
    "tuple_min_error = min(results_dict.items(), key=lambda x: x[1]['test_metric'])\n",
    "configuration_with_min_error = tuple_min_error[0]\n",
    "print('task with smallest error: {} ({})'.format(configuration_with_min_error, tuple_min_error[1]['test_metric']))\n",
    "\n",
    "# copy best model to wd\n",
    "MODEL_NAME = 'ConvNet_CIFAR10_model.dnn'\n",
    "shutil.copy(os.path.join(MODELS_DIR, configuration_with_min_error, MODEL_NAME), '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to prepare the file to be uploaded to the Azure Storage account to be referenced in the task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 12\r\n",
      "drwxr-xr-x  2 nbuser nbuser 4096 Aug 11 10:43 ./\r\n",
      "drwx------ 16 nbuser nbuser 4096 Aug 11 10:43 ../\r\n",
      "-rw-r--r--  1 nbuser nbuser 1001 Aug 11 10:43 autoselect.py\r\n"
     ]
    }
   ],
   "source": [
    "INPUT_CONTAINER = 'input-autoselect'\n",
    "OUTPUT_CONTAINER = 'output-autoselect'\n",
    "UPLOAD_DIR = 'autoselect_upload'\n",
    "\n",
    "!rm -rf $UPLOAD_DIR\n",
    "!mkdir -p $UPLOAD_DIR\n",
    "!mv autoselect.py $UPLOAD_DIR\n",
    "!ls -alF $UPLOAD_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alias `blobxfer` and upload it to `INPUT_CONTAINER`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%alias blobxfer python -m blobxfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      " azure blobxfer parameters [v0.12.1]\n",
      "=====================================\n",
      "             platform: Linux-4.4.0-87-generic-x86_64-with-debian-stretch-sid\n",
      "   python interpreter: CPython 2.7.11\n",
      "     package versions: az.common=1.1.8 az.sml=0.20.5 az.stor=0.35.1 crypt=2.0.3 req=2.18.3\n",
      "      subscription id: None\n",
      "      management cert: None\n",
      "   transfer direction: local->Azure\n",
      "       local resource: autoselect_upload\n",
      "      include pattern: None\n",
      "      remote resource: None\n",
      "   max num of workers: 24\n",
      "              timeout: None\n",
      "      storage account: batch71d77646st\n",
      "              use SAS: False\n",
      "  upload as page blob: False\n",
      "  auto vhd->page blob: False\n",
      " upload to file share: False\n",
      " container/share name: input-autoselect\n",
      "  container/share URI: https://batch71d77646st.blob.core.windows.net/input-autoselect\n",
      "    compute block MD5: False\n",
      "     compute file MD5: True\n",
      "    skip on MD5 match: True\n",
      "   chunk size (bytes): 4194304\n",
      "     create container: True\n",
      "  keep mismatched MD5: False\n",
      "     recursive if dir: True\n",
      "component strip on up: 1\n",
      "        remote delete: False\n",
      "           collate to: disabled\n",
      "      local overwrite: True\n",
      "      encryption mode: disabled\n",
      "         RSA key file: disabled\n",
      "         RSA key type: disabled\n",
      "=======================================\n",
      "\n",
      "script start time: 2017-08-11 10:43:58\n",
      "computing file md5 on: autoselect_upload/autoselect.py\n",
      "  >> md5: A0V93+uyerutTDe7jckLLA==\n",
      "creating container, if needed: input-autoselect\n",
      "detected 0 empty files to upload\n",
      "performing 1 put blocks/blobs and 1 put block lists\n",
      "spawning 1 worker threads\n",
      "xfer progress: [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100.00%     340.84 blocks/min    \n",
      "\n",
      "0.000954627990723 MiB transfered, elapsed 0.176033973694 sec. Throughput = 0.0433838069182 Mbit/sec\n",
      "\n",
      "\n",
      "script elapsed time: 0.645431995392 sec\n",
      "script end time: 2017-08-11 10:43:59\n"
     ]
    }
   ],
   "source": [
    "blobxfer $storage_account_name $INPUT_CONTAINER $UPLOAD_DIR --upload --storageaccountkey $storage_account_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll append the `auto-model-selection` task which depends on the prior training tasks. The important properties here are `depends_on_range` which specifies a range of task ids the `auto-model-selection` task depends on. Additionally, this task requires data from the prior run task which is specified in `input_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_input_data_spec(job_id, task_id):\n",
    "    return {\n",
    "        \"job_id\": job_id,\n",
    "        \"task_id\": task_id,\n",
    "        \"include\": [\"wd/Models/*_{}_{}/*\".format(task_id, job_id)]\n",
    "    }\n",
    "\n",
    "input_data = []\n",
    "for x in range(0, num_parameter_sweep_tasks):\n",
    "    input_data.append(generate_input_data_spec(JOB_ID, '{}'.format(x)))\n",
    "\n",
    "model_selection_task = {\n",
    "    \"id\": \"auto-model-selection\",\n",
    "    \"command\": 'bash -c \"source /cntk/activate-cntk; python -u autoselect.py\"',\n",
    "    \"depends_on_range\": [0, num_parameter_sweep_tasks - 1],\n",
    "    \"image\": IMAGE_NAME,\n",
    "    \"remove_container_after_exit\": True,\n",
    "    \"input_data\": {\n",
    "        \"azure_batch\": input_data,\n",
    "        \"azure_storage\": [\n",
    "            {\n",
    "                \"storage_account_settings\": STORAGE_ALIAS,\n",
    "                \"container\": INPUT_CONTAINER\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"output_data\": {\n",
    "        \"azure_storage\": [\n",
    "            {\n",
    "                \"storage_account_settings\": STORAGE_ALIAS,\n",
    "                \"container\": OUTPUT_CONTAINER,\n",
    "                \"include\": [\"*wd/ConvNet_CIFAR10_model.dnn\"],\n",
    "                \"blobxfer_extra_options\": \"--delete --strip-components 2\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# append auto-model-selection task to jobs\n",
    "jobs['job_specifications'][0]['tasks'].append(model_selection_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"job_specifications\": [\n",
      "        {\n",
      "            \"id\": \"cntk-ps-as-job\", \n",
      "            \"tasks\": [\n",
      "                {\n",
      "                    \"command\": \"bash -c \\\"source /cntk/activate-cntk; python -u ConvNet_CIFAR10.py --datadir $AZ_BATCH_NODE_SHARED_DIR/data --num_convolution_layers {0} --minibatch_size {1} --max_epochs 30\\\"\", \n",
      "                    \"gpu\": true, \n",
      "                    \"image\": \"microsoft/cntk:2.0-gpu-python3.5-cuda8.0-cudnn5.1\", \n",
      "                    \"output_data\": {\n",
      "                        \"azure_storage\": [\n",
      "                            {\n",
      "                                \"container\": \"output\", \n",
      "                                \"source\": \"$AZ_BATCH_TASK_DIR/wd/Models\", \n",
      "                                \"storage_account_settings\": \"mystorageaccount\"\n",
      "                            }\n",
      "                        ]\n",
      "                    }, \n",
      "                    \"remove_container_after_exit\": true, \n",
      "                    \"resource_files\": [\n",
      "                        {\n",
      "                            \"blob_source\": \"https://batchshipyardexamples.blob.core.windows.net/code/ConvNet_CIFAR10.py\", \n",
      "                            \"file_mode\": \"0777\", \n",
      "                            \"file_path\": \"ConvNet_CIFAR10.py\"\n",
      "                        }\n",
      "                    ], \n",
      "                    \"task_factory\": {\n",
      "                        \"parametric_sweep\": {\n",
      "                            \"product\": [\n",
      "                                {\n",
      "                                    \"start\": 2, \n",
      "                                    \"step\": 1, \n",
      "                                    \"stop\": 4\n",
      "                                }, \n",
      "                                {\n",
      "                                    \"start\": 32, \n",
      "                                    \"step\": 32, \n",
      "                                    \"stop\": 96\n",
      "                                }\n",
      "                            ]\n",
      "                        }\n",
      "                    }\n",
      "                }, \n",
      "                {\n",
      "                    \"command\": \"bash -c \\\"source /cntk/activate-cntk; python -u autoselect.py\\\"\", \n",
      "                    \"depends_on_range\": [\n",
      "                        0, \n",
      "                        3\n",
      "                    ], \n",
      "                    \"id\": \"auto-model-selection\", \n",
      "                    \"image\": \"microsoft/cntk:2.0-gpu-python3.5-cuda8.0-cudnn5.1\", \n",
      "                    \"input_data\": {\n",
      "                        \"azure_batch\": [\n",
      "                            {\n",
      "                                \"include\": [\n",
      "                                    \"wd/Models/*_0_cntk-ps-as-job/*\"\n",
      "                                ], \n",
      "                                \"job_id\": \"cntk-ps-as-job\", \n",
      "                                \"task_id\": \"0\"\n",
      "                            }, \n",
      "                            {\n",
      "                                \"include\": [\n",
      "                                    \"wd/Models/*_1_cntk-ps-as-job/*\"\n",
      "                                ], \n",
      "                                \"job_id\": \"cntk-ps-as-job\", \n",
      "                                \"task_id\": \"1\"\n",
      "                            }, \n",
      "                            {\n",
      "                                \"include\": [\n",
      "                                    \"wd/Models/*_2_cntk-ps-as-job/*\"\n",
      "                                ], \n",
      "                                \"job_id\": \"cntk-ps-as-job\", \n",
      "                                \"task_id\": \"2\"\n",
      "                            }, \n",
      "                            {\n",
      "                                \"include\": [\n",
      "                                    \"wd/Models/*_3_cntk-ps-as-job/*\"\n",
      "                                ], \n",
      "                                \"job_id\": \"cntk-ps-as-job\", \n",
      "                                \"task_id\": \"3\"\n",
      "                            }\n",
      "                        ], \n",
      "                        \"azure_storage\": [\n",
      "                            {\n",
      "                                \"container\": \"input-autoselect\", \n",
      "                                \"storage_account_settings\": \"mystorageaccount\"\n",
      "                            }\n",
      "                        ]\n",
      "                    }, \n",
      "                    \"output_data\": {\n",
      "                        \"azure_storage\": [\n",
      "                            {\n",
      "                                \"blobxfer_extra_options\": \"--delete --strip-components 2\", \n",
      "                                \"container\": \"output-autoselect\", \n",
      "                                \"include\": [\n",
      "                                    \"*wd/ConvNet_CIFAR10_model.dnn\"\n",
      "                                ], \n",
      "                                \"storage_account_settings\": \"mystorageaccount\"\n",
      "                            }\n",
      "                        ]\n",
      "                    }, \n",
      "                    \"remove_container_after_exit\": true\n",
      "                }\n",
      "            ]\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "write_json_to_file(jobs, os.path.join('config', 'jobs.json'))\n",
    "print(json.dumps(jobs, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit job\n",
    "Check that everything is ok with our pool before we submit our jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-11 10:44:11,748 INFO - list of pools\r\n",
      "* pool id: gpupool\r\n",
      "  * vm size: standard_nc6\r\n",
      "  * state: PoolState.active\r\n",
      "  * allocation state: AllocationState.steady\r\n",
      "  * no resize errors\r\n",
      "  * vm count:\r\n",
      "    * dedicated:\r\n",
      "      * current: 3\r\n",
      "      * target: 3\r\n",
      "    * low priority:\r\n",
      "      * current: 0\r\n",
      "      * target: 0\r\n",
      "  * node agent: batch.node.ubuntu 16.04\r\n"
     ]
    }
   ],
   "source": [
    "shipyard pool list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have confirmed everything is working we can execute our job using the command below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-11 10:44:19,959 INFO - Adding job cntk-ps-as-job to pool gpupool\n",
      "2017-08-11 10:44:20,682 INFO - uploading file /tmp/tmpMUZiU1 as u'shipyardtaskrf-cntk-ps-as-job/0.shipyard.envlist'\n",
      "2017-08-11 10:44:21,099 INFO - uploading file /tmp/tmpymGaGJ as u'shipyardtaskrf-cntk-ps-as-job/1.shipyard.envlist'\n",
      "2017-08-11 10:44:21,703 INFO - uploading file /tmp/tmpnZ5CsU as u'shipyardtaskrf-cntk-ps-as-job/2.shipyard.envlist'\n",
      "2017-08-11 10:44:22,317 INFO - uploading file /tmp/tmp2Ah4dW as u'shipyardtaskrf-cntk-ps-as-job/3.shipyard.envlist'\n",
      "2017-08-11 10:44:22,871 DEBUG - submitting 5 tasks (0 -> 4) to job cntk-ps-as-job\n",
      "2017-08-11 10:44:23,259 INFO - submitted all 5 tasks to job cntk-ps-as-job\n"
     ]
    }
   ],
   "source": [
    "shipyard jobs add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the command below we can check the status of our jobs. Once all jobs have an exit code we can continue. You can also view the **heatmap** of this pool on [Azure Portal](https://portal.azure.com) to monitor the progress of this job on the compute nodes under your Batch account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-11 11:11:46,992 INFO - list of tasks for job cntk-ps-as-job\r\n",
      "* task id: 0\r\n",
      "  * job id: cntk-ps-as-job\r\n",
      "  * state: TaskState.completed\r\n",
      "  * max retries: 0\r\n",
      "  * retention time: 10675199 days, 2:48:05.477581\r\n",
      "  * execution details:\r\n",
      "    * pool id: gpupool\r\n",
      "    * node id: tvm-3257026573_1-20170811t093905z\r\n",
      "    * started: 2017-08-11 10:44:24.853753+00:00\r\n",
      "    * completed: 2017-08-11 10:54:49.762664+00:00\r\n",
      "    * duration: 0:10:24.908911\r\n",
      "    * exit code: 0\r\n",
      "* task id: 1\r\n",
      "  * job id: cntk-ps-as-job\r\n",
      "  * state: TaskState.completed\r\n",
      "  * max retries: 0\r\n",
      "  * retention time: 10675199 days, 2:48:05.477581\r\n",
      "  * execution details:\r\n",
      "    * pool id: gpupool\r\n",
      "    * node id: tvm-3257026573_2-20170811t093905z\r\n",
      "    * started: 2017-08-11 10:44:24.884756+00:00\r\n",
      "    * completed: 2017-08-11 10:52:26.989212+00:00\r\n",
      "    * duration: 0:08:02.104456\r\n",
      "    * exit code: 0\r\n",
      "* task id: 2\r\n",
      "  * job id: cntk-ps-as-job\r\n",
      "  * state: TaskState.completed\r\n",
      "  * max retries: 0\r\n",
      "  * retention time: 10675199 days, 2:48:05.477581\r\n",
      "  * execution details:\r\n",
      "    * pool id: gpupool\r\n",
      "    * node id: tvm-3257026573_3-20170811t093905z\r\n",
      "    * started: 2017-08-11 10:44:24.848633+00:00\r\n",
      "    * completed: 2017-08-11 10:55:52.572920+00:00\r\n",
      "    * duration: 0:11:27.724287\r\n",
      "    * exit code: 0\r\n",
      "* task id: 3\r\n",
      "  * job id: cntk-ps-as-job\r\n",
      "  * state: TaskState.completed\r\n",
      "  * max retries: 0\r\n",
      "  * retention time: 10675199 days, 2:48:05.477581\r\n",
      "  * execution details:\r\n",
      "    * pool id: gpupool\r\n",
      "    * node id: tvm-3257026573_2-20170811t093905z\r\n",
      "    * started: 2017-08-11 10:52:28.115171+00:00\r\n",
      "    * completed: 2017-08-11 11:01:35.947822+00:00\r\n",
      "    * duration: 0:09:07.832651\r\n",
      "    * exit code: 0\r\n",
      "* task id: auto-model-selection\r\n",
      "  * job id: cntk-ps-as-job\r\n",
      "  * state: TaskState.completed\r\n",
      "  * max retries: 0\r\n",
      "  * retention time: 10675199 days, 2:48:05.477581\r\n",
      "  * execution details:\r\n",
      "    * pool id: gpupool\r\n",
      "    * node id: tvm-3257026573_3-20170811t093905z\r\n",
      "    * started: 2017-08-11 11:01:50.324648+00:00\r\n",
      "    * completed: 2017-08-11 11:01:58.744248+00:00\r\n",
      "    * duration: 0:00:08.419600\r\n",
      "    * exit code: 0\r\n"
     ]
    }
   ],
   "source": [
    "shipyard jobs listtasks --jobid $JOB_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download best model\n",
    "The best performing model from the parametric sweep job should now be saved to our `OUTPUT_CONTAINER` container by the `auto-model-selection` task. Let's save this model in `MODELS_DIR`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MODELS_DIR = 'auto-selected-model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the best performing model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      " azure blobxfer parameters [v0.12.1]\n",
      "=====================================\n",
      "             platform: Linux-4.4.0-87-generic-x86_64-with-debian-stretch-sid\n",
      "   python interpreter: CPython 2.7.11\n",
      "     package versions: az.common=1.1.8 az.sml=0.20.5 az.stor=0.35.1 crypt=2.0.3 req=2.18.3\n",
      "      subscription id: None\n",
      "      management cert: None\n",
      "   transfer direction: Azure->local\n",
      "       local resource: auto-selected-model\n",
      "      include pattern: None\n",
      "      remote resource: .\n",
      "   max num of workers: 24\n",
      "              timeout: None\n",
      "      storage account: batch71d77646st\n",
      "              use SAS: False\n",
      "  upload as page blob: False\n",
      "  auto vhd->page blob: False\n",
      " upload to file share: False\n",
      " container/share name: output-autoselect\n",
      "  container/share URI: https://batch71d77646st.blob.core.windows.net/output-autoselect\n",
      "    compute block MD5: False\n",
      "     compute file MD5: True\n",
      "    skip on MD5 match: True\n",
      "   chunk size (bytes): 4194304\n",
      "     create container: True\n",
      "  keep mismatched MD5: False\n",
      "     recursive if dir: True\n",
      "component strip on up: 1\n",
      "        remote delete: False\n",
      "           collate to: disabled\n",
      "      local overwrite: True\n",
      "      encryption mode: disabled\n",
      "         RSA key file: disabled\n",
      "         RSA key type: disabled\n",
      "=======================================\n",
      "\n",
      "script start time: 2017-08-11 11:11:55\n",
      "attempting to copy entire container output-autoselect to auto-selected-model\n",
      "generating local directory structure and pre-allocating space\n",
      "created local directory: auto-selected-model\n",
      "remote blob: ConvNet_CIFAR10_model.dnn length: 1957354 bytes, md5: CMAcuByyd8gol22apse0Zw==\n",
      "performing 1 range-gets\n",
      "spawning 1 worker threads\n",
      "xfer progress: [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100.00%      82.62 range-gets/min    \n",
      "\n",
      "1.86667823792 MiB transfered, elapsed 0.726207971573 sec. Throughput = 20.5635664822 Mbit/sec\n",
      "\n",
      "performing finalization (if applicable): HMAC-SHA256: False, MD5: True\n",
      "[MD5: OK, auto-selected-model/ConvNet_CIFAR10_model.dnn] CMAcuByyd8gol22apse0Zw== <L..R> CMAcuByyd8gol22apse0Zw==\n",
      "finalization complete.\n",
      "\n",
      "script elapsed time: 1.0967669487 sec\n",
      "script end time: 2017-08-11 11:11:56\n"
     ]
    }
   ],
   "source": [
    "blobxfer $storage_account_name $OUTPUT_CONTAINER $MODELS_DIR --remoteresource . --download --storageaccountkey $storage_account_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model file (`ConvNet_CIFAR10_model.dnn`) is now ready for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1920\r\n",
      "drwxr-xr-x  2 nbuser nbuser    4096 Aug 11 11:11 ./\r\n",
      "drwx------ 17 nbuser nbuser    4096 Aug 11 11:11 ../\r\n",
      "-rw-r--r--  1 nbuser nbuser 1957354 Aug 11 11:11 ConvNet_CIFAR10_model.dnn\r\n"
     ]
    }
   ],
   "source": [
    "!ls -alF $MODELS_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To delete the job use the command below. Just be aware that this will get rid of all the files created by the job and tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-11 11:12:04,390 DEBUG - Skipping termination of completed task 0 on job cntk-ps-as-job\n",
      "2017-08-11 11:12:04,754 DEBUG - Skipping termination of completed task 1 on job cntk-ps-as-job\n",
      "2017-08-11 11:12:05,129 DEBUG - Skipping termination of completed task 2 on job cntk-ps-as-job\n",
      "2017-08-11 11:12:05,530 DEBUG - Skipping termination of completed task 3 on job cntk-ps-as-job\n",
      "2017-08-11 11:12:05,717 DEBUG - Skipping termination of completed task auto-model-selection on job cntk-ps-as-job\n",
      "2017-08-11 11:12:06,176 INFO - deleting job: cntk-ps-as-job\n",
      "2017-08-11 11:12:06,358 DEBUG - waiting for job cntk-ps-as-job to delete\n",
      "2017-08-11 11:12:38,422 INFO - job cntk-ps-as-job does not exist\n"
     ]
    }
   ],
   "source": [
    "shipyard jobs del -y --termtasks --wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
