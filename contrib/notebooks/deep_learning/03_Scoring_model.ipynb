{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III - Scoring the Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Once the model is trained on Azure Batch Shipyard, it can be retrieved from blob storage and used to score unseen data\n",
    "* [Setup](#section1)\n",
    "* [Downloading the trained model](#section2)\n",
    "* [Loading the trained model in memory](#section3)\n",
    "* [Scoring unseen images](#section4)\n",
    "* [Test with your own image](#section5)\n",
    "* [Scoring at scale on Batch Shipyard](#section6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the tqdm progress bar utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create alias for shipyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%alias shipyard SHIPYARD_CONFIGDIR=config python $HOME/batch-shipyard/shipyard.py %l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that everything is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shipyard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports, configuration and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "import json\n",
    "from math import sqrt\n",
    "import cntk\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import HTML\n",
    "from tqdm import tqdm\n",
    "import xml\n",
    "import pickle\n",
    "\n",
    "HTML(\"\"\"<style>.output_png {display: table-cell;text-align: right;vertical-align: middle;}</style>\"\"\")\n",
    "\n",
    "# Downloading assets\n",
    "MODEL = 'ConvNet_CIFAR10_model.dnn'\n",
    "MODEL_FOLDER = 'models'\n",
    "IMAGE_FOLDER = 'images'\n",
    "MODEL_PATH = os.path.join(MODEL_FOLDER, MODEL)\n",
    "\n",
    "# Assets for scoring on the notebook\n",
    "LOCAL_MEAN_FILE = 'mean.xml'\n",
    "LOCAL_TEST_BATCH = 'test_batch.pickle'\n",
    "URL_FMT = 'https://batchshipyardexamples.blob.core.windows.net/scoring/{}'\n",
    "\n",
    "MEAN_IMAGE_URI = URL_FMT.format(LOCAL_MEAN_FILE)\n",
    "TEST_BATCH_URI = URL_FMT.format(LOCAL_TEST_BATCH)\n",
    "\n",
    "# Loading the configuration from setup notebook\n",
    "CONFIG_FILE = 'account_information.json'\n",
    "with open(CONFIG_FILE, 'r') as f:\n",
    "    config = json.load(f)\n",
    "    \n",
    "STORAGE_ACCOUNT_NAME = config['storage_account_name']\n",
    "STORAGE_ACCOUNT_KEY = config['storage_account_key']\n",
    "IMAGE_NAME = config['IMAGE_NAME']\n",
    "STORAGE_ALIAS = config['STORAGE_ALIAS']\n",
    "\n",
    "# Utility function\n",
    "def write_json_to_file(json_dict, filename):\n",
    "    \"\"\" Simple function to write JSON dictionaries to files\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as outfile:\n",
    "        json.dump(json_dict, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating the folder for the trained models\n",
    "!rm -rf $MODEL_FOLDER $IMAGE_FOLDER\n",
    "!mkdir -p $MODEL_FOLDER\n",
    "!mkdir -p $IMAGE_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the trained model\n",
    "The model we trained in the previous notebook can be downloaded from blob storage. First, let's alias `blobxfer` to simplify transfers rather than using the Azure CLI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%alias blobxfer python -m blobxfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will attempt to download the model from the storage account. `blobxfer` will complete successfully with exit code of 0 if the model exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blobxfer $STORAGE_ACCOUNT_NAME output $MODEL_FOLDER --remoteresource . --include \"*_cntk-training-job/*.dnn\" --download --storageaccountkey $STORAGE_ACCOUNT_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Downloaded model from prior notebook training run\")\n",
    "!mv $MODEL_FOLDER/*_cntk-training-job/*.dnn $MODEL_FOLDER\n",
    "!rm -rf $MODEL_FOLDER/*_cntk-training-job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has been downloaded on the environment of the notebook and is ready to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!ls -alF $MODEL_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the trained model in memory\n",
    "\n",
    "The model is expecting CIFAR-10 type images, i.e. RGB images with dimensions 32x32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model dimensions\n",
    "IMAGE_HEIGHT = 32\n",
    "IMAGE_WIDTH = 32\n",
    "NUM_CHANNELS = 3\n",
    "NUM_CLASSES = 10\n",
    "# Class labels in order\n",
    "LABELS = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = cntk.load_model(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring unseen images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We download some unseen test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!wget $MEAN_IMAGE_URI -O $LOCAL_MEAN_FILE\n",
    "!wget $TEST_BATCH_URI -O $LOCAL_TEST_BATCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the mean image, as a pre-processing step applied to the images to score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_image = xml.etree.ElementTree.parse(LOCAL_MEAN_FILE).getroot()\n",
    "mean_image = [float(i) for i in mean_image.find('MeanImg').find('data').text.strip().split(' ')]\n",
    "mean_image = np.array(mean_image).reshape((32, 32, 3)).transpose((2, 0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility functions to help loading and scoring images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_image(filepath):\n",
    "    \"\"\" \n",
    "    Loading the image and resizing it to match\n",
    "    the expected format from the network\n",
    "    \"\"\"\n",
    "    img = Image.open(filepath)\n",
    "    img.load()\n",
    "    wpercent = (IMAGE_WIDTH/float(img.size[0]))\n",
    "    hsize = int((float(img.size[1])*float(wpercent)))\n",
    "    img = img.resize((IMAGE_WIDTH,hsize), Image.ANTIALIAS)\n",
    "    return img\n",
    "\n",
    "def get_predicted_label(model, img, mean_image): \n",
    "    \"\"\" \n",
    "    Perform a forward pass on the network\n",
    "    and return the predicted label\n",
    "    \"\"\"\n",
    "    # Convert image to array\n",
    "    img = np.asarray(img, dtype=\"float32\")\n",
    "    # Add padding to be 32x32\n",
    "    img = np.lib.pad(img, ((IMAGE_WIDTH-img.shape[0],0),(IMAGE_HEIGHT-img.shape[1],0),(0,0)), 'constant', constant_values=(0))\n",
    "    # Transpose from 32x32x3 to 3x32x32\n",
    "    img = np.transpose(img, (2, 0, 1))\n",
    "    img -= mean_image\n",
    "    \n",
    "    # Forward pass\n",
    "    out = model.forward(img)\n",
    "    \n",
    "    # Getting the predicted label\n",
    "    predictions = out[1].values()[0][0]\n",
    "    selected_label = LABELS[predictions.argmax()]\n",
    "    return selected_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading 10000 unseen images from a pickled file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reshape_image(record):\n",
    "    image, label, filename = record\n",
    "    return image.reshape(3,32,32).transpose(1,2,0), label, filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(LOCAL_TEST_BATCH, 'r') as f:\n",
    "    test_batch = pickle.load(f)\n",
    "records = zip(test_batch['data'], test_batch['labels'], test_batch['filenames'])\n",
    "records = map(reshape_image, records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scoring the images in turn and displaying the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = 6\n",
    "\n",
    "# Creating a grid of m by m plots\n",
    "f, axarr = plt.subplots(m, m)\n",
    "f.set_size_inches(m*2, m*2)\n",
    "f.suptitle(\"Scoring {} images [predicted|success]\".format(m*m))\n",
    "\n",
    "# Scoring each image and plotting it in the grid\n",
    "# with the label as a title\n",
    "random.shuffle(records)\n",
    "for i in range(m):\n",
    "    for j in range(m):\n",
    "        img = records[i*m+j][0]\n",
    "        label = get_predicted_label(model, img, mean_image)\n",
    "        axarr[i, j].set_title(\"{}|{}\".format(label, label==LABELS[records[i*m+j][1]]))\n",
    "        axarr[i, j].axis('off')\n",
    "        axarr[i, j].imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with your own image\n",
    "\n",
    "Specify the location of your own image or upload an image, of one of the 10 labels. If you are running in Azure notebooks you can do this by using the `Data` > `Upload...` menu in the tool bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_name = \"<YOUR_IMAGE_NAME.PNG>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img = load_image(image_name)\n",
    "label = get_predicted_label(model, img, mean_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img = Image.open(image_name)\n",
    "img.load()\n",
    "f = plt.figure()\n",
    "plt.imshow(img)\n",
    "plt.title(label)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring at scale on Batch Shipyard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running locally on the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = []\n",
    "for record in tqdm(records):\n",
    "    label = get_predicted_label(model, record[0], mean_image)\n",
    "    result.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is pretty fast... can we do better on Batch Shipyard?\n",
    "\n",
    "Let's first write a driver file that will perform the scoring and upload everything we need on our storage account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile score.py\n",
    "import os\n",
    "import json\n",
    "import cntk\n",
    "import numpy as np \n",
    "import xml\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "# model dimensions\n",
    "IMAGE_HEIGHT = 32\n",
    "IMAGE_WIDTH = 32\n",
    "NUM_CHANNELS = 3\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "MODEL_PATH = 'ConvNet_CIFAR10_model.dnn'\n",
    "LOCAL_MEAN_FILE = 'mean.xml'\n",
    "LOCAL_TEST_BATCH = 'test_batch.pickle'\n",
    "\n",
    "def reshape_image(record):\n",
    "    image, label, filename = record\n",
    "    return image.reshape(3,32,32).transpose(1,2,0), label, filename\n",
    "\n",
    "def get_predicted_label(model, img, mean_image): \n",
    "    \"\"\" \n",
    "    Perform a forward pass on the network\n",
    "    and return the predicted label\n",
    "    \"\"\"\n",
    "    # Convert image to array\n",
    "    img = np.asarray(img, dtype=\"float32\")\n",
    "    # Add padding to be 32x32\n",
    "    img = np.lib.pad(img, ((IMAGE_WIDTH-img.shape[0],0),(IMAGE_HEIGHT-img.shape[1],0),(0,0)), 'constant', constant_values=(0))\n",
    "    # Transpose from 32x32x3 to 3x32x32\n",
    "    img = np.transpose(img, (2, 0, 1))\n",
    "    img -= mean_image\n",
    "    \n",
    "    # Forward pass\n",
    "    out = model.forward(img)\n",
    "    \n",
    "    # Getting the predicted label\n",
    "    predictions = list(out[1].values())[0][0]\n",
    "    return predictions.argmax()\n",
    "\n",
    "\n",
    "# Loading the model\n",
    "model = cntk.load_model(MODEL_PATH)\n",
    "\n",
    "# Loading the mean image\n",
    "mean_image = xml.etree.ElementTree.parse(LOCAL_MEAN_FILE).getroot()\n",
    "mean_image = [float(i) for i in mean_image.find('MeanImg').find('data').text.strip().split(' ')]\n",
    "mean_image = np.array(mean_image).reshape((32, 32, 3)).transpose((2, 0, 1))\n",
    "\n",
    "# Loading the images\n",
    "with open(LOCAL_TEST_BATCH, 'rb') as f:\n",
    "    u = pickle._Unpickler(f)\n",
    "    u.encoding = 'latin1'\n",
    "    test_batch = u.load()\n",
    "records = zip(test_batch['data'], test_batch['labels'], test_batch['filenames'])\n",
    "records = map(reshape_image, records)\n",
    "\n",
    "result = []\n",
    "i = 0\n",
    "for record in records:\n",
    "    if i % 1000 == 0:\n",
    "        print(\"processed {} records\".format(i))\n",
    "    i+=1\n",
    "    label = get_predicted_label(model, record[0], mean_image)\n",
    "    # convert label type numpy.int64 to python int\n",
    "    result.append((label.item(), record[1], record[2]))\n",
    "\n",
    "toc = time.time()\n",
    "print(\"{} seconds elapsed to process {} records\".format(toc-tic, i))\n",
    "\n",
    "with open('results.json', 'w') as f:\n",
    "    json.dump(result, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's upload all of the data we need to ingress in the batch task:\n",
    "- the mean file\n",
    "- the image data file\n",
    "- the trained model\n",
    "- the python driver, score.py\n",
    "\n",
    "Let's designate the containers to use for input and output and then copy all of the input data into one directory to upload via blobxfer to the `INPUT_CONTAINER`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "INPUT_CONTAINER = \"inputscore\"\n",
    "OUTPUT_CONTAINER = \"outputscore\"\n",
    "\n",
    "UPLOAD_DIR = 'score_upload'\n",
    "\n",
    "!mkdir -p $UPLOAD_DIR\n",
    "!cp $LOCAL_TEST_BATCH $LOCAL_MEAN_FILE $MODEL_PATH score.py $UPLOAD_DIR\n",
    "!ls -alF $UPLOAD_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload via `blobxfer` to the `INPUT_CONTAINER`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blobxfer $STORAGE_ACCOUNT_NAME $INPUT_CONTAINER $UPLOAD_DIR --upload --storageaccountkey $STORAGE_ACCOUNT_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the jobs json specification. The task will first activate cntk and then run the scoring script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "JOB_ID = 'cntk-scoring-job'\n",
    "\n",
    "COMMAND = 'bash -c \"source /cntk/activate-cntk; python -u score.py\"'\n",
    "\n",
    "jobs = {\n",
    "    \"job_specifications\": [\n",
    "        {\n",
    "            \"id\": JOB_ID,\n",
    "            \"tasks\": [\n",
    "                {\n",
    "                    \"image\": IMAGE_NAME,\n",
    "                    \"remove_container_after_exit\": True,\n",
    "                    \"command\": COMMAND,\n",
    "                    \"gpu\": True,\n",
    "                    \"output_data\": {\n",
    "                        \"azure_storage\": [\n",
    "                            {\n",
    "                                \"storage_account_settings\": STORAGE_ALIAS,\n",
    "                                \"container\": OUTPUT_CONTAINER,\n",
    "                                \"include\": [\"*.json\"],\n",
    "                                \"blobxfer_extra_options\": \"--delete --strip-components 2\"\n",
    "                            }\n",
    "                        ]\n",
    "                    },\n",
    "                    \"input_data\": {\n",
    "                        \"azure_storage\": [\n",
    "                            {\n",
    "                                \"storage_account_settings\": STORAGE_ALIAS,\n",
    "                                \"container\": INPUT_CONTAINER\n",
    "                            }\n",
    "                        ]\n",
    "                    },\n",
    "                }\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_json_to_file(jobs, os.path.join('config', 'jobs.json'))\n",
    "print(json.dumps(jobs, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the specification for the jobs is written, we add the task to batch shipyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shipyard jobs add --tail stdout.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the total duration of the time taken for the task with the command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shipyard jobs listtasks --jobid $JOB_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can retrieve the results from the executed task from the `OUTPUT_CONTAINER`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blobxfer $STORAGE_ACCOUNT_NAME $OUTPUT_CONTAINER $MODEL_FOLDER --download --remoteresource results.json --storageaccountkey $STORAGE_ACCOUNT_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!ls -alF $MODEL_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Note:** we could have used the `shipyard data getfile` command to retrieve the `results.json` file directly from the compute node if we did not need to persist the results to Azure Storage and the compute node is still running.\n",
    "\n",
    "Now that we are done with the scoring job, delete it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shipyard jobs del -y --termtasks --wait"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Next notebook: Parametric Sweep](04_Parameter_Sweep.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
