{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI - Tensorboard Visualization\n",
    "This notebook will show how to create an SSH tunnel from the machine running the Notebook to the compute node of a task that is running or has run a task that has generated [Tensorboard](https://www.tensorflow.org/get_started/summaries_and_tensorboard) summary compatible output.\n",
    "\n",
    "**NOTE:** This notebook cannot be run on Azure notebooks due to restrictions. Please run this notebook locally. If you are running this notebook on Windows, please ensure you have `ssh.exe` in your `%PATH%`. You can download OpenSSH binaries for Windows [here](https://github.com/PowerShell/Win32-OpenSSH/releases).\n",
    "\n",
    "* [Setup](#section1)\n",
    "* [Configure job](#section2)\n",
    "* [Submit job](#section3)\n",
    "* [Delete job](#section4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a simple alias for Batch Shipyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%alias shipyard SHIPYARD_CONFIGDIR=config python $HOME/batch-shipyard/shipyard.py %l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that everything is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shipyard.py, version 2.9.0rc1\r\n"
     ]
    }
   ],
   "source": [
    "shipyard --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get some variables stored in the Setup notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_json(filename):\n",
    "    with open(filename, 'r') as infile:\n",
    "        return json.load(infile)\n",
    "    \n",
    "account_info = read_json('account_information.json')\n",
    "\n",
    "IMAGE_NAME = account_info['IMAGE_NAME']\n",
    "STORAGE_ALIAS = account_info['STORAGE_ALIAS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "## Configure job\n",
    "The following will be similar to the [Single GPU Training](02_Single_GPU_Training.ipynb) notebook from earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the `jobs` configuration, we will add `--logdir=tensorboard_logs` as a parameter to generate the Tensorboard summary log data during the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TASK_ID = 'run_cifar10' # This should be changed per task\n",
    "\n",
    "JOB_ID = 'cntk-train-tensorboard-job'\n",
    "\n",
    "COMMAND = 'bash -c \"source /cntk/activate-cntk; python -u ConvNet_CIFAR10.py --datadir $AZ_BATCH_NODE_SHARED_DIR/data --tensorboard_logdir tensorboard_logs\"'\n",
    "\n",
    "jobs = {\n",
    "    \"job_specifications\": [\n",
    "        {\n",
    "            \"id\": JOB_ID,\n",
    "            \"tasks\": [\n",
    "                {\n",
    "                    \"id\": TASK_ID,\n",
    "                    \"image\": IMAGE_NAME,\n",
    "                    \"remove_container_after_exit\": True,\n",
    "                    \"command\": COMMAND,\n",
    "                    \"gpu\": True,\n",
    "                    \"resource_files\": [\n",
    "                        {\n",
    "                            \"file_path\": \"ConvNet_CIFAR10.py\",\n",
    "                            \"blob_source\": \"https://batchshipyardexamples.blob.core.windows.net/code/ConvNet_CIFAR10.py\",\n",
    "                            \"file_mode\":'0777'\n",
    "                        }\n",
    "                    ],\n",
    "                    \"output_data\": {\n",
    "                        \"azure_storage\": [\n",
    "                            {\n",
    "                                \"storage_account_settings\": STORAGE_ALIAS,\n",
    "                                \"container\": \"output\",\n",
    "                                \"source\": \"$AZ_BATCH_TASK_WORKING_DIR/Models\"\n",
    "                            },\n",
    "                        ]\n",
    "                    },\n",
    "                }\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Write the jobs configuration to the `jobs.json` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"job_specifications\": [\n",
      "        {\n",
      "            \"id\": \"cntk-train-tensorboard-job\", \n",
      "            \"tasks\": [\n",
      "                {\n",
      "                    \"command\": \"bash -c \\\"source /cntk/activate-cntk; python -u ConvNet_CIFAR10.py --datadir $AZ_BATCH_NODE_SHARED_DIR/data --tensorboard_logdir tensorboard_logs\\\"\", \n",
      "                    \"gpu\": true, \n",
      "                    \"id\": \"run_cifar10\", \n",
      "                    \"image\": \"microsoft/cntk:2.0-gpu-python3.5-cuda8.0-cudnn5.1\", \n",
      "                    \"output_data\": {\n",
      "                        \"azure_storage\": [\n",
      "                            {\n",
      "                                \"container\": \"output\", \n",
      "                                \"source\": \"$AZ_BATCH_TASK_WORKING_DIR/Models\", \n",
      "                                \"storage_account_settings\": \"mystorageaccount\"\n",
      "                            }\n",
      "                        ]\n",
      "                    }, \n",
      "                    \"remove_container_after_exit\": true, \n",
      "                    \"resource_files\": [\n",
      "                        {\n",
      "                            \"blob_source\": \"https://batchshipyardexamples.blob.core.windows.net/code/ConvNet_CIFAR10.py\", \n",
      "                            \"file_mode\": \"0777\", \n",
      "                            \"file_path\": \"ConvNet_CIFAR10.py\"\n",
      "                        }\n",
      "                    ]\n",
      "                }\n",
      "            ]\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def write_json_to_file(json_dict, filename):\n",
    "    \"\"\" Simple function to write JSON dictionaries to files\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as outfile:\n",
    "        json.dump(json_dict, outfile)\n",
    "\n",
    "write_json_to_file(jobs, os.path.join('config', 'jobs.json'))\n",
    "print(json.dumps(jobs, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit job\n",
    "Check that everything is ok with our pool before we submit our jobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-11 11:13:37,846 INFO - compute nodes for pool gpupool\r\n",
      "* node id: tvm-3257026573_1-20170811t093905z\r\n",
      "  * state: ComputeNodeState.idle\r\n",
      "  * scheduling state: SchedulingState.enabled\r\n",
      "  * no errors\r\n",
      "  * start task:\r\n",
      "    * exit code: 0\r\n",
      "    * started: 2017-08-11 09:41:12.873732+00:00\r\n",
      "    * completed: 2017-08-11 09:48:32.262192+00:00\r\n",
      "    * duration: 0:07:19.388460\r\n",
      "  * vm size: standard_nc6\r\n",
      "  * dedicated: True\r\n",
      "  * ip address: 10.0.0.6\r\n",
      "  * running tasks: 0\r\n",
      "  * total tasks run: 2\r\n",
      "  * total tasks succeeded: 2\r\n",
      "* node id: tvm-3257026573_2-20170811t093905z\r\n",
      "  * state: ComputeNodeState.idle\r\n",
      "  * scheduling state: SchedulingState.enabled\r\n",
      "  * no errors\r\n",
      "  * start task:\r\n",
      "    * exit code: 0\r\n",
      "    * started: 2017-08-11 09:40:34.895513+00:00\r\n",
      "    * completed: 2017-08-11 09:47:46.141084+00:00\r\n",
      "    * duration: 0:07:11.245571\r\n",
      "  * vm size: standard_nc6\r\n",
      "  * dedicated: True\r\n",
      "  * ip address: 10.0.0.5\r\n",
      "  * running tasks: 0\r\n",
      "  * total tasks run: 4\r\n",
      "  * total tasks succeeded: 4\r\n",
      "* node id: tvm-3257026573_3-20170811t093905z\r\n",
      "  * state: ComputeNodeState.idle\r\n",
      "  * scheduling state: SchedulingState.enabled\r\n",
      "  * no errors\r\n",
      "  * start task:\r\n",
      "    * exit code: 0\r\n",
      "    * started: 2017-08-11 09:40:44.741433+00:00\r\n",
      "    * completed: 2017-08-11 09:47:43.821572+00:00\r\n",
      "    * duration: 0:06:59.080139\r\n",
      "  * vm size: standard_nc6\r\n",
      "  * dedicated: True\r\n",
      "  * ip address: 10.0.0.4\r\n",
      "  * running tasks: 0\r\n",
      "  * total tasks run: 3\r\n",
      "  * total tasks succeeded: 3\r\n"
     ]
    }
   ],
   "source": [
    "shipyard pool listnodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have confirmed everything is working we can execute our job using the command below. Note that we'll not be using the `--tail` option so that the command completes and we can tunnel to Tensorboard concurrently as the task is executing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-11 11:13:41,548 INFO - Adding job cntk-train-tensorboard-job to pool gpupool\n",
      "2017-08-11 11:13:42,102 INFO - uploading file /tmp/tmpG7IKtS as u'shipyardtaskrf-cntk-train-tensorboard-job/run_cifar10.shipyard.envlist'\n",
      "2017-08-11 11:13:42,476 DEBUG - submitting 1 tasks (0 -> 0) to job cntk-train-tensorboard-job\n",
      "2017-08-11 11:13:42,745 INFO - submitted all 1 tasks to job cntk-train-tensorboard-job\n"
     ]
    }
   ],
   "source": [
    "shipyard jobs add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the Batch Shipyard command to instantiate a Tensorboard instance and create an SSH tunnel. The following cell should not return immediately if it is working. Browse to the Tensorboard URL output by the command (which will not be output in the notebook since it is a blocking call), which is http://localhost:6006/\n",
    "\n",
    "**Notes:**\n",
    "1. The Tensorboard instance may take some time to start since this pool does not have the TensorFlow Docker image pre-loaded.\n",
    "2. You will need to manually interrupt the kernel once you are done with your Tensorboard visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-11 11:27:56,747 INFO - list of tasks for job cntk-train-tensorboard-job\r\n",
      "* task id: run_cifar10\r\n",
      "  * job id: cntk-train-tensorboard-job\r\n",
      "  * state: TaskState.completed\r\n",
      "  * max retries: 0\r\n",
      "  * retention time: 10675199 days, 2:48:05.477581\r\n",
      "  * execution details:\r\n",
      "    * pool id: gpupool\r\n",
      "    * node id: tvm-3257026573_3-20170811t093905z\r\n",
      "    * started: 2017-08-11 11:13:43.632112+00:00\r\n",
      "    * completed: 2017-08-11 11:19:08.956084+00:00\r\n",
      "    * duration: 0:05:25.323972\r\n",
      "    * exit code: 0\r\n"
     ]
    }
   ],
   "source": [
    "shipyard jobs listtasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-11 11:14:08,135 DEBUG - waiting for task run_cifar10 in job cntk-train-tensorboard-job to reach a valid state\n",
      "2017-08-11 11:14:08,504 DEBUG - using auto-detected logdir: tensorboard_logs\n",
      "2017-08-11 11:14:08,505 DEBUG - using logpath: /mnt/batch/tasks/workitems/cntk-train-tensorboard-job/job-1/run_cifar10/wd/tensorboard_logs\n",
      "2017-08-11 11:14:08,505 WARNING - no pre-loaded tensorflow Docker image detected on pool, using: gcr.io/tensorflow/tensorflow\n",
      "2017-08-11 11:14:08,931 INFO - \n",
      "\n",
      ">> Please connect to Tensorboard at http://localhost:6006/\n",
      "\n",
      ">> Note that Tensorboard may take a while to start if the Docker is\n",
      ">> not present. Please keep retrying the URL every few seconds.\n",
      "\n",
      ">> Terminate your session with CTRL+C\n",
      "\n",
      ">> If you cannot terminate your session cleanly, run:\n",
      "     shipyard pool ssh --nodeid tvm-3257026573_3-20170811t093905z sudo docker kill 83e81f88\n",
      "\n",
      "Warning: Permanently added '[13.92.233.192]:50000' (ECDSA) to the list of known hosts.\n",
      "Warning: Permanently added '[13.92.233.192]:50000' (ECDSA) to the list of known hosts.\n",
      "Connection to 13.92.233.192 closed.\n",
      "2017-08-11 11:14:13,301 DEBUG - attempting clean up of Tensorboard instance and SSH tunnel\n"
     ]
    }
   ],
   "source": [
    "shipyard misc tensorboard --jobid $JOB_ID --taskid $TASK_ID -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To delete the job use the command below. Just be aware that this will get rid of all the files created by the job and tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-11 11:28:10,039 DEBUG - Skipping termination of completed task run_cifar10 on job cntk-train-tensorboard-job\n",
      "2017-08-11 11:28:10,272 INFO - deleting job: cntk-train-tensorboard-job\n",
      "2017-08-11 11:28:10,484 DEBUG - waiting for job cntk-train-tensorboard-job to delete\n",
      "2017-08-11 11:28:40,980 INFO - job cntk-train-tensorboard-job does not exist\n"
     ]
    }
   ],
   "source": [
    "shipyard jobs del -y --termtasks --wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-11 11:28:42,279 INFO - Deleting pool: gpupool\n",
      "2017-08-11 11:28:42,661 DEBUG - clearing table (pk=batch71d77646ba$gpupool): shipyarddht\n",
      "2017-08-11 11:28:42,861 DEBUG - clearing table (pk=batch71d77646ba$gpupool): shipyardtorrentinfo\n",
      "2017-08-11 11:28:42,911 DEBUG - clearing table (pk=batch71d77646ba$gpupool): shipyardgr\n",
      "2017-08-11 11:28:43,014 DEBUG - clearing table (pk=batch71d77646ba$gpupool): shipyardregistry\n",
      "2017-08-11 11:28:43,116 DEBUG - clearing table (pk=batch71d77646ba$gpupool): shipyardimages\n",
      "2017-08-11 11:28:43,216 DEBUG - clearing table (pk=batch71d77646ba$gpupool): shipyardperf\n",
      "2017-08-11 11:28:43,265 DEBUG - deleting queue: shipyardgr-batch71d77646ba-gpupool\n",
      "2017-08-11 11:28:43,443 DEBUG - deleting container: shipyardrf-batch71d77646ba-gpupool\n",
      "2017-08-11 11:28:43,703 DEBUG - deleting container: shipyardtor-batch71d77646ba-gpupool\n",
      "2017-08-11 11:28:43,756 DEBUG - deleting container: shipyardgr-batch71d77646ba-gpupool\n",
      "2017-08-11 11:28:43,806 DEBUG - waiting for pool gpupool to delete\n"
     ]
    }
   ],
   "source": [
    "shipyard pool del -y --wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
