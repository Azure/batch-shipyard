{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VII - Parallel and Distributed Execution\n",
    "In this notebook, we will execute training across multiple nodes (or in parallel across a single node over multiple GPUs). We will train an image classification model with Resnet20 on the CIFAR-10 data set across multiple nodes in this notebook.\n",
    "\n",
    "Azure Batch and Batch Shipyard have the ability to perform \"gang scheduling\" or scheduling multiple nodes for a single task. This is most commonly used for Message Passing Interface (MPI) jobs.\n",
    "\n",
    "* [Setup](#section1)\n",
    "* [Configure and Submit MPI Job and Submit](#section2)\n",
    "* [Delete Multi-Instance Job](#section3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a simple alias for Batch Shipyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%alias shipyard SHIPYARD_CONFIGDIR=config python $HOME/batch-shipyard/shipyard.py %l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that everything is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: shipyard.py [OPTIONS] COMMAND [ARGS]...\r\n",
      "\r\n",
      "  Batch Shipyard: Provision and Execute Docker Workloads on Azure Batch\r\n",
      "\r\n",
      "Options:\r\n",
      "  --version   Show the version and exit.\r\n",
      "  -h, --help  Show this message and exit.\r\n",
      "\r\n",
      "Commands:\r\n",
      "  cert      Certificate actions\r\n",
      "  data      Data actions\r\n",
      "  fs        Filesystem in Azure actions\r\n",
      "  jobs      Jobs actions\r\n",
      "  keyvault  KeyVault actions\r\n",
      "  misc      Miscellaneous actions\r\n",
      "  pool      Pool actions\r\n",
      "  storage   Storage actions\r\n"
     ]
    }
   ],
   "source": [
    "shipyard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the account information we saved earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def read_json(filename):\n",
    "    with open(filename, 'r') as infile:\n",
    "        return json.load(infile)\n",
    "    \n",
    "def write_json_to_file(json_dict, filename):\n",
    "    \"\"\" Simple function to write JSON dictionaries to files\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as outfile:\n",
    "        json.dump(json_dict, outfile)\n",
    "\n",
    "account_info = read_json('account_information.json')\n",
    "\n",
    "storage_account_key = account_info['storage_account_key']\n",
    "storage_account_name = account_info['storage_account_name']\n",
    "STORAGE_ALIAS = account_info['STORAGE_ALIAS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the `resource_files` to randomly download train and test data for CNTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "IMAGE_NAME = 'alfpark/cntk:2.0.rc2-gpu-1bit-sgd-python3.5-cuda8.0-cudnn5.1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data set conversion scripts to be uploaded. On real production runs, we would already have this data pre-converted instead of converting at the time of node startup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we'll create an MPI helper script for executing the MPI job. This helper script does the following:\n",
    "1. Ensures that there are GPUs available to execute the task.\n",
    "2. Parses the `$AZ_BATCH_HOST_LIST` for all of the hosts participating in the MPI job and creates a `hostfile` from it\n",
    "3. Computes the total number of slots (processors)\n",
    "4. Sets the proper CNTK training directory, script and options\n",
    "5. Executes the MPI job via `mpirun`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing run_cifar10.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_cifar10.sh\n",
    "#!/usr/bin/env bash\n",
    "\n",
    "set -e\n",
    "set -o pipefail\n",
    "\n",
    "# get number of GPUs on machine\n",
    "ngpus=$(nvidia-smi -L | wc -l)\n",
    "echo \"num gpus: $ngpus\"\n",
    "\n",
    "if [ $ngpus -eq 0 ]; then\n",
    "    echo \"No GPUs detected.\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# get number of nodes\n",
    "IFS=',' read -ra HOSTS <<< \"$AZ_BATCH_HOST_LIST\"\n",
    "nodes=${#HOSTS[@]}\n",
    "\n",
    "# create hostfile\n",
    "touch hostfile\n",
    ">| hostfile\n",
    "for node in \"${HOSTS[@]}\"\n",
    "do\n",
    "    echo $node slots=$ngpus max-slots=$ngpus >> hostfile\n",
    "done\n",
    "\n",
    "# compute number of processors\n",
    "np=$(($nodes * $ngpus))\n",
    "\n",
    "# print configuration\n",
    "echo \"num nodes: $nodes\"\n",
    "echo \"hosts: ${HOSTS[@]}\"\n",
    "echo \"num mpi processes: $np\"\n",
    "\n",
    "# set cntk related vars\n",
    "trainscript=$AZ_BATCH_TASK_SHARED_DIR/TrainResNet_CIFAR10.py\n",
    "\n",
    "# set training options\n",
    "trainopts=\"--datadir $AZ_BATCH_NODE_SHARED_DIR/data --modeldir $AZ_BATCH_TASK_WORKING_DIR/output --network resnet20 --distributed True -q 1 -a 0\"\n",
    "\n",
    "# execute mpi job\n",
    "/root/openmpi/bin/mpirun --allow-run-as-root --mca btl_tcp_if_exclude docker0 \\\n",
    "    -np $np --hostfile hostfile -x LD_LIBRARY_PATH\\\n",
    "    /bin/bash -c \"source /cntk/activate-cntk; python -u $trainscript $trainopts $*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move the files into a directory to be uploaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 12\r\n",
      "drwxr-xr-x  2 nbuser nbuser 4096 Aug 11 12:19 ./\r\n",
      "drwx------ 20 nbuser nbuser 4096 Aug 11 12:19 ../\r\n",
      "-rw-r--r--  1 nbuser nbuser 1080 Aug 11 12:19 run_cifar10.sh\r\n"
     ]
    }
   ],
   "source": [
    "INPUT_CONTAINER = 'input-dist'\n",
    "UPLOAD_DIR = 'dist_upload'\n",
    "\n",
    "!rm -rf $UPLOAD_DIR\n",
    "!mkdir -p $UPLOAD_DIR\n",
    "!mv run_cifar10.sh $UPLOAD_DIR\n",
    "!ls -alF $UPLOAD_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create the config structure to directly reference these files to ingress into Azure Storage. This obviates the need to call `blobxfer` as it will be done for us during pool creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"batch_shipyard\": {\n",
    "        \"storage_account_settings\": STORAGE_ALIAS\n",
    "    },\n",
    "    \"global_resources\": {\n",
    "        \"docker_images\": [\n",
    "            IMAGE_NAME\n",
    "        ],\n",
    "        \"files\": [\n",
    "            {\n",
    "                \"source\": {\n",
    "                    \"path\": UPLOAD_DIR\n",
    "                },\n",
    "                \"destination\": {\n",
    "                    \"storage_account_settings\": STORAGE_ALIAS,\n",
    "                    \"data_transfer\": {\n",
    "                        \"container\": INPUT_CONTAINER\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create the pool specification with a few modifications for this particular execution:\n",
    "- `inter_node_communication_enabled` will ensure nodes are allocated such that they can communicate with each other (e.g., send and receive network packets)\n",
    "- `input_data` specifies the scripts we created above to be downloaded into `$AZ_BATCH_NODE_SHARED_DIR/cifar10_data`\n",
    "- `transfer_files_on_pool_creation` will transfer the `files` specified in `global_resources` to be transferred during pool creation (i.e., `pool add`)\n",
    "- `resource_files` are the CNTK train and test data files\n",
    "- `additional_node_prep_commands` are commands to execute for node preparation of all compute nodes. Our additional node prep command is to execute the conversion script we created in an earlier step above\n",
    "\n",
    "**Note:** Most often it is better to scale up the execution first, prior to scale out. Due to the default Batch core quota of just 20 cores, we are using 3 `STANDARD_NC6` nodes. In real production runs, we'd most likely scale up to multiple GPUs within a single node (parallel execution) such as `STANDARD_NC12` or `STANDARD_NC24` prior to scaling out to multiple NC nodes (parallel and distributed execution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "POOL_ID = 'gpupool-multi-instance'\n",
    "\n",
    "pool = {\n",
    "    \"pool_specification\": {\n",
    "        \"id\": POOL_ID,\n",
    "        \"vm_configuration\": {\n",
    "            \"platform_image\": {\n",
    "                \"publisher\": \"Canonical\",\n",
    "                \"offer\": \"UbuntuServer\",\n",
    "                \"sku\": \"16.04-LTS\"\n",
    "            },\n",
    "        },\n",
    "        \"vm_size\": \"STANDARD_NC6\",\n",
    "        \"vm_count\": {\n",
    "            \"dedicated\": 3\n",
    "        },\n",
    "        \"ssh\": {\n",
    "            \"username\": \"docker\"\n",
    "        },\n",
    "        \"inter_node_communication_enabled\": True,\n",
    "        \"reboot_on_start_task_failed\": False,\n",
    "        \"block_until_all_global_resources_loaded\": True,\n",
    "        \"transfer_files_on_pool_creation\": True,\n",
    "        \"input_data\": {\n",
    "            \"azure_storage\": [\n",
    "                {\n",
    "                    \"storage_account_settings\": STORAGE_ALIAS,\n",
    "                    \"container\": INPUT_CONTAINER,\n",
    "                    \"destination\": \"$AZ_BATCH_NODE_SHARED_DIR\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        \"resource_files\": [\n",
    "            {\n",
    "                \"file_path\": \"cifar_data_processing.py\",\n",
    "                \"blob_source\": \"https://batchshipyardexamples.blob.core.windows.net/code/cifar_data_processing.py\",\n",
    "                \"file_mode\":'0777'\n",
    "            },\n",
    "            {\n",
    "                \"file_path\": \"convert_cifar10.sh\",\n",
    "                \"blob_source\": \"https://batchshipyardexamples.blob.core.windows.net/code/convert_cifar10.sh\",\n",
    "                \"file_mode\":'0777'\n",
    "            }\n",
    "        ],\n",
    "         \"additional_node_prep_commands\": [\n",
    "            \"/bin/bash convert_cifar10.sh {} $AZ_BATCH_NODE_SHARED_DIR/data\".format(IMAGE_NAME),\n",
    "             \"chmod 777 $AZ_BATCH_NODE_SHARED_DIR/run_cifar10.sh\"\n",
    "        ]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory 'config': File exists\n",
      "{\n",
      "    \"batch_shipyard\": {\n",
      "        \"storage_account_settings\": \"mystorageaccount\"\n",
      "    }, \n",
      "    \"global_resources\": {\n",
      "        \"docker_images\": [\n",
      "            \"alfpark/cntk:2.0.rc2-gpu-1bit-sgd-python3.5-cuda8.0-cudnn5.1\"\n",
      "        ], \n",
      "        \"files\": [\n",
      "            {\n",
      "                \"destination\": {\n",
      "                    \"data_transfer\": {\n",
      "                        \"container\": \"input-dist\"\n",
      "                    }, \n",
      "                    \"storage_account_settings\": \"mystorageaccount\"\n",
      "                }, \n",
      "                \"source\": {\n",
      "                    \"path\": \"dist_upload\"\n",
      "                }\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"pool_specification\": {\n",
      "        \"additional_node_prep_commands\": [\n",
      "            \"/bin/bash convert_cifar10.sh alfpark/cntk:2.0.rc2-gpu-1bit-sgd-python3.5-cuda8.0-cudnn5.1 $AZ_BATCH_NODE_SHARED_DIR/data\", \n",
      "            \"chmod 777 $AZ_BATCH_NODE_SHARED_DIR/run_cifar10.sh\"\n",
      "        ], \n",
      "        \"block_until_all_global_resources_loaded\": true, \n",
      "        \"id\": \"gpupool-multi-instance\", \n",
      "        \"input_data\": {\n",
      "            \"azure_storage\": [\n",
      "                {\n",
      "                    \"container\": \"input-dist\", \n",
      "                    \"destination\": \"$AZ_BATCH_NODE_SHARED_DIR\", \n",
      "                    \"storage_account_settings\": \"mystorageaccount\"\n",
      "                }\n",
      "            ]\n",
      "        }, \n",
      "        \"inter_node_communication_enabled\": true, \n",
      "        \"reboot_on_start_task_failed\": false, \n",
      "        \"resource_files\": [\n",
      "            {\n",
      "                \"blob_source\": \"https://batchshipyardexamples.blob.core.windows.net/code/cifar_data_processing.py\", \n",
      "                \"file_mode\": \"0777\", \n",
      "                \"file_path\": \"cifar_data_processing.py\"\n",
      "            }, \n",
      "            {\n",
      "                \"blob_source\": \"https://batchshipyardexamples.blob.core.windows.net/code/convert_cifar10.sh\", \n",
      "                \"file_mode\": \"0777\", \n",
      "                \"file_path\": \"convert_cifar10.sh\"\n",
      "            }\n",
      "        ], \n",
      "        \"ssh\": {\n",
      "            \"username\": \"docker\"\n",
      "        }, \n",
      "        \"transfer_files_on_pool_creation\": true, \n",
      "        \"vm_configuration\": {\n",
      "            \"platform_image\": {\n",
      "                \"offer\": \"UbuntuServer\", \n",
      "                \"publisher\": \"Canonical\", \n",
      "                \"sku\": \"16.04-LTS\"\n",
      "            }\n",
      "        }, \n",
      "        \"vm_count\": {\n",
      "            \"dedicated\": 3\n",
      "        }, \n",
      "        \"vm_size\": \"STANDARD_NC6\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!mkdir config\n",
    "write_json_to_file(config, os.path.join('config', 'config.json'))\n",
    "write_json_to_file(pool, os.path.join('config', 'pool.json'))\n",
    "print(json.dumps(config, indent=4, sort_keys=True))\n",
    "print(json.dumps(pool, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the pool, please be patient while the compute nodes are allocated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-11 12:20:07,933 INFO - creating container: shipyardremotefs\n",
      "2017-08-11 12:20:08,092 INFO - creating table: shipyarddht\n",
      "2017-08-11 12:20:08,275 INFO - creating table: shipyardtorrentinfo\n",
      "2017-08-11 12:20:08,322 INFO - creating table: shipyardgr\n",
      "2017-08-11 12:20:08,375 INFO - creating table: shipyardregistry\n",
      "2017-08-11 12:20:08,421 INFO - creating container: shipyardrf-batch71d77646ba-gpupool-multi-instance\n",
      "2017-08-11 12:20:08,465 INFO - creating table: shipyardimages\n",
      "2017-08-11 12:20:08,510 INFO - creating container: shipyardtor-batch71d77646ba-gpupool-multi-instance\n",
      "2017-08-11 12:20:08,552 INFO - creating container: shipyardgr-batch71d77646ba-gpupool-multi-instance\n",
      "2017-08-11 12:20:08,594 DEBUG - clearing table (pk=batch71d77646ba$gpupool-multi-instance): shipyarddht\n",
      "2017-08-11 12:20:08,741 DEBUG - clearing table (pk=batch71d77646ba$gpupool-multi-instance): shipyardtorrentinfo\n",
      "2017-08-11 12:20:08,787 DEBUG - clearing table (pk=batch71d77646ba$gpupool-multi-instance): shipyardgr\n",
      "2017-08-11 12:20:08,836 DEBUG - clearing table (pk=batch71d77646ba$gpupool-multi-instance): shipyardregistry\n",
      "2017-08-11 12:20:08,880 INFO - deleting blobs: shipyardrf-batch71d77646ba-gpupool-multi-instance\n",
      "2017-08-11 12:20:08,922 DEBUG - clearing table (pk=batch71d77646ba$gpupool-multi-instance): shipyardimages\n",
      "2017-08-11 12:20:08,970 INFO - deleting blobs: shipyardtor-batch71d77646ba-gpupool-multi-instance\n",
      "2017-08-11 12:20:09,009 INFO - deleting blobs: shipyardgr-batch71d77646ba-gpupool-multi-instance\n",
      "2017-08-11 12:20:09,048 DEBUG - clearing table (pk=batch71d77646ba$gpupool-multi-instance): shipyardperf\n",
      "2017-08-11 12:20:09,094 WARNING - disabling block until all global resources loaded with transfer files on pool creation enabled\n",
      "2017-08-11 12:20:09,141 INFO - adding global resource: docker:alfpark/cntk:2.0.rc2-gpu-1bit-sgd-python3.5-cuda8.0-cudnn5.1 hash=9f2f8f2e9b7858a43d9051b46c29c60b550c68ee\n",
      "2017-08-11 12:20:09,605 DEBUG - autoscale enabled: False\n",
      "2017-08-11 12:20:09,605 DEBUG - no virtual network settings specified\n",
      "2017-08-11 12:20:10,185 INFO - uploading file /home/nbuser/batch-shipyard/scripts/docker_jp_block.sh as u'docker_jp_block.sh'\n",
      "2017-08-11 12:20:10,264 INFO - uploading file /home/nbuser/batch-shipyard/scripts/shipyard_blobxfer.sh as u'shipyard_blobxfer.sh'\n",
      "2017-08-11 12:20:10,357 INFO - uploading file /home/nbuser/batch-shipyard/resources/nvidia-driver.run as 'nvidia-driver.run'\n",
      "2017-08-11 12:20:12,461 INFO - uploading file /home/nbuser/batch-shipyard/resources/nvidia-docker.deb as 'nvidia-docker.deb'\n",
      "2017-08-11 12:20:12,662 INFO - uploading file /home/nbuser/batch-shipyard/scripts/shipyard_nodeprep.sh as u'shipyard_nodeprep.sh'\n",
      "2017-08-11 12:20:12,725 INFO - begin ingressing data from dist_upload to container input-dist\n",
      "2017-08-11 12:20:12,726 INFO - Attempting to create pool: gpupool-multi-instance\n",
      "2017-08-11 12:20:13,046 INFO - Created pool: gpupool-multi-instance\n",
      "2017-08-11 12:20:13,047 INFO - waiting for all nodes in pool gpupool-multi-instance to reach one of: frozenset([<ComputeNodeState.idle: 'idle'>, <ComputeNodeState.preempted: 'preempted'>, <ComputeNodeState.start_task_failed: 'startTaskFailed'>, <ComputeNodeState.unusable: 'unusable'>])\n",
      "2017-08-11 12:20:13,648 INFO - =====================================\n",
      " azure blobxfer parameters [v0.12.1]\n",
      "=====================================\n",
      "             platform: Linux-4.4.0-87-generic-x86_64-with-debian-stretch-sid\n",
      "   python interpreter: CPython 2.7.11\n",
      "     package versions: az.common=1.1.8 az.sml=0.20.5 az.stor=0.35.1 crypt=2.0.3 req=2.18.3\n",
      "      subscription id: None\n",
      "      management cert: None\n",
      "   transfer direction: local->Azure\n",
      "       local resource: dist_upload\n",
      "      include pattern: None\n",
      "      remote resource: None\n",
      "   max num of workers: 24\n",
      "              timeout: None\n",
      "      storage account: batch71d77646st\n",
      "              use SAS: False\n",
      "  upload as page blob: False\n",
      "  auto vhd->page blob: False\n",
      " upload to file share: False\n",
      " container/share name: input-dist\n",
      "  container/share URI: https://batch71d77646st.blob.core.windows.net/input-dist\n",
      "    compute block MD5: False\n",
      "     compute file MD5: True\n",
      "    skip on MD5 match: True\n",
      "   chunk size (bytes): 4194304\n",
      "     create container: True\n",
      "  keep mismatched MD5: False\n",
      "     recursive if dir: True\n",
      "component strip on up: 1\n",
      "        remote delete: False\n",
      "           collate to: disabled\n",
      "      local overwrite: True\n",
      "      encryption mode: disabled\n",
      "         RSA key file: disabled\n",
      "         RSA key type: disabled\n",
      "=======================================\n",
      "\n",
      "script start time: 2017-08-11 12:20:13\n",
      "computing file md5 on: dist_upload/run_cifar10.sh\n",
      "  >> oAjxPhIr6gNE1v68IgaiMQ== <L..R> oAjxPhIr6gNE1v68IgaiMQ== run_cifar10.sh match: skip\n",
      "creating container, if needed: input-dist\n",
      "detected no transfer actions needed to be taken, exiting...\n",
      "\n",
      "2017-08-11 12:20:33,666 DEBUG - waiting for 3 dedicated nodes and 0 low priority nodes of size standard_nc6 to reach desired state in pool gpupool-multi-instance [resize_timeout=0:15:00 allocation_state=AllocationState.resizing allocation_state_transition_time=2017-08-11 12:20:12.901609+00:00]\n",
      "2017-08-11 12:21:04,341 DEBUG - waiting for 3 dedicated nodes and 0 low priority nodes of size standard_nc6 to reach desired state in pool gpupool-multi-instance [resize_timeout=0:15:00 allocation_state=AllocationState.resizing allocation_state_transition_time=2017-08-11 12:20:12.901609+00:00]\n",
      "2017-08-11 12:21:35,350 DEBUG - waiting for 3 dedicated nodes and 0 low priority nodes of size standard_nc6 to reach desired state in pool gpupool-multi-instance [resize_timeout=0:15:00 allocation_state=AllocationState.steady allocation_state_transition_time=2017-08-11 12:21:22.811901+00:00]\n",
      "2017-08-11 12:21:35,350 DEBUG - tvm-1783593343_1-20170811t122122z: ComputeNodeState.starting\n",
      "2017-08-11 12:21:35,350 DEBUG - tvm-1783593343_2-20170811t122122z: ComputeNodeState.starting\n",
      "2017-08-11 12:21:35,350 DEBUG - tvm-1783593343_3-20170811t122122z: ComputeNodeState.starting\n",
      "2017-08-11 12:22:06,558 DEBUG - waiting for 3 dedicated nodes and 0 low priority nodes of size standard_nc6 to reach desired state in pool gpupool-multi-instance [resize_timeout=0:15:00 allocation_state=AllocationState.steady allocation_state_transition_time=2017-08-11 12:21:22.811901+00:00]\n",
      "2017-08-11 12:22:06,558 DEBUG - tvm-1783593343_1-20170811t122122z: ComputeNodeState.starting\n",
      "2017-08-11 12:22:06,558 DEBUG - tvm-1783593343_2-20170811t122122z: ComputeNodeState.starting\n",
      "2017-08-11 12:22:06,558 DEBUG - tvm-1783593343_3-20170811t122122z: ComputeNodeState.starting\n",
      "2017-08-11 12:22:37,805 DEBUG - waiting for 3 dedicated nodes and 0 low priority nodes of size standard_nc6 to reach desired state in pool gpupool-multi-instance [resize_timeout=0:15:00 allocation_state=AllocationState.steady allocation_state_transition_time=2017-08-11 12:21:22.811901+00:00]\n",
      "2017-08-11 12:22:37,805 DEBUG - tvm-1783593343_1-20170811t122122z: ComputeNodeState.starting\n",
      "2017-08-11 12:22:37,805 DEBUG - tvm-1783593343_2-20170811t122122z: ComputeNodeState.starting\n",
      "2017-08-11 12:22:37,805 DEBUG - tvm-1783593343_3-20170811t122122z: ComputeNodeState.starting\n",
      "2017-08-11 12:23:09,058 DEBUG - waiting for 3 dedicated nodes and 0 low priority nodes of size standard_nc6 to reach desired state in pool gpupool-multi-instance [resize_timeout=0:15:00 allocation_state=AllocationState.steady allocation_state_transition_time=2017-08-11 12:21:22.811901+00:00]\n",
      "2017-08-11 12:23:09,059 DEBUG - tvm-1783593343_1-20170811t122122z: ComputeNodeState.waiting_for_start_task\n",
      "2017-08-11 12:23:09,059 DEBUG - tvm-1783593343_2-20170811t122122z: ComputeNodeState.waiting_for_start_task\n",
      "2017-08-11 12:23:09,059 DEBUG - tvm-1783593343_3-20170811t122122z: ComputeNodeState.waiting_for_start_task\n",
      "2017-08-11 12:23:40,357 DEBUG - waiting for 3 dedicated nodes and 0 low priority nodes of size standard_nc6 to reach desired state in pool gpupool-multi-instance [resize_timeout=0:15:00 allocation_state=AllocationState.steady allocation_state_transition_time=2017-08-11 12:21:22.811901+00:00]\n",
      "2017-08-11 12:23:40,357 DEBUG - tvm-1783593343_1-20170811t122122z: ComputeNodeState.waiting_for_start_task\n",
      "2017-08-11 12:23:40,357 DEBUG - tvm-1783593343_2-20170811t122122z: ComputeNodeState.waiting_for_start_task\n",
      "2017-08-11 12:23:40,357 DEBUG - tvm-1783593343_3-20170811t122122z: ComputeNodeState.waiting_for_start_task\n",
      "2017-08-11 12:24:11,532 DEBUG - waiting for 3 dedicated nodes and 0 low priority nodes of size standard_nc6 to reach desired state in pool gpupool-multi-instance [resize_timeout=0:15:00 allocation_state=AllocationState.steady allocation_state_transition_time=2017-08-11 12:21:22.811901+00:00]\n",
      "2017-08-11 12:24:11,532 DEBUG - tvm-1783593343_1-20170811t122122z: ComputeNodeState.waiting_for_start_task\n",
      "2017-08-11 12:24:11,533 DEBUG - tvm-1783593343_2-20170811t122122z: ComputeNodeState.waiting_for_start_task\n",
      "2017-08-11 12:24:11,533 DEBUG - tvm-1783593343_3-20170811t122122z: ComputeNodeState.waiting_for_start_task\n",
      "2017-08-11 12:24:42,880 DEBUG - waiting for 3 dedicated nodes and 0 low priority nodes of size standard_nc6 to reach desired state in pool gpupool-multi-instance [resize_timeout=0:15:00 allocation_state=AllocationState.steady allocation_state_transition_time=2017-08-11 12:21:22.811901+00:00]\n",
      "2017-08-11 12:24:42,880 DEBUG - tvm-1783593343_1-20170811t122122z: ComputeNodeState.waiting_for_start_task\n",
      "2017-08-11 12:24:42,880 DEBUG - tvm-1783593343_2-20170811t122122z: ComputeNodeState.waiting_for_start_task\n",
      "2017-08-11 12:24:42,880 DEBUG - tvm-1783593343_3-20170811t122122z: ComputeNodeState.waiting_for_start_task\n",
      "2017-08-11 12:25:14,077 DEBUG - waiting for 3 dedicated nodes and 0 low priority nodes of size standard_nc6 to reach desired state in pool gpupool-multi-instance [resize_timeout=0:15:00 allocation_state=AllocationState.steady allocation_state_transition_time=2017-08-11 12:21:22.811901+00:00]\n",
      "2017-08-11 12:25:14,078 DEBUG - tvm-1783593343_1-20170811t122122z: ComputeNodeState.waiting_for_start_task\n",
      "2017-08-11 12:25:14,078 DEBUG - tvm-1783593343_2-20170811t122122z: ComputeNodeState.waiting_for_start_task\n",
      "2017-08-11 12:25:14,078 DEBUG - tvm-1783593343_3-20170811t122122z: ComputeNodeState.waiting_for_start_task\n",
      "2017-08-11 12:25:45,306 DEBUG - waiting for 3 dedicated nodes and 0 low priority nodes of size standard_nc6 to reach desired state in pool gpupool-multi-instance [resize_timeout=0:15:00 allocation_state=AllocationState.steady allocation_state_transition_time=2017-08-11 12:21:22.811901+00:00]\n",
      "2017-08-11 12:25:45,306 DEBUG - tvm-1783593343_1-20170811t122122z: ComputeNodeState.waiting_for_start_task\n",
      "2017-08-11 12:25:45,307 DEBUG - tvm-1783593343_2-20170811t122122z: ComputeNodeState.waiting_for_start_task\n",
      "2017-08-11 12:25:45,307 DEBUG - tvm-1783593343_3-20170811t122122z: ComputeNodeState.waiting_for_start_task\n",
      "2017-08-11 12:26:16,513 DEBUG - waiting for 3 dedicated nodes and 0 low priority nodes of size standard_nc6 to reach desired state in pool gpupool-multi-instance [resize_timeout=0:15:00 allocation_state=AllocationState.steady allocation_state_transition_time=2017-08-11 12:21:22.811901+00:00]\n",
      "2017-08-11 12:26:16,513 DEBUG - tvm-1783593343_1-20170811t122122z: ComputeNodeState.waiting_for_start_task\n",
      "2017-08-11 12:26:16,513 DEBUG - tvm-1783593343_2-20170811t122122z: ComputeNodeState.waiting_for_start_task\n",
      "2017-08-11 12:26:16,513 DEBUG - tvm-1783593343_3-20170811t122122z: ComputeNodeState.waiting_for_start_task\n",
      "2017-08-11 12:26:48,008 DEBUG - waiting for 3 dedicated nodes and 0 low priority nodes of size standard_nc6 to reach desired state in pool gpupool-multi-instance [resize_timeout=0:15:00 allocation_state=AllocationState.steady allocation_state_transition_time=2017-08-11 12:21:22.811901+00:00]\n",
      "2017-08-11 12:26:48,008 DEBUG - tvm-1783593343_1-20170811t122122z: ComputeNodeState.waiting_for_start_task\n",
      "2017-08-11 12:26:48,008 DEBUG - tvm-1783593343_2-20170811t122122z: ComputeNodeState.waiting_for_start_task\n",
      "2017-08-11 12:26:48,008 DEBUG - tvm-1783593343_3-20170811t122122z: ComputeNodeState.waiting_for_start_task\n",
      "2017-08-11 12:27:19,307 DEBUG - waiting for 3 dedicated nodes and 0 low priority nodes of size standard_nc6 to reach desired state in pool gpupool-multi-instance [resize_timeout=0:15:00 allocation_state=AllocationState.steady allocation_state_transition_time=2017-08-11 12:21:22.811901+00:00]\n",
      "2017-08-11 12:27:19,307 DEBUG - tvm-1783593343_1-20170811t122122z: ComputeNodeState.waiting_for_start_task\n",
      "2017-08-11 12:27:19,308 DEBUG - tvm-1783593343_2-20170811t122122z: ComputeNodeState.waiting_for_start_task\n",
      "2017-08-11 12:27:19,308 DEBUG - tvm-1783593343_3-20170811t122122z: ComputeNodeState.waiting_for_start_task\n",
      "2017-08-11 12:27:51,056 DEBUG - waiting for 3 dedicated nodes and 0 low priority nodes of size standard_nc6 to reach desired state in pool gpupool-multi-instance [resize_timeout=0:15:00 allocation_state=AllocationState.steady allocation_state_transition_time=2017-08-11 12:21:22.811901+00:00]\n",
      "2017-08-11 12:27:51,056 DEBUG - tvm-1783593343_1-20170811t122122z: ComputeNodeState.waiting_for_start_task\n",
      "2017-08-11 12:27:51,056 DEBUG - tvm-1783593343_2-20170811t122122z: ComputeNodeState.waiting_for_start_task\n",
      "2017-08-11 12:27:51,056 DEBUG - tvm-1783593343_3-20170811t122122z: ComputeNodeState.waiting_for_start_task\n",
      "2017-08-11 12:28:22,540 DEBUG - waiting for 3 dedicated nodes and 0 low priority nodes of size standard_nc6 to reach desired state in pool gpupool-multi-instance [resize_timeout=0:15:00 allocation_state=AllocationState.steady allocation_state_transition_time=2017-08-11 12:21:22.811901+00:00]\n",
      "2017-08-11 12:28:22,540 DEBUG - tvm-1783593343_1-20170811t122122z: ComputeNodeState.waiting_for_start_task\n",
      "2017-08-11 12:28:22,540 DEBUG - tvm-1783593343_2-20170811t122122z: ComputeNodeState.waiting_for_start_task\n",
      "2017-08-11 12:28:22,540 DEBUG - tvm-1783593343_3-20170811t122122z: ComputeNodeState.waiting_for_start_task\n",
      "2017-08-11 12:28:54,100 DEBUG - waiting for 3 dedicated nodes and 0 low priority nodes of size standard_nc6 to reach desired state in pool gpupool-multi-instance [resize_timeout=0:15:00 allocation_state=AllocationState.steady allocation_state_transition_time=2017-08-11 12:21:22.811901+00:00]\n",
      "2017-08-11 12:28:54,100 DEBUG - tvm-1783593343_1-20170811t122122z: ComputeNodeState.waiting_for_start_task\n",
      "2017-08-11 12:28:54,100 DEBUG - tvm-1783593343_2-20170811t122122z: ComputeNodeState.waiting_for_start_task\n",
      "2017-08-11 12:28:54,100 DEBUG - tvm-1783593343_3-20170811t122122z: ComputeNodeState.waiting_for_start_task\n",
      "2017-08-11 12:29:25,483 DEBUG - waiting for 3 dedicated nodes and 0 low priority nodes of size standard_nc6 to reach desired state in pool gpupool-multi-instance [resize_timeout=0:15:00 allocation_state=AllocationState.steady allocation_state_transition_time=2017-08-11 12:21:22.811901+00:00]\n",
      "2017-08-11 12:29:25,483 DEBUG - tvm-1783593343_1-20170811t122122z: ComputeNodeState.waiting_for_start_task\n",
      "2017-08-11 12:29:25,483 DEBUG - tvm-1783593343_2-20170811t122122z: ComputeNodeState.waiting_for_start_task\n",
      "2017-08-11 12:29:25,483 DEBUG - tvm-1783593343_3-20170811t122122z: ComputeNodeState.waiting_for_start_task\n",
      "2017-08-11 12:29:56,912 DEBUG - waiting for 3 dedicated nodes and 0 low priority nodes of size standard_nc6 to reach desired state in pool gpupool-multi-instance [resize_timeout=0:15:00 allocation_state=AllocationState.steady allocation_state_transition_time=2017-08-11 12:21:22.811901+00:00]\n",
      "2017-08-11 12:29:56,912 DEBUG - tvm-1783593343_1-20170811t122122z: ComputeNodeState.waiting_for_start_task\n",
      "2017-08-11 12:29:56,912 DEBUG - tvm-1783593343_2-20170811t122122z: ComputeNodeState.waiting_for_start_task\n",
      "2017-08-11 12:29:56,912 DEBUG - tvm-1783593343_3-20170811t122122z: ComputeNodeState.waiting_for_start_task\n",
      "2017-08-11 12:30:28,303 DEBUG - waiting for 3 dedicated nodes and 0 low priority nodes of size standard_nc6 to reach desired state in pool gpupool-multi-instance [resize_timeout=0:15:00 allocation_state=AllocationState.steady allocation_state_transition_time=2017-08-11 12:21:22.811901+00:00]\n",
      "2017-08-11 12:30:28,303 DEBUG - tvm-1783593343_1-20170811t122122z: ComputeNodeState.idle\n",
      "2017-08-11 12:30:28,303 DEBUG - tvm-1783593343_2-20170811t122122z: ComputeNodeState.idle\n",
      "2017-08-11 12:30:28,303 DEBUG - tvm-1783593343_3-20170811t122122z: ComputeNodeState.waiting_for_start_task\n",
      "2017-08-11 12:30:39,434 INFO - statistics summary for pool gpupool-multi-instance\n",
      "* Batch Shipyard version: 2.9.0rc1\n",
      "* Total nodes: 3\n",
      "  * Dedicated nodes: 3 (100.0% of target)\n",
      "  * Low Priority nodes: 0 (100.0% of target)\n",
      "* Node states:\n",
      "  * creating: 0\n",
      "  * idle: 3\n",
      "  * leaving_pool: 0\n",
      "  * offline: 0\n",
      "  * preempted: 0\n",
      "  * rebooting: 0\n",
      "  * reimaging: 0\n",
      "  * running: 0\n",
      "  * start_task_failed: 0\n",
      "  * starting: 0\n",
      "  * unknown: 0\n",
      "  * unusable: 0\n",
      "  * waiting_for_start_task: 0\n",
      "* Node uptime:\n",
      "  * Mean: 0:07:42.436868\n",
      "  * Min: 0:07:42.434852\n",
      "  * Max: 0:07:42.438912\n",
      "* Time taken for node creation to ready:\n",
      "  * Mean: 0:09:03.404887\n",
      "  * Min: 0:08:58.986434\n",
      "  * Max: 0:09:08.848516\n",
      "* Time taken for last boot startup (includes prep):\n",
      "  * Mean: 0:07:28.421661\n",
      "  * Min: 0:07:24.003180\n",
      "  * Max: 0:07:33.863274\n",
      "* Running tasks:\n",
      "  * Sum: 0\n",
      "  * Mean: 0.0\n",
      "  * Min: 0\n",
      "  * Max: 0\n",
      "* Total tasks run:\n",
      "  * Sum: 0\n",
      "  * Mean: 0.0\n",
      "  * Min: 0\n",
      "  * Max: 0\n",
      "* Task scheduling slots:\n",
      "  * Busy: 0 (0.00% of runnable)\n",
      "  * Available: 3 (100.00% of runnable)\n",
      "  * Runnable: 3 (100.00% of total)\n",
      "  * Total: 3\n",
      "2017-08-11 12:30:39,723 INFO - generating ssh key pair to path: .\n",
      "Generating public/private rsa key pair.\n",
      "Your identification has been saved in id_rsa_shipyard.\n",
      "Your public key has been saved in id_rsa_shipyard.pub.\n",
      "The key fingerprint is:\n",
      "SHA256:e6j7EJGM/Bv+dwALVp3IQwbQQqygYS2kezCucENS9Uk nbuser@nbserver\n",
      "The key's randomart image is:\n",
      "+---[RSA 2048]----+\n",
      "|..o++oE+oo .     |\n",
      "|++ oo=.+= o      |\n",
      "|Boo.o.*. .       |\n",
      "|o*.  .o..        |\n",
      "|o.+  .+.So       |\n",
      "|oo . . +.o.      |\n",
      "|.     + o ..     |\n",
      "|       + .. .    |\n",
      "|      ooo. .     |\n",
      "+----[SHA256]-----+\n",
      "2017-08-11 12:30:39,818 INFO - adding user docker to node tvm-1783593343_1-20170811t122122z in pool gpupool-multi-instance, expiry=2017-09-10 12:30:39.818355\n",
      "No handlers could be found for logger \"msrest.serialization\"\n",
      "2017-08-11 12:30:40,519 INFO - adding user docker to node tvm-1783593343_2-20170811t122122z in pool gpupool-multi-instance, expiry=2017-09-10 12:30:40.519261\n",
      "2017-08-11 12:30:41,294 INFO - adding user docker to node tvm-1783593343_3-20170811t122122z in pool gpupool-multi-instance, expiry=2017-09-10 12:30:41.294823\n",
      "2017-08-11 12:30:42,356 INFO - node tvm-1783593343_1-20170811t122122z: ip 40.71.210.38 port 50002\n",
      "2017-08-11 12:30:42,577 INFO - node tvm-1783593343_2-20170811t122122z: ip 40.71.210.38 port 50001\n",
      "2017-08-11 12:30:42,787 INFO - node tvm-1783593343_3-20170811t122122z: ip 40.71.210.38 port 50000\n",
      "2017-08-11 12:30:42,787 WARNING - skipping data ingress from dist_upload for pool as ingress to Azure Blob/File Storage not specified\n",
      "2017-08-11 12:30:42,787 INFO - Azure Blob/File Storage transfer completed\n"
     ]
    }
   ],
   "source": [
    "shipyard pool add -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure that all compute nodes are `idle` and ready to accept tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-11 12:40:49,746 INFO - compute nodes for pool gpupool-multi-instance\r\n",
      "* node id: tvm-1783593343_1-20170811t122122z\r\n",
      "  * state: ComputeNodeState.idle\r\n",
      "  * scheduling state: SchedulingState.enabled\r\n",
      "  * no errors\r\n",
      "  * start task:\r\n",
      "    * exit code: 0\r\n",
      "    * started: 2017-08-11 12:22:57.407723+00:00\r\n",
      "    * completed: 2017-08-11 12:30:21.000244+00:00\r\n",
      "    * duration: 0:07:23.592521\r\n",
      "  * vm size: standard_nc6\r\n",
      "  * dedicated: True\r\n",
      "  * ip address: 10.0.0.6\r\n",
      "  * running tasks: 0\r\n",
      "  * total tasks run: 0\r\n",
      "  * total tasks succeeded: 0\r\n",
      "* node id: tvm-1783593343_2-20170811t122122z\r\n",
      "  * state: ComputeNodeState.idle\r\n",
      "  * scheduling state: SchedulingState.enabled\r\n",
      "  * no errors\r\n",
      "  * start task:\r\n",
      "    * exit code: 0\r\n",
      "    * started: 2017-08-11 12:22:57.549985+00:00\r\n",
      "    * completed: 2017-08-11 12:30:24.393521+00:00\r\n",
      "    * duration: 0:07:26.843536\r\n",
      "  * vm size: standard_nc6\r\n",
      "  * dedicated: True\r\n",
      "  * ip address: 10.0.0.5\r\n",
      "  * running tasks: 0\r\n",
      "  * total tasks run: 0\r\n",
      "  * total tasks succeeded: 0\r\n",
      "* node id: tvm-1783593343_3-20170811t122122z\r\n",
      "  * state: ComputeNodeState.idle\r\n",
      "  * scheduling state: SchedulingState.enabled\r\n",
      "  * no errors\r\n",
      "  * start task:\r\n",
      "    * exit code: 0\r\n",
      "    * started: 2017-08-11 12:22:57.686153+00:00\r\n",
      "    * completed: 2017-08-11 12:30:30.862326+00:00\r\n",
      "    * duration: 0:07:33.176173\r\n",
      "  * vm size: standard_nc6\r\n",
      "  * dedicated: True\r\n",
      "  * ip address: 10.0.0.4\r\n",
      "  * running tasks: 0\r\n",
      "  * total tasks run: 0\r\n",
      "  * total tasks succeeded: 0\r\n"
     ]
    }
   ],
   "source": [
    "shipyard pool listnodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure MPI Job and Submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MPI jobs in Batch require execution as a multi-instance task. Essentially this allows multiple compute nodes to be used for a single task.\n",
    "\n",
    "A few things to note in this jobs configuration:\n",
    "- The `COMMAND` executes the `run_cifar10.sh` script that was uploaded earlier as part of the node preparation task.\n",
    "- `auto_complete` is being set to `True` which forces the job to move from `active` to `completed` state once all tasks complete. Note that once a job has moved to `completed` state, no new tasks can be added to it.\n",
    "- `multi_instance` property is populated which enables multiple nodes, e.g., `num_instances` to participate in the execution of this task. The `coordination_command` is the command that is run on all nodes prior to the `command`. Here, we are simply executing the Docker image to run the SSH server for the MPI daemon (e.g., orted, hydra, etc.) to initialize all of the nodes prior to running the application command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "JOB_ID = 'cntk-mpi-job'\n",
    "\n",
    "# reduce the nubmer of epochs to 20 for purposes of this notebook\n",
    "COMMAND = '$AZ_BATCH_NODE_SHARED_DIR/run_cifar10.sh -e 20'\n",
    "jobs = {\n",
    "    \"job_specifications\": [\n",
    "        {\n",
    "            \"id\": JOB_ID,\n",
    "            \"auto_complete\": True,\n",
    "            \"tasks\": [\n",
    "                {\n",
    "                    \"image\": IMAGE_NAME,\n",
    "                    \"remove_container_after_exit\": True,\n",
    "                    \"command\": COMMAND,\n",
    "                    \"gpu\": True,\n",
    "                    \"multi_instance\": {\n",
    "                        \"num_instances\": \"pool_current_dedicated\",\n",
    "                        \"coordination_command\": \"/usr/sbin/sshd -D -p 23\",\n",
    "                        \"resource_files\": [\n",
    "                            {\n",
    "                                \"file_path\": \"resnet_models.py\",\n",
    "                                \"blob_source\": \"https://batchshipyardexamples.blob.core.windows.net/code/resnet_models.py\",\n",
    "                                \"file_mode\":'0777'\n",
    "                            },\n",
    "                            {\n",
    "                                \"file_path\": \"TrainResNet_CIFAR10.py\",\n",
    "                                \"blob_source\": \"https://batchshipyardexamples.blob.core.windows.net/code/TrainResNet_CIFAR10.py\",\n",
    "                                \"file_mode\":'0777'\n",
    "                            }\n",
    "                        ],\n",
    "                    },\n",
    "                }\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"job_specifications\": [\n",
      "        {\n",
      "            \"auto_complete\": true, \n",
      "            \"id\": \"cntk-mpi-job\", \n",
      "            \"tasks\": [\n",
      "                {\n",
      "                    \"command\": \"$AZ_BATCH_NODE_SHARED_DIR/run_cifar10.sh -e 20\", \n",
      "                    \"gpu\": true, \n",
      "                    \"image\": \"alfpark/cntk:2.0.rc2-gpu-1bit-sgd-python3.5-cuda8.0-cudnn5.1\", \n",
      "                    \"multi_instance\": {\n",
      "                        \"coordination_command\": \"/usr/sbin/sshd -D -p 23\", \n",
      "                        \"num_instances\": \"pool_current_dedicated\", \n",
      "                        \"resource_files\": [\n",
      "                            {\n",
      "                                \"blob_source\": \"https://batchshipyardexamples.blob.core.windows.net/code/resnet_models.py\", \n",
      "                                \"file_mode\": \"0777\", \n",
      "                                \"file_path\": \"resnet_models.py\"\n",
      "                            }, \n",
      "                            {\n",
      "                                \"blob_source\": \"https://batchshipyardexamples.blob.core.windows.net/code/TrainResNet_CIFAR10.py\", \n",
      "                                \"file_mode\": \"0777\", \n",
      "                                \"file_path\": \"TrainResNet_CIFAR10.py\"\n",
      "                            }\n",
      "                        ]\n",
      "                    }, \n",
      "                    \"remove_container_after_exit\": true\n",
      "                }\n",
      "            ]\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "write_json_to_file(jobs, os.path.join('config', 'jobs.json'))\n",
    "print(json.dumps(jobs, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit the job and tail `stdout.txt`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-11 12:40:57,376 INFO - Adding job cntk-mpi-job to pool gpupool-multi-instance\n",
      "2017-08-11 12:40:57,957 INFO - uploading file /tmp/tmpG9mmhO as u'shipyardtaskrf-cntk-mpi-job/task-00000.shipyard.envlist'\n",
      "2017-08-11 12:40:57,999 DEBUG - submitting 1 tasks (0 -> 0) to job cntk-mpi-job\n",
      "2017-08-11 12:40:58,261 INFO - submitted all 1 tasks to job cntk-mpi-job\n",
      "2017-08-11 12:40:58,469 DEBUG - attempting to stream file stdout.txt from job=cntk-mpi-job task=task-00000\n",
      "8c36f6001f233d02345634dd4547f23abf9324b0c081c6319d314b8c32cdb536\n",
      "num gpus: 1\n",
      "num nodes: 3\n",
      "hosts: 10.0.0.4 10.0.0.5 10.0.0.6\n",
      "num mpi processes: 3\n",
      "\n",
      "************************************************************\n",
      "CNTK is activated.\n",
      "\n",
      "Please checkout tutorials and examples here:\n",
      "  /cntk/Tutorials\n",
      "  /cntk/Examples\n",
      "\n",
      "To deactivate the environment run\n",
      "\n",
      "  source /root/anaconda3/bin/deactivate\n",
      "\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "CNTK is activated.\n",
      "\n",
      "Please checkout tutorials and examples here:\n",
      "  /cntk/Tutorials\n",
      "  /cntk/Examples\n",
      "\n",
      "To deactivate the environment run\n",
      "\n",
      "  source /root/anaconda3/bin/deactivate\n",
      "\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "CNTK is activated.\n",
      "\n",
      "Please checkout tutorials and examples here:\n",
      "  /cntk/Tutorials\n",
      "  /cntk/Examples\n",
      "\n",
      "To deactivate the environment run\n",
      "\n",
      "  source /root/anaconda3/bin/deactivate\n",
      "\n",
      "************************************************************\n",
      "Finished Epoch[1 of 20]: [Training] loss = 1.709072 * 50048, metric = 63.28% * 50048 21.764s (2299.6 samples/s);\n",
      "Finished Epoch[1 of 20]: [Training] loss = 1.709072 * 50048, metric = 63.28% * 50048 22.517s (2222.7 samples/s);\n",
      "Finished Epoch[1 of 20]: [Training] loss = 1.709072 * 50048, metric = 63.28% * 50048 22.017s (2273.2 samples/s);\n",
      "Finished Epoch[2 of 20]: [Training] loss = 1.216296 * 50048, metric = 43.85% * 50048 19.959s (2507.5 samples/s);\n",
      "Finished Epoch[2 of 20]: [Training] loss = 1.216296 * 50048, metric = 43.85% * 50048 19.958s (2507.7 samples/s);\n",
      "Finished Epoch[2 of 20]: [Training] loss = 1.216296 * 50048, metric = 43.85% * 50048 19.960s (2507.4 samples/s);\n",
      "Finished Epoch[3 of 20]: [Training] loss = 1.025824 * 49920, metric = 36.67% * 49920 19.921s (2505.9 samples/s);\n",
      "Finished Epoch[3 of 20]: [Training] loss = 1.025824 * 49920, metric = 36.67% * 49920 19.921s (2505.9 samples/s);\n",
      "Finished Epoch[3 of 20]: [Training] loss = 1.025824 * 49920, metric = 36.67% * 49920 19.921s (2505.9 samples/s);\n",
      "Finished Epoch[4 of 20]: [Training] loss = 0.893139 * 50048, metric = 31.66% * 50048 19.972s (2505.9 samples/s);\n",
      "Finished Epoch[4 of 20]: [Training] loss = 0.893139 * 50048, metric = 31.66% * 50048 19.971s (2506.0 samples/s);\n",
      "Finished Epoch[4 of 20]: [Training] loss = 0.893139 * 50048, metric = 31.66% * 50048 19.971s (2506.0 samples/s);\n",
      "Finished Epoch[5 of 20]: [Training] loss = 0.790767 * 50048, metric = 27.74% * 50048 19.738s (2535.6 samples/s);\n",
      "Finished Epoch[5 of 20]: [Training] loss = 0.790767 * 50048, metric = 27.74% * 50048 19.738s (2535.6 samples/s);\n",
      "Finished Epoch[5 of 20]: [Training] loss = 0.790767 * 50048, metric = 27.74% * 50048 19.739s (2535.5 samples/s);\n",
      "Finished Epoch[6 of 20]: [Training] loss = 0.724875 * 49920, metric = 25.13% * 49920 19.894s (2509.3 samples/s);\n",
      "Finished Epoch[6 of 20]: [Training] loss = 0.724875 * 49920, metric = 25.13% * 49920 19.894s (2509.3 samples/s);\n",
      "Finished Epoch[6 of 20]: [Training] loss = 0.724875 * 49920, metric = 25.13% * 49920 19.895s (2509.2 samples/s);\n",
      "Finished Epoch[7 of 20]: [Training] loss = 0.667505 * 50048, metric = 23.08% * 50048 20.277s (2468.2 samples/s);\n",
      "Finished Epoch[7 of 20]: [Training] loss = 0.667505 * 50048, metric = 23.08% * 50048 20.278s (2468.1 samples/s);\n",
      "Finished Epoch[7 of 20]: [Training] loss = 0.667505 * 50048, metric = 23.08% * 50048 20.278s (2468.1 samples/s);\n",
      "Finished Epoch[8 of 20]: [Training] loss = 0.627818 * 49920, metric = 21.72% * 49920 20.143s (2478.3 samples/s);\n",
      "Finished Epoch[8 of 20]: [Training] loss = 0.627818 * 49920, metric = 21.72% * 49920 20.143s (2478.3 samples/s);\n",
      "Finished Epoch[8 of 20]: [Training] loss = 0.627818 * 49920, metric = 21.72% * 49920 20.143s (2478.3 samples/s);\n",
      "Finished Epoch[9 of 20]: [Training] loss = 0.586962 * 50048, metric = 20.27% * 50048 19.724s (2537.4 samples/s);\n",
      "Finished Epoch[9 of 20]: [Training] loss = 0.586962 * 50048, metric = 20.27% * 50048 19.724s (2537.4 samples/s);\n",
      "Finished Epoch[9 of 20]: [Training] loss = 0.586962 * 50048, metric = 20.27% * 50048 19.725s (2537.3 samples/s);\n",
      "Finished Epoch[10 of 20]: [Training] loss = 0.558493 * 50048, metric = 19.34% * 50048 19.846s (2521.8 samples/s);\n",
      "Finished Epoch[10 of 20]: [Training] loss = 0.558493 * 50048, metric = 19.34% * 50048 19.846s (2521.8 samples/s);\n",
      "Finished Epoch[10 of 20]: [Training] loss = 0.558493 * 50048, metric = 19.34% * 50048 19.846s (2521.8 samples/s);\n",
      "Finished Epoch[11 of 20]: [Training] loss = 0.541807 * 49920, metric = 18.67% * 49920 19.794s (2522.0 samples/s);\n",
      "Finished Epoch[11 of 20]: [Training] loss = 0.541807 * 49920, metric = 18.67% * 49920 19.793s (2522.1 samples/s);\n",
      "Finished Epoch[11 of 20]: [Training] loss = 0.541807 * 49920, metric = 18.67% * 49920 19.793s (2522.1 samples/s);\n",
      "Finished Epoch[12 of 20]: [Training] loss = 0.516149 * 50048, metric = 17.82% * 50048 20.045s (2496.8 samples/s);\n",
      "Finished Epoch[12 of 20]: [Training] loss = 0.516149 * 50048, metric = 17.82% * 50048 20.045s (2496.8 samples/s);\n",
      "Finished Epoch[12 of 20]: [Training] loss = 0.516149 * 50048, metric = 17.82% * 50048 20.045s (2496.8 samples/s);\n",
      "Finished Epoch[13 of 20]: [Training] loss = 0.495774 * 50048, metric = 17.13% * 50048 20.018s (2500.1 samples/s);\n",
      "Finished Epoch[13 of 20]: [Training] loss = 0.495774 * 50048, metric = 17.13% * 50048 20.018s (2500.1 samples/s);\n",
      "Finished Epoch[13 of 20]: [Training] loss = 0.495774 * 50048, metric = 17.13% * 50048 20.018s (2500.1 samples/s);\n",
      "Finished Epoch[14 of 20]: [Training] loss = 0.491014 * 49920, metric = 16.96% * 49920 20.163s (2475.8 samples/s);\n",
      "Finished Epoch[14 of 20]: [Training] loss = 0.491014 * 49920, metric = 16.96% * 49920 20.162s (2475.9 samples/s);\n",
      "Finished Epoch[14 of 20]: [Training] loss = 0.491014 * 49920, metric = 16.96% * 49920 20.162s (2475.9 samples/s);\n",
      "Finished Epoch[15 of 20]: [Training] loss = 0.471330 * 50048, metric = 16.13% * 50048 20.308s (2464.4 samples/s);\n",
      "Finished Epoch[15 of 20]: [Training] loss = 0.471330 * 50048, metric = 16.13% * 50048 20.308s (2464.4 samples/s);\n",
      "Finished Epoch[15 of 20]: [Training] loss = 0.471330 * 50048, metric = 16.13% * 50048 20.308s (2464.4 samples/s);\n",
      "Finished Epoch[16 of 20]: [Training] loss = 0.451230 * 49920, metric = 15.63% * 49920 20.203s (2470.9 samples/s);\n",
      "Finished Epoch[16 of 20]: [Training] loss = 0.451230 * 49920, metric = 15.63% * 49920 20.203s (2470.9 samples/s);\n",
      "Finished Epoch[16 of 20]: [Training] loss = 0.451230 * 49920, metric = 15.63% * 49920 20.203s (2470.9 samples/s);\n",
      "Finished Epoch[17 of 20]: [Training] loss = 0.441581 * 50048, metric = 15.18% * 50048 20.071s (2493.5 samples/s);\n",
      "Finished Epoch[17 of 20]: [Training] loss = 0.441581 * 50048, metric = 15.18% * 50048 20.071s (2493.5 samples/s);\n",
      "Finished Epoch[17 of 20]: [Training] loss = 0.441581 * 50048, metric = 15.18% * 50048 20.072s (2493.4 samples/s);\n",
      "Finished Epoch[18 of 20]: [Training] loss = 0.427989 * 50048, metric = 14.91% * 50048 19.636s (2548.8 samples/s);\n",
      "Finished Epoch[18 of 20]: [Training] loss = 0.427989 * 50048, metric = 14.91% * 50048 19.636s (2548.8 samples/s);\n",
      "Finished Epoch[18 of 20]: [Training] loss = 0.427989 * 50048, metric = 14.91% * 50048 19.637s (2548.7 samples/s);\n",
      "Finished Epoch[19 of 20]: [Training] loss = 0.426834 * 49920, metric = 14.72% * 49920 20.081s (2485.9 samples/s);\n",
      "Finished Epoch[19 of 20]: [Training] loss = 0.426834 * 49920, metric = 14.72% * 49920 20.082s (2485.8 samples/s);\n",
      "Finished Epoch[19 of 20]: [Training] loss = 0.426834 * 49920, metric = 14.72% * 49920 20.083s (2485.7 samples/s);\n",
      "Finished Epoch[20 of 20]: [Training] loss = 0.431178 * 49984, metric = 14.70% * 49984 26.554s (1882.4 samples/s);\n",
      "Finished Epoch[20 of 20]: [Training] loss = 0.431178 * 49984, metric = 14.70% * 49984 26.556s (1882.2 samples/s);\n",
      "Finished Epoch[20 of 20]: [Training] loss = 0.431178 * 49984, metric = 14.70% * 49984 26.555s (1882.3 samples/s);\n",
      "Finished Evaluation [1]: Minibatch[1-667]: metric = 27.40% * 10000;\n",
      "Finished Evaluation [1]: Minibatch[1-667]: metric = 27.40% * 10000;\n",
      "Finished Evaluation [1]: Minibatch[1-667]: metric = 27.40% * 10000;\n",
      "\n"
     ]
    }
   ],
   "source": [
    "shipyard jobs add --tail stdout.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the command below we can check the status of our jobs. Once all jobs have an exit code we can continue. You can also view the **heatmap** of this pool on [Azure Portal](https://portal.azure.com) to monitor the progress of this job on the compute nodes under your Batch account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Multi-instance Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleting multi-instance jobs running as Docker containers requires a little more care. We will need to first ensure that the job has entered `completed` state. In the above `jobs` configuration, we set `auto_complete` to `True` enabling the Batch service to automatically complete the job when all tasks finish. This also allows automatic cleanup of the running Docker containers used for executing the MPI job.\n",
    "\n",
    "Special logic is required to cleanup MPI jobs since the `coordination_command` that runs actually detaches an SSH server. The job auto completion logic Batch Shipyard injects ensures that these containers are killed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-11 13:10:34,253 INFO - list of tasks for job cntk-mpi-job\r\n",
      "* task id: task-00000\r\n",
      "  * job id: cntk-mpi-job\r\n",
      "  * state: TaskState.completed\r\n",
      "  * max retries: 0\r\n",
      "  * retention time: 10675199 days, 2:48:05.477581\r\n",
      "  * execution details:\r\n",
      "    * pool id: gpupool-multi-instance\r\n",
      "    * node id: tvm-1783593343_3-20170811t122122z\r\n",
      "    * started: 2017-08-11 12:40:59.639256+00:00\r\n",
      "    * completed: 2017-08-11 12:47:56.294652+00:00\r\n",
      "    * duration: 0:06:56.655396\r\n",
      "    * exit code: 0\r\n"
     ]
    }
   ],
   "source": [
    "shipyard jobs listtasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we are sure that the job is completed, then we issue the standard delete command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-11 13:10:40,927 DEBUG - Skipping termination of completed task task-00000 on job cntk-mpi-job\n",
      "2017-08-11 13:10:41,160 INFO - deleting job: cntk-mpi-job\n",
      "2017-08-11 13:10:41,433 DEBUG - waiting for job cntk-mpi-job to delete\n",
      "2017-08-11 13:10:42,827 INFO - job cntk-mpi-job does not exist\n"
     ]
    }
   ],
   "source": [
    "shipyard jobs del -y --termtasks --wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-11 13:10:44,523 INFO - Deleting pool: gpupool-multi-instance\n",
      "2017-08-11 13:10:44,775 DEBUG - clearing table (pk=batch71d77646ba$gpupool-multi-instance): shipyarddht\n",
      "2017-08-11 13:10:45,013 DEBUG - clearing table (pk=batch71d77646ba$gpupool-multi-instance): shipyardtorrentinfo\n",
      "2017-08-11 13:10:45,051 DEBUG - clearing table (pk=batch71d77646ba$gpupool-multi-instance): shipyardgr\n",
      "2017-08-11 13:10:45,145 DEBUG - clearing table (pk=batch71d77646ba$gpupool-multi-instance): shipyardregistry\n",
      "2017-08-11 13:10:45,225 DEBUG - clearing table (pk=batch71d77646ba$gpupool-multi-instance): shipyardimages\n",
      "2017-08-11 13:10:45,303 DEBUG - clearing table (pk=batch71d77646ba$gpupool-multi-instance): shipyardperf\n",
      "2017-08-11 13:10:45,342 DEBUG - deleting queue: shipyardgr-batch71d77646ba-gpupool-multi-instance\n",
      "2017-08-11 13:10:45,567 DEBUG - deleting container: shipyardrf-batch71d77646ba-gpupool-multi-instance\n",
      "2017-08-11 13:10:45,758 DEBUG - deleting container: shipyardtor-batch71d77646ba-gpupool-multi-instance\n",
      "2017-08-11 13:10:45,798 DEBUG - deleting container: shipyardgr-batch71d77646ba-gpupool-multi-instance\n",
      "2017-08-11 13:10:45,839 DEBUG - waiting for pool gpupool-multi-instance to delete\n"
     ]
    }
   ],
   "source": [
    "shipyard pool del -y --wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
