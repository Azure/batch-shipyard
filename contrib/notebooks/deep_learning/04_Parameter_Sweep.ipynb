{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV - Parameter Sweep\n",
    "In this notebook we will be running a simple parameter sweep on the model we have. We will then pull the results of our sweep and based on the results of our sweep pull the best performing model from blob.\n",
    "\n",
    "* [Setup](#section1)\n",
    "* [Configure job](#section2)\n",
    "* [Submit job](#section3)\n",
    "* [Check results](#section4)\n",
    "* [Download best model](#section5)\n",
    "* [Delete job](#section6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a simple alias for Batch Shipyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%alias shipyard SHIPYARD_CONFIGDIR=config python $HOME/batch-shipyard/shipyard.py %l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that everything is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: shipyard.py [OPTIONS] COMMAND [ARGS]...\r\n",
      "\r\n",
      "  Batch Shipyard: Provision and Execute Docker Workloads on Azure Batch\r\n",
      "\r\n",
      "Options:\r\n",
      "  --version   Show the version and exit.\r\n",
      "  -h, --help  Show this message and exit.\r\n",
      "\r\n",
      "Commands:\r\n",
      "  cert      Certificate actions\r\n",
      "  data      Data actions\r\n",
      "  fs        Filesystem in Azure actions\r\n",
      "  jobs      Jobs actions\r\n",
      "  keyvault  KeyVault actions\r\n",
      "  misc      Miscellaneous actions\r\n",
      "  pool      Pool actions\r\n",
      "  storage   Storage actions\r\n"
     ]
    }
   ],
   "source": [
    "shipyard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the account information we saved earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_json(filename):\n",
    "    with open(filename, 'r') as infile:\n",
    "        return json.load(infile)\n",
    "    \n",
    "account_info = read_json('account_information.json')\n",
    "\n",
    "storage_account_key = account_info['storage_account_key']\n",
    "storage_account_name = account_info['storage_account_name']\n",
    "IMAGE_NAME = account_info['IMAGE_NAME']\n",
    "STORAGE_ALIAS = account_info['STORAGE_ALIAS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous job we ran on a single node we will be running the job on GPU enabled nodes. The difference here is that we will be using a [task factory](https://github.com/Azure/batch-shipyard/blob/master/docs/35-batch-shipyard-task-factory.md). The task factory will generate a number of tasks, each task will have a different set of parmeters that we will be passing to our model training script. This parameters effect the training of the model and in the end the performance of the model. The model and results of its evaluation are recorded and stored on the node. At the end of the task the results are pulled into the specified storage container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def write_json_to_file(json_dict, filename):\n",
    "    \"\"\" Simple function to write JSON dictionaries to files\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as outfile:\n",
    "        json.dump(json_dict, outfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the `jobs.json` configuration file\n",
    "\n",
    "For the purposes of the tutorial, we are constraining the parameter search space to just 4 final combinations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "JOB_ID = 'cntk-parametricsweep-job'\n",
    "\n",
    "jobs = {\n",
    "    \"job_specifications\": [\n",
    "        {\n",
    "            \"id\": JOB_ID,\n",
    "            \"tasks\": [\n",
    "                        {\n",
    "                            \"image\": IMAGE_NAME,\n",
    "                            \"task_factory\": {\n",
    "                                \"parametric_sweep\": {\n",
    "                                    \"product\": [\n",
    "                                        {#num_convolution_layers\n",
    "                                            \"start\": 2,\n",
    "                                            \"stop\": 4,\n",
    "                                            \"step\": 1\n",
    "                                        },\n",
    "                                        {#minibatch_size\n",
    "                                            \"start\": 32,\n",
    "                                            \"stop\": 96,\n",
    "                                            \"step\": 32\n",
    "                                        }\n",
    "                                    ]\n",
    "                                }\n",
    "                            },\n",
    "                            \"command\": \"bash -c \\\"source /cntk/activate-cntk; python -u ConvNet_CIFAR10.py --datadir $AZ_BATCH_NODE_SHARED_DIR/data --num_convolution_layers {0} --minibatch_size {1} --max_epochs 30\\\"\",\n",
    "                            \"remove_container_after_exit\": True,\n",
    "                            \"gpu\": True,\n",
    "                            \"resource_files\": [\n",
    "                                    {\n",
    "                                        \"file_path\": \"ConvNet_CIFAR10.py\",\n",
    "                                        \"blob_source\": \"https://batchshipyardexamples.blob.core.windows.net/code/ConvNet_CIFAR10.py\",\n",
    "                                        \"file_mode\":'0777'\n",
    "                                    }\n",
    "                            ],\n",
    "                            \"output_data\": {\n",
    "                                \"azure_storage\": [\n",
    "                                    {\n",
    "                                        \"storage_account_settings\": STORAGE_ALIAS,\n",
    "                                        \"container\": \"output\",\n",
    "                                        \"source\": \"$AZ_BATCH_TASK_DIR/wd/Models\"\n",
    "                                    },\n",
    "                                ]\n",
    "                            },\n",
    "                        }\n",
    "            ]  \n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"job_specifications\": [\n",
      "        {\n",
      "            \"id\": \"cntk-parametricsweep-job\", \n",
      "            \"tasks\": [\n",
      "                {\n",
      "                    \"command\": \"bash -c \\\"source /cntk/activate-cntk; python -u ConvNet_CIFAR10.py --datadir $AZ_BATCH_NODE_SHARED_DIR/data --num_convolution_layers {0} --minibatch_size {1} --max_epochs 30\\\"\", \n",
      "                    \"gpu\": true, \n",
      "                    \"image\": \"microsoft/cntk:2.0-gpu-python3.5-cuda8.0-cudnn5.1\", \n",
      "                    \"output_data\": {\n",
      "                        \"azure_storage\": [\n",
      "                            {\n",
      "                                \"container\": \"output\", \n",
      "                                \"source\": \"$AZ_BATCH_TASK_DIR/wd/Models\", \n",
      "                                \"storage_account_settings\": \"mystorageaccount\"\n",
      "                            }\n",
      "                        ]\n",
      "                    }, \n",
      "                    \"remove_container_after_exit\": true, \n",
      "                    \"resource_files\": [\n",
      "                        {\n",
      "                            \"blob_source\": \"https://batchshipyardexamples.blob.core.windows.net/code/ConvNet_CIFAR10.py\", \n",
      "                            \"file_mode\": \"0777\", \n",
      "                            \"file_path\": \"ConvNet_CIFAR10.py\"\n",
      "                        }\n",
      "                    ], \n",
      "                    \"task_factory\": {\n",
      "                        \"parametric_sweep\": {\n",
      "                            \"product\": [\n",
      "                                {\n",
      "                                    \"start\": 2, \n",
      "                                    \"step\": 1, \n",
      "                                    \"stop\": 4\n",
      "                                }, \n",
      "                                {\n",
      "                                    \"start\": 32, \n",
      "                                    \"step\": 32, \n",
      "                                    \"stop\": 96\n",
      "                                }\n",
      "                            ]\n",
      "                        }\n",
      "                    }\n",
      "                }\n",
      "            ]\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "write_json_to_file(jobs, os.path.join('config', 'jobs.json'))\n",
    "print(json.dumps(jobs, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit job\n",
    "Check that everything is ok with our pool before we submit our jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-11 10:19:40,332 INFO - compute nodes for pool gpupool\r\n",
      "* node id: tvm-3257026573_1-20170811t093905z\r\n",
      "  * state: ComputeNodeState.idle\r\n",
      "  * scheduling state: SchedulingState.enabled\r\n",
      "  * no errors\r\n",
      "  * start task:\r\n",
      "    * exit code: 0\r\n",
      "    * started: 2017-08-11 09:41:12.873732+00:00\r\n",
      "    * completed: 2017-08-11 09:48:32.262192+00:00\r\n",
      "    * duration: 0:07:19.388460\r\n",
      "  * vm size: standard_nc6\r\n",
      "  * dedicated: True\r\n",
      "  * ip address: 10.0.0.6\r\n",
      "  * running tasks: 0\r\n",
      "  * total tasks run: 0\r\n",
      "  * total tasks succeeded: 0\r\n",
      "* node id: tvm-3257026573_2-20170811t093905z\r\n",
      "  * state: ComputeNodeState.idle\r\n",
      "  * scheduling state: SchedulingState.enabled\r\n",
      "  * no errors\r\n",
      "  * start task:\r\n",
      "    * exit code: 0\r\n",
      "    * started: 2017-08-11 09:40:34.895513+00:00\r\n",
      "    * completed: 2017-08-11 09:47:46.141084+00:00\r\n",
      "    * duration: 0:07:11.245571\r\n",
      "  * vm size: standard_nc6\r\n",
      "  * dedicated: True\r\n",
      "  * ip address: 10.0.0.5\r\n",
      "  * running tasks: 0\r\n",
      "  * total tasks run: 0\r\n",
      "  * total tasks succeeded: 0\r\n",
      "* node id: tvm-3257026573_3-20170811t093905z\r\n",
      "  * state: ComputeNodeState.idle\r\n",
      "  * scheduling state: SchedulingState.enabled\r\n",
      "  * no errors\r\n",
      "  * start task:\r\n",
      "    * exit code: 0\r\n",
      "    * started: 2017-08-11 09:40:44.741433+00:00\r\n",
      "    * completed: 2017-08-11 09:47:43.821572+00:00\r\n",
      "    * duration: 0:06:59.080139\r\n",
      "  * vm size: standard_nc6\r\n",
      "  * dedicated: True\r\n",
      "  * ip address: 10.0.0.4\r\n",
      "  * running tasks: 0\r\n",
      "  * total tasks run: 0\r\n",
      "  * total tasks succeeded: 0\r\n"
     ]
    }
   ],
   "source": [
    "shipyard pool listnodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have confirmed everything is working we can execute our job using the command below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-11 10:19:49,006 INFO - Adding job cntk-parametricsweep-job to pool gpupool\n",
      "2017-08-11 10:19:50,268 INFO - uploading file /tmp/tmpvMVlvn as u'shipyardtaskrf-cntk-parametricsweep-job/0.shipyard.envlist'\n",
      "2017-08-11 10:19:50,950 INFO - uploading file /tmp/tmpOF_qOz as u'shipyardtaskrf-cntk-parametricsweep-job/1.shipyard.envlist'\n",
      "2017-08-11 10:19:51,646 INFO - uploading file /tmp/tmpffi81f as u'shipyardtaskrf-cntk-parametricsweep-job/2.shipyard.envlist'\n",
      "2017-08-11 10:19:52,511 INFO - uploading file /tmp/tmpSwIVwP as u'shipyardtaskrf-cntk-parametricsweep-job/3.shipyard.envlist'\n",
      "2017-08-11 10:19:52,759 DEBUG - submitting 4 tasks (0 -> 3) to job cntk-parametricsweep-job\n",
      "2017-08-11 10:19:53,259 INFO - submitted all 4 tasks to job cntk-parametricsweep-job\n"
     ]
    }
   ],
   "source": [
    "shipyard jobs add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the command below we can check the status of our job. Only after all tasks have an exit code can we continue with the notebook. Please keep re-running the cell below periodically until you see that all tasks show completed state with an exit code. Continuing on with the notebook without all tasks in the job completing their training execution will result in failure in subsequent cells.\n",
    "\n",
    "You can also view the **heatmap** of this pool on [Azure Portal](https://portal.azure.com) to monitor the progress of this job on the compute nodes under your Batch account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-11 10:39:39,162 INFO - list of tasks for job cntk-parametricsweep-job\r\n",
      "* task id: 0\r\n",
      "  * job id: cntk-parametricsweep-job\r\n",
      "  * state: TaskState.completed\r\n",
      "  * max retries: 0\r\n",
      "  * retention time: 10675199 days, 2:48:05.477581\r\n",
      "  * execution details:\r\n",
      "    * pool id: gpupool\r\n",
      "    * node id: tvm-3257026573_1-20170811t093905z\r\n",
      "    * started: 2017-08-11 10:19:54.480387+00:00\r\n",
      "    * completed: 2017-08-11 10:30:18.602361+00:00\r\n",
      "    * duration: 0:10:24.121974\r\n",
      "    * exit code: 0\r\n",
      "* task id: 1\r\n",
      "  * job id: cntk-parametricsweep-job\r\n",
      "  * state: TaskState.completed\r\n",
      "  * max retries: 0\r\n",
      "  * retention time: 10675199 days, 2:48:05.477581\r\n",
      "  * execution details:\r\n",
      "    * pool id: gpupool\r\n",
      "    * node id: tvm-3257026573_2-20170811t093905z\r\n",
      "    * started: 2017-08-11 10:19:54.388684+00:00\r\n",
      "    * completed: 2017-08-11 10:27:55.844296+00:00\r\n",
      "    * duration: 0:08:01.455612\r\n",
      "    * exit code: 0\r\n",
      "* task id: 2\r\n",
      "  * job id: cntk-parametricsweep-job\r\n",
      "  * state: TaskState.completed\r\n",
      "  * max retries: 0\r\n",
      "  * retention time: 10675199 days, 2:48:05.477581\r\n",
      "  * execution details:\r\n",
      "    * pool id: gpupool\r\n",
      "    * node id: tvm-3257026573_2-20170811t093905z\r\n",
      "    * started: 2017-08-11 10:27:57.611529+00:00\r\n",
      "    * completed: 2017-08-11 10:39:25.916624+00:00\r\n",
      "    * duration: 0:11:28.305095\r\n",
      "    * exit code: 0\r\n",
      "* task id: 3\r\n",
      "  * job id: cntk-parametricsweep-job\r\n",
      "  * state: TaskState.completed\r\n",
      "  * max retries: 0\r\n",
      "  * retention time: 10675199 days, 2:48:05.477581\r\n",
      "  * execution details:\r\n",
      "    * pool id: gpupool\r\n",
      "    * node id: tvm-3257026573_3-20170811t093905z\r\n",
      "    * started: 2017-08-11 10:19:54.449579+00:00\r\n",
      "    * completed: 2017-08-11 10:29:03.276617+00:00\r\n",
      "    * duration: 0:09:08.827038\r\n",
      "    * exit code: 0\r\n"
     ]
    }
   ],
   "source": [
    "shipyard jobs listtasks --jobid $JOB_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check results\n",
    "The results of our parameter search should now be saved to our output container.\n",
    "\n",
    "**Note:** You will encounter errors if you did not wait for all tasks to complete with an exit code in the previous cell.\n",
    "\n",
    "First let's alias `blobxfer` to aid in downloading our blobs. We will aggregate our results in the `MODELS_DIR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%alias blobxfer python -m blobxfer\n",
    "\n",
    "MODELS_DIR = 'psmodels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      " azure blobxfer parameters [v0.12.1]\n",
      "=====================================\n",
      "             platform: Linux-4.4.0-87-generic-x86_64-with-debian-stretch-sid\n",
      "   python interpreter: CPython 2.7.11\n",
      "     package versions: az.common=1.1.8 az.sml=0.20.5 az.stor=0.35.1 crypt=2.0.3 req=2.18.3\n",
      "      subscription id: None\n",
      "      management cert: None\n",
      "   transfer direction: Azure->local\n",
      "       local resource: psmodels\n",
      "      include pattern: *_cntk-parametricsweep-job/model_results.json\n",
      "      remote resource: .\n",
      "   max num of workers: 24\n",
      "              timeout: None\n",
      "      storage account: batch71d77646st\n",
      "              use SAS: False\n",
      "  upload as page blob: False\n",
      "  auto vhd->page blob: False\n",
      " upload to file share: False\n",
      " container/share name: output\n",
      "  container/share URI: https://batch71d77646st.blob.core.windows.net/output\n",
      "    compute block MD5: False\n",
      "     compute file MD5: True\n",
      "    skip on MD5 match: True\n",
      "   chunk size (bytes): 4194304\n",
      "     create container: True\n",
      "  keep mismatched MD5: False\n",
      "     recursive if dir: True\n",
      "component strip on up: 1\n",
      "        remote delete: False\n",
      "           collate to: disabled\n",
      "      local overwrite: True\n",
      "      encryption mode: disabled\n",
      "         RSA key file: disabled\n",
      "         RSA key type: disabled\n",
      "=======================================\n",
      "\n",
      "script start time: 2017-08-11 10:39:52\n",
      "attempting to copy entire container output to psmodels\n",
      "generating local directory structure and pre-allocating space\n",
      "created local directory: psmodels/91f65623_task-00003_cntk-parametricsweep-job\n",
      "remote blob: 91f65623_task-00003_cntk-parametricsweep-job/model_results.json length: 108 bytes, md5: AT4+4ufwdBxCF+TFMZLo3g==\n",
      "created local directory: psmodels/6834b826_3_cntk-parametricsweep-job\n",
      "remote blob: 6834b826_3_cntk-parametricsweep-job/model_results.json length: 108 bytes, md5: QMHj9NI6OZbTFuMvUhBfmQ==\n",
      "created local directory: psmodels/09ec4f8d_task-00001_cntk-parametricsweep-job\n",
      "remote blob: 09ec4f8d_task-00001_cntk-parametricsweep-job/model_results.json length: 108 bytes, md5: uqjcZl/xDgFk6DwzOwfkag==\n",
      "created local directory: psmodels/6c24fc0b_1_cntk-parametricsweep-job\n",
      "remote blob: 6c24fc0b_1_cntk-parametricsweep-job/model_results.json length: 108 bytes, md5: OldRtP3AjEE7afrotB/vWw==\n",
      "created local directory: psmodels/83fe081c_task-00002_cntk-parametricsweep-job\n",
      "remote blob: 83fe081c_task-00002_cntk-parametricsweep-job/model_results.json length: 108 bytes, md5: mFzbh3THr5tKYFYeehttvA==\n",
      "created local directory: psmodels/270d141e_0_cntk-parametricsweep-job\n",
      "remote blob: 270d141e_0_cntk-parametricsweep-job/model_results.json length: 108 bytes, md5: gbodWA8Pw2+R/d1GJDTXMw==\n",
      "created local directory: psmodels/f85c1d1a_task-00000_cntk-parametricsweep-job\n",
      "remote blob: f85c1d1a_task-00000_cntk-parametricsweep-job/model_results.json length: 108 bytes, md5: dB6UpUCsbpsIBseM4M4p6A==\n",
      "created local directory: psmodels/80534ba8_2_cntk-parametricsweep-job\n",
      "remote blob: 80534ba8_2_cntk-parametricsweep-job/model_results.json length: 108 bytes, md5: SaINuajO1bHQPbEgdxukVg==\n",
      "performing 8 range-gets\n",
      "spawning 8 worker threads\n",
      "xfer progress: [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100.00%    1311.02 range-gets/min    \n",
      "\n",
      "0.000823974609375 MiB transfered, elapsed 0.366124153137 sec. Throughput = 0.0180042666361 Mbit/sec\n",
      "\n",
      "performing finalization (if applicable): HMAC-SHA256: False, MD5: True\n",
      "[MD5: OK, psmodels/09ec4f8d_task-00001_cntk-parametricsweep-job/model_results.json] uqjcZl/xDgFk6DwzOwfkag== <L..R> uqjcZl/xDgFk6DwzOwfkag==\n",
      "[MD5: OK, psmodels/f85c1d1a_task-00000_cntk-parametricsweep-job/model_results.json] dB6UpUCsbpsIBseM4M4p6A== <L..R> dB6UpUCsbpsIBseM4M4p6A==\n",
      "[MD5: OK, psmodels/91f65623_task-00003_cntk-parametricsweep-job/model_results.json] AT4+4ufwdBxCF+TFMZLo3g== <L..R> AT4+4ufwdBxCF+TFMZLo3g==\n",
      "[MD5: OK, psmodels/80534ba8_2_cntk-parametricsweep-job/model_results.json] SaINuajO1bHQPbEgdxukVg== <L..R> SaINuajO1bHQPbEgdxukVg==\n",
      "[MD5: OK, psmodels/83fe081c_task-00002_cntk-parametricsweep-job/model_results.json] mFzbh3THr5tKYFYeehttvA== <L..R> mFzbh3THr5tKYFYeehttvA==\n",
      "[MD5: OK, psmodels/270d141e_0_cntk-parametricsweep-job/model_results.json] gbodWA8Pw2+R/d1GJDTXMw== <L..R> gbodWA8Pw2+R/d1GJDTXMw==\n",
      "[MD5: OK, psmodels/6834b826_3_cntk-parametricsweep-job/model_results.json] QMHj9NI6OZbTFuMvUhBfmQ== <L..R> QMHj9NI6OZbTFuMvUhBfmQ==\n",
      "[MD5: OK, psmodels/6c24fc0b_1_cntk-parametricsweep-job/model_results.json] OldRtP3AjEE7afrotB/vWw== <L..R> OldRtP3AjEE7afrotB/vWw==\n",
      "finalization complete.\n",
      "\n",
      "script elapsed time: 0.917708158493 sec\n",
      "script end time: 2017-08-11 10:39:53\n"
     ]
    }
   ],
   "source": [
    "blobxfer $storage_account_name output $MODELS_DIR --remoteresource . --download --include \"*_$JOB_ID/model_results.json\" --storageaccountkey $storage_account_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will combine all of the `model_results.json` files into one dictionary for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"09ec4f8d_task-00001_cntk-parametricsweep-job\": {\n",
      "        \"parameters\": {\n",
      "            \"max_epochs\": 30, \n",
      "            \"minibatch_size\": 64, \n",
      "            \"num_convolution_layers\": 2\n",
      "        }, \n",
      "        \"test_metric\": 0.2509\n",
      "    }, \n",
      "    \"270d141e_0_cntk-parametricsweep-job\": {\n",
      "        \"parameters\": {\n",
      "            \"max_epochs\": 30, \n",
      "            \"minibatch_size\": 32, \n",
      "            \"num_convolution_layers\": 2\n",
      "        }, \n",
      "        \"test_metric\": 0.2339\n",
      "    }, \n",
      "    \"6834b826_3_cntk-parametricsweep-job\": {\n",
      "        \"parameters\": {\n",
      "            \"max_epochs\": 30, \n",
      "            \"minibatch_size\": 64, \n",
      "            \"num_convolution_layers\": 3\n",
      "        }, \n",
      "        \"test_metric\": 0.2349\n",
      "    }, \n",
      "    \"6c24fc0b_1_cntk-parametricsweep-job\": {\n",
      "        \"parameters\": {\n",
      "            \"max_epochs\": 30, \n",
      "            \"minibatch_size\": 64, \n",
      "            \"num_convolution_layers\": 2\n",
      "        }, \n",
      "        \"test_metric\": 0.2293\n",
      "    }, \n",
      "    \"80534ba8_2_cntk-parametricsweep-job\": {\n",
      "        \"parameters\": {\n",
      "            \"max_epochs\": 30, \n",
      "            \"minibatch_size\": 32, \n",
      "            \"num_convolution_layers\": 3\n",
      "        }, \n",
      "        \"test_metric\": 0.2212\n",
      "    }, \n",
      "    \"83fe081c_task-00002_cntk-parametricsweep-job\": {\n",
      "        \"parameters\": {\n",
      "            \"max_epochs\": 30, \n",
      "            \"minibatch_size\": 32, \n",
      "            \"num_convolution_layers\": 3\n",
      "        }, \n",
      "        \"test_metric\": 0.2241\n",
      "    }, \n",
      "    \"91f65623_task-00003_cntk-parametricsweep-job\": {\n",
      "        \"parameters\": {\n",
      "            \"max_epochs\": 30, \n",
      "            \"minibatch_size\": 64, \n",
      "            \"num_convolution_layers\": 3\n",
      "        }, \n",
      "        \"test_metric\": 0.2297\n",
      "    }, \n",
      "    \"f85c1d1a_task-00000_cntk-parametricsweep-job\": {\n",
      "        \"parameters\": {\n",
      "            \"max_epochs\": 30, \n",
      "            \"minibatch_size\": 32, \n",
      "            \"num_convolution_layers\": 2\n",
      "        }, \n",
      "        \"test_metric\": 0.2297\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def scandir(basedir):\n",
    "    for root, dirs, files in os.walk(basedir):\n",
    "        for f in files:\n",
    "            yield os.path.join(root, f) \n",
    "\n",
    "results_dict = {}\n",
    "for model in scandir(MODELS_DIR):\n",
    "    if not model.endswith('.json'):\n",
    "        continue\n",
    "    key = model.split(os.sep)[1]\n",
    "    results_dict[key] = read_json(model)\n",
    "    \n",
    "print(json.dumps(results_dict, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the aggregated results dictionary, we select the one with the smallest error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task with smallest error: 80534ba8_2_cntk-parametricsweep-job (0.2212)\n"
     ]
    }
   ],
   "source": [
    "tuple_min_error = min(results_dict.iteritems(), key=lambda x: x[1]['test_metric'])\n",
    "configuration_with_min_error = tuple_min_error[0]\n",
    "print('task with smallest error: {} ({})'.format(configuration_with_min_error, tuple_min_error[1]['test_metric']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download best model\n",
    "Now we'll download the corresponding best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80534ba8_2_cntk-parametricsweep-job/ConvNet_CIFAR10_model.dnn\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'ConvNet_CIFAR10_model.dnn'\n",
    "BEST_MODEL_BLOB_NAME = '{}/{}'.format(configuration_with_min_error, MODEL_NAME)\n",
    "print(BEST_MODEL_BLOB_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      " azure blobxfer parameters [v0.12.1]\n",
      "=====================================\n",
      "             platform: Linux-4.4.0-87-generic-x86_64-with-debian-stretch-sid\n",
      "   python interpreter: CPython 2.7.11\n",
      "     package versions: az.common=1.1.8 az.sml=0.20.5 az.stor=0.35.1 crypt=2.0.3 req=2.18.3\n",
      "      subscription id: None\n",
      "      management cert: None\n",
      "   transfer direction: Azure->local\n",
      "       local resource: psmodels\n",
      "      include pattern: None\n",
      "      remote resource: 80534ba8_2_cntk-parametricsweep-job/ConvNet_CIFAR10_model.dnn\n",
      "   max num of workers: 24\n",
      "              timeout: None\n",
      "      storage account: batch71d77646st\n",
      "              use SAS: False\n",
      "  upload as page blob: False\n",
      "  auto vhd->page blob: False\n",
      " upload to file share: False\n",
      " container/share name: output\n",
      "  container/share URI: https://batch71d77646st.blob.core.windows.net/output\n",
      "    compute block MD5: False\n",
      "     compute file MD5: True\n",
      "    skip on MD5 match: True\n",
      "   chunk size (bytes): 4194304\n",
      "     create container: True\n",
      "  keep mismatched MD5: False\n",
      "     recursive if dir: True\n",
      "component strip on up: 1\n",
      "        remote delete: False\n",
      "           collate to: disabled\n",
      "      local overwrite: True\n",
      "      encryption mode: disabled\n",
      "         RSA key file: disabled\n",
      "         RSA key type: disabled\n",
      "=======================================\n",
      "\n",
      "script start time: 2017-08-11 10:40:43\n",
      "generating local directory structure and pre-allocating space\n",
      "remote blob: 80534ba8_2_cntk-parametricsweep-job/ConvNet_CIFAR10_model.dnn length: 1957354 bytes, md5: rhqFsOnLgPRdgX+vKXKmog==\n",
      "performing 1 range-gets\n",
      "spawning 1 worker threads\n",
      "xfer progress: [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100.00%      66.90 range-gets/min    \n",
      "\n",
      "1.86667823792 MiB transfered, elapsed 0.896901130676 sec. Throughput = 16.6500246154 Mbit/sec\n",
      "\n",
      "performing finalization (if applicable): HMAC-SHA256: False, MD5: True\n",
      "[MD5: OK, psmodels/80534ba8_2_cntk-parametricsweep-job/ConvNet_CIFAR10_model.dnn] rhqFsOnLgPRdgX+vKXKmog== <L..R> rhqFsOnLgPRdgX+vKXKmog==\n",
      "finalization complete.\n",
      "\n",
      "script elapsed time: 1.23469495773 sec\n",
      "script end time: 2017-08-11 10:40:45\n"
     ]
    }
   ],
   "source": [
    "blobxfer $storage_account_name output $MODELS_DIR --remoteresource $BEST_MODEL_BLOB_NAME --download --storageaccountkey $storage_account_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1920\r\n",
      "drwxr-xr-x  2 nbuser nbuser    4096 Aug 11 10:40 ./\r\n",
      "drwx------ 15 nbuser nbuser    4096 Aug 11 09:48 ../\r\n",
      "-rw-r--r--  1 nbuser nbuser 1957354 Aug 11 10:40 ConvNet_CIFAR10_model.dnn\r\n"
     ]
    }
   ],
   "source": [
    "!mv $MODELS_DIR/$configuration_with_min_error/$MODEL_NAME $MODELS_DIR\n",
    "!rm -rf $MODELS_DIR/*_$JOB_ID  # optionally remove all of the temporary result json directories/files\n",
    "!ls -alF $MODELS_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model file (`ConvNet_CIFAR10_model.dnn`) is now ready for use.\n",
    "\n",
    "**Note:** We could have created a Batch task that did the model selection for us using task dependencies. The model selection task would be dependent upon all of the parametric sweep training tasks and would only run after those tasks complete successfully. The Batch task could then proceed with the logic above.\n",
    "\n",
    "Please see the advanced notebook that shows how this is accomplished: [Automatic Model Selection from Parametric Sweep with Task Dependencies](05_Advanced_Auto_Model_Selection.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section6'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To delete the job use the command below. Just be aware that this will get rid of all the files created by the job and tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-11 10:40:55,297 DEBUG - Skipping termination of completed task 0 on job cntk-parametricsweep-job\n",
      "2017-08-11 10:40:55,651 DEBUG - Skipping termination of completed task 1 on job cntk-parametricsweep-job\n",
      "2017-08-11 10:40:56,017 DEBUG - Skipping termination of completed task 2 on job cntk-parametricsweep-job\n",
      "2017-08-11 10:40:56,188 DEBUG - Skipping termination of completed task 3 on job cntk-parametricsweep-job\n",
      "2017-08-11 10:40:56,375 INFO - deleting job: cntk-parametricsweep-job\n",
      "2017-08-11 10:40:56,712 DEBUG - waiting for job cntk-parametricsweep-job to delete\n",
      "2017-08-11 10:41:28,661 INFO - job cntk-parametricsweep-job does not exist\n"
     ]
    }
   ],
   "source": [
    "shipyard jobs del -y --termtasks --wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
