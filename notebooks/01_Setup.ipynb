{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I - Setup\n",
    "This notebook will set up everything for the tutorial. This notebook assumes that nothing has been set up previously and will create everything from scratch. The necessary steps are broken up into the following sections:\n",
    "\n",
    "**Note:** By using these notebooks on GPU VMs, you agree to the [NVIDIA Software License](http://www.nvidia.com/content/DriverDownload-March2009/licence.php?lang=us).\n",
    "\n",
    "* [Install tools and dependencies](#section1)\n",
    "* [Azure account login](#section2)\n",
    "* [Setup](#section3)\n",
    "* [Create Azure resources](#section4)\n",
    "* [Batch Shipyard Configuration](#section5)\n",
    "* [Create Azure Batch Pool](#section6)\n",
    "* [View Resources on Azure Portal or Batch Labs](#section7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install tools and dependencies\n",
    "Install Batch Shipyard and Azure CLI 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/Azure/batch-shipyard.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally you would use `install.sh` or `install.cmd` helper scripts to install, but due to the Notebook environment, we will instead install with the `requirements.txt` file directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -r batch-shipyard/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Azure CLI 2.0 will also be installed to help us in provisioning Azure Batch and Storage accounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -I azure-cli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create an alias for invoking Batch Shipyard which points to a `config` directory which will hold our json config files (this directory will be created at a later step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%alias shipyard SHIPYARD_CONFIGDIR=config python $HOME/batch-shipyard/shipyard.py %l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that everything is working:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shipyard --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure account login\n",
    "The command below will initiate a login to your Azure account. You will see a url to browse to where you will enter the specified code. This will log you into the Azure account within the Azure CLI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!az login -o table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have multiple subscriptions you can select the one you need with the command below. This will not be necessary for your assigned Azure Pass account for the workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selected_subscription = '\"YOUR TEAM SUBSCRIPTION\"' # Replace with the name of your subscription\n",
    "!az account set --subscription $selected_subscription"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** If you cannot login with the Azure CLI, you can create Azure Batch and Storage accounts on the [Azure Portal](https://portal.azure.com).\n",
    "- [Instructions for creating an Azure Batch Account](https://docs.microsoft.com/en-us/azure/batch/batch-account-create-portal#batch-service-mode)\n",
    "- [Instructions for creating an Azure Storage Account](https://docs.microsoft.com/en-us/azure/storage/storage-create-storage-account#create-a-storage-account) (You can create an \"Auto Storage\" account at the same time as your Batch Account on the portal instead)\n",
    "\n",
    "Please pay attention to special instructions regarding Azure Portal created accounts below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will need to register the Azure Batch service as a resource provider for the Azure subscription. The following will do so and poll until the registration process has completed. This will take approximately 30 seconds to complete.\n",
    "\n",
    "**Note:** This registration process needs to be performed only once for the Azure subscription. If you created your Azure Batch account via the Azure Portal, you do not need to perform this action as it is done automatically for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# register resource provider with subscription\n",
    "print('Registering Microsoft.Batch with subscription. Please be patient for this process.')\n",
    "!az provider register -n Microsoft.Batch\n",
    "\n",
    "# poll until registration completed\n",
    "while True:\n",
    "    status = !az provider show -n Microsoft.Batch -o table\n",
    "    if status[-1].split()[-1] == 'Registered':\n",
    "        print('\\n'.join(status))\n",
    "        break\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define the various names for the resources needed to run Azure Batch jobs.\n",
    "\n",
    "**Note:** If you manually created your accounts on the Azure Portal, you will need to modify `GROUP_NAME`, `BATCH_ACCOUNT_NAME` and `STORAGE_ACCOUNT_NAME` accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import uuid\n",
    "import random\n",
    "\n",
    "def write_json_to_file(json_dict, filename):\n",
    "    \"\"\" Simple function to write JSON dictionaries to files\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as outfile:\n",
    "        json.dump(json_dict, outfile)\n",
    "\n",
    "LOCATION = 'eastus' # We are setting everything up in East US\n",
    "                    # Be aware that you need to set things up in a region that has GPU VMs (N-Series)\n",
    "\n",
    "# load base CNTK image from Docker hub\n",
    "IMAGE_NAME = \"microsoft/cntk:2.0-gpu-python3.5-cuda8.0-cudnn5.1\"\n",
    "\n",
    "# Please do not modify below unless you created your accounts on the Azure Portal\n",
    "short_uuid = str(uuid.uuid4())[:8]\n",
    "\n",
    "GROUP_NAME = \"batch{uuid}rg\".format(uuid=short_uuid)\n",
    "BATCH_ACCOUNT_NAME = \"batch{uuid}ba\".format(uuid=short_uuid)\n",
    "STORAGE_ACCOUNT_NAME = \"batch{uuid}st\".format(uuid=short_uuid)\n",
    "STORAGE_ALIAS = \"mystorageaccount\"\n",
    "STORAGE_ENDPOINT = \"core.windows.net\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Azure resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Resource Group\n",
    "Azure encourages the use of resource groups to organise all the Azure components you deploy. That way it is easier to find them but also we can deleted a number of resources simply by deleting the Resource Group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!az group create -n $GROUP_NAME -l $LOCATION -o table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Batch and Storage accounts\n",
    "Here we simply crate the Batch and Storage accounts. Once we have created the accounts we can the use the Azure CLI to query them and obtain the **batch_account_key**, **batch_service_url** and **storage_account_key** which we will need for our Batch Shipyard configuration files later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "json_data = !az storage account create -l $LOCATION -n $STORAGE_ACCOUNT_NAME -g $GROUP_NAME --sku Standard_LRS\n",
    "print('Storage account {} provisioning state: {}'.format(STORAGE_ACCOUNT_NAME, json.loads(''.join(json_data))['provisioningState']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "json_data = !az batch account create -l $LOCATION -n $BATCH_ACCOUNT_NAME -g $GROUP_NAME --storage-account $STORAGE_ACCOUNT_NAME\n",
    "print('Batch account {} provisioning state: {}'.format(BATCH_ACCOUNT_NAME, json.loads(''.join(json_data))['provisioningState']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we retrieve the **batch_account_key**, **batch_service_url** and **storage_account_key** which we will need for the Batch Shipyard configuration files further down.\n",
    "\n",
    "**Note:** If you created your Batch and Storage accounts in the Azure Portal, you will need to retrieve your keys in the Portal. Then set `batch_account_key`, `batch_service_url`, and `storage_account_key` to their appropriate values instead of the Azure CLI callouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json_data = !az batch account keys list -n $BATCH_ACCOUNT_NAME -g $GROUP_NAME\n",
    "batch_account_key = json.loads(''.join(json_data))['primary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json_data = !az batch account list -g $GROUP_NAME\n",
    "batch_service_url = 'https://'+json.loads(''.join(json_data))[0]['accountEndpoint']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json_data = !az storage account keys list -n $STORAGE_ACCOUNT_NAME -g $GROUP_NAME\n",
    "storage_account_key = json.loads(''.join(json_data))[0]['value']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save credentials required for other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "account_information = {\n",
    "    \"IMAGE_NAME\": IMAGE_NAME,\n",
    "    \"LOCATION\": LOCATION,\n",
    "    \"resource_group\": GROUP_NAME,\n",
    "    \"STORAGE_ALIAS\": STORAGE_ALIAS,\n",
    "    \"storage_account_key\": storage_account_key,\n",
    "    \"storage_account_name\": STORAGE_ACCOUNT_NAME,\n",
    "}\n",
    "write_json_to_file(account_information, 'account_information.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Shiyard configuration\n",
    "In order to execute a job on Batch Shipyard you need a minimum of four configuration files. We will set three of them here and leave the job one for later.\n",
    "* [credentials](#credentials)\n",
    "* [configuration](#configuration)\n",
    "* [pool](#pool)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='credentials'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credentials\n",
    "Here we define all the credentials necessary for Batch Shipyard to connect to Azure for resource provisioning and executing our jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "credentials = {\n",
    "    \"credentials\": {\n",
    "        \"batch\": {\n",
    "            \"account_key\": batch_account_key,\n",
    "            \"account_service_url\": batch_service_url\n",
    "        },\n",
    "        \"storage\": {\n",
    "            STORAGE_ALIAS : {\n",
    "                    \"account\": STORAGE_ACCOUNT_NAME,\n",
    "                    \"account_key\": storage_account_key,\n",
    "                    \"endpoint\": STORAGE_ENDPOINT\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='configuration'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "The config mainly contains the configuration for Batch Shipyard. Here we simply define the storage alias that Batch Shipyard should use as well as the Docker image to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"batch_shipyard\": {\n",
    "        \"storage_account_settings\": STORAGE_ALIAS\n",
    "    },\n",
    "    \"global_resources\": {\n",
    "        \"docker_images\": [\n",
    "            IMAGE_NAME\n",
    "        ]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pool'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pool\n",
    "This is where we define the properties of the compute pool we wish to create. The configuration below creates a pool that is made up of a three NC6 VM running Ubuntu 16.04. If you wish to run a job that uses GPU accelerated compute, as we will be doing for these notebooks, then you will need to choose a VM from the NC series. Here we will allocate 3 `STANDARD_NC6` instances.\n",
    "\n",
    "Under resource files we are pulling in two scripts that will be run as part of the pool creation. They simply download and prepare the CIFAR10 data. These files can be found in the github repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "POOL_ID = 'gpupool'\n",
    "\n",
    "pool = {\n",
    "    \"pool_specification\": {\n",
    "        \"id\": POOL_ID,\n",
    "        \"vm_configuration\": {\n",
    "            \"platform_image\": {\n",
    "                \"publisher\": \"Canonical\",\n",
    "                \"offer\": \"UbuntuServer\",\n",
    "                \"sku\": \"16.04-LTS\"\n",
    "            },\n",
    "        },\n",
    "        \"vm_size\": \"STANDARD_NC6\",\n",
    "        \"vm_count\": {\n",
    "            \"dedicated\": 3\n",
    "        },\n",
    "        \"ssh\": {\n",
    "            \"username\": \"docker\"\n",
    "        },\n",
    "        \"reboot_on_start_task_failed\": False,\n",
    "        \"block_until_all_global_resources_loaded\": True,\n",
    "        \"resource_files\": [\n",
    "            {\n",
    "                \"file_path\": \"cifar_data_processing.py\",\n",
    "                \"blob_source\": \"https://batchshipyardexamples.blob.core.windows.net/code/cifar_data_processing.py\",\n",
    "                \"file_mode\":'0777'\n",
    "            },\n",
    "            {\n",
    "                \"file_path\": \"convert_cifar10.sh\",\n",
    "                \"blob_source\": \"https://batchshipyardexamples.blob.core.windows.net/code/convert_cifar10.sh\",\n",
    "                \"file_mode\":'0777'\n",
    "            }\n",
    "        ],\n",
    "         \"additional_node_prep_commands\": [\n",
    "            \"/bin/bash convert_cifar10.sh {} $AZ_BATCH_NODE_SHARED_DIR/data\".format(IMAGE_NAME)\n",
    "        ]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!mkdir -p config # Create config file directory where we will store all our Batch Shipyard configuration files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_json_to_file(credentials, os.path.join('config', 'credentials.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_json_to_file(config, os.path.join('config', 'config.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_json_to_file(pool, os.path.join('config', 'pool.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please run the following cell and copy/paste the information output from the cell into a scratch space (e.g., Notepad), just in case for recovery purposes. Note that this will output credential secrets for your accounts, so do not openly distribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('IMAGE_NAME = \"{}\"'.format(IMAGE_NAME))\n",
    "print('GROUP_NAME = \"{}\"'.format(GROUP_NAME))\n",
    "print('LOCATION = \"{}\"'.format(LOCATION))\n",
    "print('BATCH_ACCOUNT_NAME = \"{}\"'.format(BATCH_ACCOUNT_NAME))\n",
    "print('batch_account_key = \"{}\"'.format(batch_account_key))\n",
    "print('batch_service_url = \"{}\"'.format(batch_service_url))\n",
    "print('STORAGE_ACCOUNT_NAME = \"{}\"'.format(STORAGE_ACCOUNT_NAME))\n",
    "print('STORAGE_ALIAS = \"{}\"'.format(STORAGE_ALIAS))\n",
    "print('storage_account_key = \"{}\"'.format(storage_account_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section6'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Azure Batch Pool\n",
    "Before we do anything we need to create the pool for Batch Shipyard jobs to run on. This can take a little bit of time so please be patient while the compute nodes are allocated from the Azure Cloud and the Docker images are pre-loaded on to the compute nodes.\n",
    "\n",
    "**Note:** As soon as one node enters `ComputeNodeState.idle` then you can proceed to the next notebook. You do not need to wait for all nodes to enter this state to begin the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "shipyard pool add -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the pool is created we can confirm everything by running the command below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shipyard pool listnodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section7'></a>\n",
    "## View Resources on Azure Portal or Batch Labs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want a graphical interface for your Azure Batch resources, you can view them on the [Azure Portal](https://portal.azure.com) or with [Batch Labs](https://github.com/Azure/BatchLabs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Next notebook: Single GPU Training](02_Single_GPU_Training.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
