{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV - Parameter Sweep\n",
    "In this notebook we will be running a simple parameter sweep on the model we have. We will then pull the results of our sweep and based on the results of our sweep pull the best performing model from blob.\n",
    "\n",
    "* [Setup](#section1)\n",
    "* [Configure job](#section2)\n",
    "* [Submit job](#section3)\n",
    "* [Check results](#section4)\n",
    "* [Download best model](#section5)\n",
    "* [Delete job](#section6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a simple alias for Batch Shipyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%alias shipyard SHIPYARD_CONFIGDIR=config python $HOME/batch-shipyard/shipyard.py %l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that everything is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: shipyard.py [OPTIONS] COMMAND [ARGS]...\r\n",
      "\r\n",
      "  Batch Shipyard: Provision and Execute Docker Workloads on Azure Batch\r\n",
      "\r\n",
      "Options:\r\n",
      "  --version   Show the version and exit.\r\n",
      "  -h, --help  Show this message and exit.\r\n",
      "\r\n",
      "Commands:\r\n",
      "  cert      Certificate actions\r\n",
      "  data      Data actions\r\n",
      "  fs        Filesystem in Azure actions\r\n",
      "  jobs      Jobs actions\r\n",
      "  keyvault  KeyVault actions\r\n",
      "  misc      Miscellaneous actions\r\n",
      "  pool      Pool actions\r\n",
      "  storage   Storage actions\r\n"
     ]
    }
   ],
   "source": [
    "shipyard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the account information we saved earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_json(filename):\n",
    "    with open(filename, 'r') as infile:\n",
    "        return json.load(infile)\n",
    "    \n",
    "account_info = read_json('account_information.json')\n",
    "\n",
    "storage_account_key = account_info['storage_account_key']\n",
    "storage_account_name = account_info['storage_account_name']\n",
    "IMAGE_NAME = account_info['IMAGE_NAME']\n",
    "STORAGE_ALIAS = account_info['STORAGE_ALIAS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous job we ran on a single node we will be running the job on GPU enabled nodes. The difference here is that depending on the number of combinations we will be creating the same number of tasks. Each task will have a different set of parmeters that we will be passing to our model training script. This parameters effect the training of the model and in the end the performance of the model. The model and results of its evaluation are recorded and stored on the node. At the end of the task the results are pulled into the specified storage container.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "import os\n",
    "import random\n",
    "from sklearn.grid_search import ParameterGrid\n",
    "from toolz import curry, pipe\n",
    "\n",
    "def write_json_to_file(json_dict, filename):\n",
    "    \"\"\" Simple function to write JSON dictionaries to files\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as outfile:\n",
    "        json.dump(json_dict, outfile)\n",
    "\n",
    "def compose_command(num_convolution_layers, minibatch_size, max_epochs=30):\n",
    "    cmd_str = ' '.join((\"source /cntk/activate-cntk;\",\n",
    "                        \"python -u ConvNet_CIFAR10.py\",\n",
    "                        \"--datadir $AZ_BATCH_NODE_SHARED_DIR/data\",\n",
    "                        \"--num_convolution_layers {num_convolution_layers}\",\n",
    "                        \"--minibatch_size {minibatch_size}\",\n",
    "                        \"--max_epochs {max_epochs}\")).format(num_convolution_layers=num_convolution_layers,\n",
    "                                                             minibatch_size=minibatch_size,\n",
    "                                                             max_epochs=max_epochs)\n",
    "    return 'bash -c \"{}\"'.format(cmd_str)\n",
    "\n",
    "@curry\n",
    "def append_parameter(param_name, param_value, data_dict):\n",
    "    data_dict[param_name]=param_value\n",
    "    return data_dict\n",
    "\n",
    "def task_generator(parameters):\n",
    "    for params in ParameterGrid(parameters):\n",
    "        yield pipe(copy(_TASK_TEMPLATE),\n",
    "                   append_parameter('command', compose_command(**params)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the `jobs.json` configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_TASK_TEMPLATE = {\n",
    "    \"image\": IMAGE_NAME,\n",
    "    \"remove_container_after_exit\": True,\n",
    "    \"gpu\": True,\n",
    "    \"resource_files\": [\n",
    "            {\n",
    "                \"file_path\": \"ConvNet_CIFAR10.py\",\n",
    "                \"blob_source\": \"https://batchshipyardexamples.blob.core.windows.net/code/ConvNet_CIFAR10.py\",\n",
    "                \"file_mode\":'0777'\n",
    "            }\n",
    "    ],\n",
    "    \"output_data\": {\n",
    "        \"azure_storage\": [\n",
    "            {\n",
    "                \"storage_account_settings\": STORAGE_ALIAS,\n",
    "                \"container\": \"output\",\n",
    "                \"source\": \"$AZ_BATCH_TASK_DIR/wd/Models\"\n",
    "            },\n",
    "        ]\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of the workshop, we are constraining the parameter search space to just 3 final combinations (4 will be generated but we will remove the first one such that we only need to wait for \"one round\" of processing. In other words, since we have 3 total compute nodes in the pool and 3 tasks, we only have to wait for \"one round.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"num_convolution_layers\": [2, 3],  # this could be expanded to [2, 3, 4] for example\n",
    "    \"minibatch_size\": [32, 64]         # this could be expanded to [32, 64, 128] for example\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tasks for parametric sweep cntk-parametricsweep-job: 3\n"
     ]
    }
   ],
   "source": [
    "JOB_ID = 'cntk-parametricsweep-job'\n",
    "\n",
    "jobs = {\n",
    "    \"job_specifications\": [\n",
    "        {\n",
    "            \"id\": JOB_ID,\n",
    "            \"tasks\": list(task_generator(parameters))    \n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# for purposes of expediency in the workshop, we'll remove one of the tasks to\n",
    "# make 3 total to match the number of compute nodes in our pool\n",
    "del jobs['job_specifications'][0]['tasks'][0]\n",
    "\n",
    "print('number of tasks for parametric sweep {}: {}'.format(JOB_ID, len(jobs['job_specifications'][0]['tasks'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"job_specifications\": [\n",
      "        {\n",
      "            \"id\": \"cntk-parametricsweep-job\", \n",
      "            \"tasks\": [\n",
      "                {\n",
      "                    \"command\": \"bash -c \\\"source /cntk/activate-cntk; python -u ConvNet_CIFAR10.py --datadir $AZ_BATCH_NODE_SHARED_DIR/data --num_convolution_layers 3 --minibatch_size 32 --max_epochs 30\\\"\", \n",
      "                    \"gpu\": true, \n",
      "                    \"image\": \"microsoft/cntk:2.0-gpu-python3.5-cuda8.0-cudnn5.1\", \n",
      "                    \"output_data\": {\n",
      "                        \"azure_storage\": [\n",
      "                            {\n",
      "                                \"container\": \"output\", \n",
      "                                \"source\": \"$AZ_BATCH_TASK_DIR/wd/Models\", \n",
      "                                \"storage_account_settings\": \"mystorageaccount\"\n",
      "                            }\n",
      "                        ]\n",
      "                    }, \n",
      "                    \"remove_container_after_exit\": true, \n",
      "                    \"resource_files\": [\n",
      "                        {\n",
      "                            \"blob_source\": \"https://batchshipyardexamples.blob.core.windows.net/code/ConvNet_CIFAR10.py\", \n",
      "                            \"file_mode\": \"0777\", \n",
      "                            \"file_path\": \"ConvNet_CIFAR10.py\"\n",
      "                        }\n",
      "                    ]\n",
      "                }, \n",
      "                {\n",
      "                    \"command\": \"bash -c \\\"source /cntk/activate-cntk; python -u ConvNet_CIFAR10.py --datadir $AZ_BATCH_NODE_SHARED_DIR/data --num_convolution_layers 2 --minibatch_size 64 --max_epochs 30\\\"\", \n",
      "                    \"gpu\": true, \n",
      "                    \"image\": \"microsoft/cntk:2.0-gpu-python3.5-cuda8.0-cudnn5.1\", \n",
      "                    \"output_data\": {\n",
      "                        \"azure_storage\": [\n",
      "                            {\n",
      "                                \"container\": \"output\", \n",
      "                                \"source\": \"$AZ_BATCH_TASK_DIR/wd/Models\", \n",
      "                                \"storage_account_settings\": \"mystorageaccount\"\n",
      "                            }\n",
      "                        ]\n",
      "                    }, \n",
      "                    \"remove_container_after_exit\": true, \n",
      "                    \"resource_files\": [\n",
      "                        {\n",
      "                            \"blob_source\": \"https://batchshipyardexamples.blob.core.windows.net/code/ConvNet_CIFAR10.py\", \n",
      "                            \"file_mode\": \"0777\", \n",
      "                            \"file_path\": \"ConvNet_CIFAR10.py\"\n",
      "                        }\n",
      "                    ]\n",
      "                }, \n",
      "                {\n",
      "                    \"command\": \"bash -c \\\"source /cntk/activate-cntk; python -u ConvNet_CIFAR10.py --datadir $AZ_BATCH_NODE_SHARED_DIR/data --num_convolution_layers 3 --minibatch_size 64 --max_epochs 30\\\"\", \n",
      "                    \"gpu\": true, \n",
      "                    \"image\": \"microsoft/cntk:2.0-gpu-python3.5-cuda8.0-cudnn5.1\", \n",
      "                    \"output_data\": {\n",
      "                        \"azure_storage\": [\n",
      "                            {\n",
      "                                \"container\": \"output\", \n",
      "                                \"source\": \"$AZ_BATCH_TASK_DIR/wd/Models\", \n",
      "                                \"storage_account_settings\": \"mystorageaccount\"\n",
      "                            }\n",
      "                        ]\n",
      "                    }, \n",
      "                    \"remove_container_after_exit\": true, \n",
      "                    \"resource_files\": [\n",
      "                        {\n",
      "                            \"blob_source\": \"https://batchshipyardexamples.blob.core.windows.net/code/ConvNet_CIFAR10.py\", \n",
      "                            \"file_mode\": \"0777\", \n",
      "                            \"file_path\": \"ConvNet_CIFAR10.py\"\n",
      "                        }\n",
      "                    ]\n",
      "                }\n",
      "            ]\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "write_json_to_file(jobs, os.path.join('config', 'jobs.json'))\n",
    "print(json.dumps(jobs, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit job\n",
    "Check that everything is ok with our pool before we submit our jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-21 15:17:54,736 DEBUG - listing nodes for pool gpupool\n",
      "2017-06-21 15:17:55,023 INFO - node_id=tvm-4283973576_1-20170621t143056z [state=ComputeNodeState.idle start_task_exit_code=0 scheduling_state=SchedulingState.enabled ip_address=10.0.0.4 vm_size=standard_nc6 dedicated=True total_tasks_run=2 running_tasks_count=0 total_tasks_succeeded=2]\n",
      "2017-06-21 15:17:55,023 INFO - node_id=tvm-4283973576_2-20170621t143056z [state=ComputeNodeState.idle start_task_exit_code=0 scheduling_state=SchedulingState.enabled ip_address=10.0.0.6 vm_size=standard_nc6 dedicated=True total_tasks_run=0 running_tasks_count=0 total_tasks_succeeded=0]\n",
      "2017-06-21 15:17:55,023 INFO - node_id=tvm-4283973576_3-20170621t143056z [state=ComputeNodeState.idle start_task_exit_code=0 scheduling_state=SchedulingState.enabled ip_address=10.0.0.5 vm_size=standard_nc6 dedicated=True total_tasks_run=0 running_tasks_count=0 total_tasks_succeeded=0]\n"
     ]
    }
   ],
   "source": [
    "shipyard pool listnodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have confirmed everything is working we can execute our job using the command below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-21 15:18:01,986 INFO - Adding job cntk-parametricsweep-job to pool gpupool\n",
      "2017-06-21 15:18:02,621 INFO - uploading file /tmp/tmpjv34ca as u'shipyardtaskrf-cntk-parametricsweep-job/dockertask-00000.shipyard.envlist'\n",
      "2017-06-21 15:18:03,086 INFO - uploading file /tmp/tmpdrlMhz as u'shipyardtaskrf-cntk-parametricsweep-job/dockertask-00001.shipyard.envlist'\n",
      "2017-06-21 15:18:03,587 INFO - uploading file /tmp/tmpaXR1Qj as u'shipyardtaskrf-cntk-parametricsweep-job/dockertask-00002.shipyard.envlist'\n",
      "2017-06-21 15:18:03,805 DEBUG - submitting 3 tasks (0 -> 2) to job cntk-parametricsweep-job\n",
      "2017-06-21 15:18:04,055 INFO - submitted all 3 tasks to job cntk-parametricsweep-job\n"
     ]
    }
   ],
   "source": [
    "shipyard jobs add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the command below we can check the status of our job. Only after all tasks have an exit code can we continue with the notebook. Please keep re-running the cell below periodically until you see that all tasks show completed state with an exit code. Continuing on with the notebook without all tasks in the job completing their training execution will result in failure in subsequent cells.\n",
    "\n",
    "You can also view the **heatmap** of this pool on [Azure Portal](https://portal.azure.com) to monitor the progress of this job on the compute nodes under your Batch account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-21 15:19:53,954 INFO - job_id=cntk-parametricsweep-job task_id=dockertask-00000 [state=TaskState.running max_retries=0 retention_time=10675199 days, 2:48:05.477581 pool_id=gpupool node_id=tvm-4283973576_3-20170621t143056z start_time=2017-06-21 15:18:06.619468+00:00 end_time=None duration=n/a exit_code=None]\r\n",
      "2017-06-21 15:19:53,954 INFO - job_id=cntk-parametricsweep-job task_id=dockertask-00001 [state=TaskState.running max_retries=0 retention_time=10675199 days, 2:48:05.477581 pool_id=gpupool node_id=tvm-4283973576_2-20170621t143056z start_time=2017-06-21 15:18:05.822523+00:00 end_time=None duration=n/a exit_code=None]\r\n",
      "2017-06-21 15:19:53,955 INFO - job_id=cntk-parametricsweep-job task_id=dockertask-00002 [state=TaskState.running max_retries=0 retention_time=10675199 days, 2:48:05.477581 pool_id=gpupool node_id=tvm-4283973576_1-20170621t143056z start_time=2017-06-21 15:18:05.827206+00:00 end_time=None duration=n/a exit_code=None]\r\n"
     ]
    }
   ],
   "source": [
    "shipyard jobs listtasks --jobid $JOB_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check results\n",
    "The results of our parameter search should now be saved to our output container.\n",
    "\n",
    "**Note:** You will encounter errors if you did not wait for all tasks to complete with an exit code in the previous cell.\n",
    "\n",
    "First let's alias `blobxfer` to aid in downloading our blobs. We will aggregate our results in the `MODELS_DIR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%alias blobxfer python -m blobxfer\n",
    "\n",
    "MODELS_DIR = 'psmodels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      " azure blobxfer parameters [v0.12.1]\n",
      "=====================================\n",
      "             platform: Linux-4.4.0-75-generic-x86_64-with-debian-stretch-sid\n",
      "   python interpreter: CPython 2.7.11\n",
      "     package versions: az.common=1.1.5 az.sml=0.20.5 az.stor=0.34.2 crypt=1.9 req=2.18.1\n",
      "      subscription id: None\n",
      "      management cert: None\n",
      "   transfer direction: Azure->local\n",
      "       local resource: psmodels\n",
      "      include pattern: *_cntk-parametricsweep-job/model_results.json\n",
      "      remote resource: .\n",
      "   max num of workers: 24\n",
      "              timeout: None\n",
      "      storage account: batche2914cdbst\n",
      "              use SAS: False\n",
      "  upload as page blob: False\n",
      "  auto vhd->page blob: False\n",
      " upload to file share: False\n",
      " container/share name: output\n",
      "  container/share URI: https://batche2914cdbst.blob.core.windows.net/output\n",
      "    compute block MD5: False\n",
      "     compute file MD5: True\n",
      "    skip on MD5 match: True\n",
      "   chunk size (bytes): 4194304\n",
      "     create container: True\n",
      "  keep mismatched MD5: False\n",
      "     recursive if dir: True\n",
      "component strip on up: 1\n",
      "        remote delete: False\n",
      "           collate to: disabled\n",
      "      local overwrite: True\n",
      "      encryption mode: disabled\n",
      "         RSA key file: disabled\n",
      "         RSA key type: disabled\n",
      "=======================================\n",
      "\n",
      "script start time: 2017-06-21 09:59:04\n",
      "attempting to copy entire container output to psmodels\n",
      "generating local directory structure and pre-allocating space\n",
      "created local directory: psmodels/586269dc_dockertask-00001_cntk-parametricsweep-job\n",
      "remote blob: 586269dc_dockertask-00001_cntk-parametricsweep-job/model_results.json length: 108 bytes, md5: I4VdWh/CoCMJhOIw05EJRQ==\n",
      "created local directory: psmodels/b11855a9_dockertask-00002_cntk-parametricsweep-job\n",
      "remote blob: b11855a9_dockertask-00002_cntk-parametricsweep-job/model_results.json length: 108 bytes, md5: jXUPplwnTZJCvHtvtLf/dA==\n",
      "created local directory: psmodels/4fe7fb06_dockertask-00000_cntk-parametricsweep-job\n",
      "remote blob: 4fe7fb06_dockertask-00000_cntk-parametricsweep-job/model_results.json length: 108 bytes, md5: CkeuzHprksoGPyU+ywSrFA==\n",
      "performing 3 range-gets\n",
      "spawning 3 worker threads\n",
      "xfer progress: [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100.00%     720.92 range-gets/min    \n",
      "\n",
      "0.000308990478516 MiB transfered, elapsed 0.249669075012 sec. Throughput = 0.0099008010023 Mbit/sec\n",
      "\n",
      "performing finalization (if applicable): HMAC-SHA256: False, MD5: True\n",
      "[MD5: OK, psmodels/b11855a9_dockertask-00002_cntk-parametricsweep-job/model_results.json] jXUPplwnTZJCvHtvtLf/dA== <L..R> jXUPplwnTZJCvHtvtLf/dA==\n",
      "[MD5: OK, psmodels/586269dc_dockertask-00001_cntk-parametricsweep-job/model_results.json] I4VdWh/CoCMJhOIw05EJRQ== <L..R> I4VdWh/CoCMJhOIw05EJRQ==\n",
      "[MD5: OK, psmodels/4fe7fb06_dockertask-00000_cntk-parametricsweep-job/model_results.json] CkeuzHprksoGPyU+ywSrFA== <L..R> CkeuzHprksoGPyU+ywSrFA==\n",
      "finalization complete.\n",
      "\n",
      "script elapsed time: 0.536843061447 sec\n",
      "script end time: 2017-06-21 09:59:04\n"
     ]
    }
   ],
   "source": [
    "blobxfer $storage_account_name output $MODELS_DIR --remoteresource . --download --include \"*_$JOB_ID/model_results.json\" --storageaccountkey $storage_account_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will combine all of the `model_results.json` files into one dictionary for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"4fe7fb06_dockertask-00000_cntk-parametricsweep-job\": {\n",
      "        \"parameters\": {\n",
      "            \"max_epochs\": 30, \n",
      "            \"minibatch_size\": 32, \n",
      "            \"num_convolution_layers\": 3\n",
      "        }, \n",
      "        \"test_metric\": 0.2188\n",
      "    }, \n",
      "    \"586269dc_dockertask-00001_cntk-parametricsweep-job\": {\n",
      "        \"parameters\": {\n",
      "            \"max_epochs\": 30, \n",
      "            \"minibatch_size\": 64, \n",
      "            \"num_convolution_layers\": 2\n",
      "        }, \n",
      "        \"test_metric\": 0.2284\n",
      "    }, \n",
      "    \"b11855a9_dockertask-00002_cntk-parametricsweep-job\": {\n",
      "        \"parameters\": {\n",
      "            \"max_epochs\": 30, \n",
      "            \"minibatch_size\": 64, \n",
      "            \"num_convolution_layers\": 3\n",
      "        }, \n",
      "        \"test_metric\": 0.2311\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def scandir(basedir):\n",
    "    for root, dirs, files in os.walk(basedir):\n",
    "        for f in files:\n",
    "            yield os.path.join(root, f) \n",
    "\n",
    "results_dict = {}\n",
    "for model in scandir(MODELS_DIR):\n",
    "    if not model.endswith('.json'):\n",
    "        continue\n",
    "    key = model.split(os.sep)[1]\n",
    "    results_dict[key] = read_json(model)\n",
    "    \n",
    "print(json.dumps(results_dict, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the aggregated results dictionary, we select the one with the smallest error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task with smallest error: 4fe7fb06_dockertask-00000_cntk-parametricsweep-job (0.2188)\n"
     ]
    }
   ],
   "source": [
    "tuple_min_error = min(results_dict.iteritems(), key=lambda x: x[1]['test_metric'])\n",
    "configuration_with_min_error = tuple_min_error[0]\n",
    "print('task with smallest error: {} ({})'.format(configuration_with_min_error, tuple_min_error[1]['test_metric']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download best model\n",
    "Now we'll download the corresponding best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4fe7fb06_dockertask-00000_cntk-parametricsweep-job/ConvNet_CIFAR10_model.dnn\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'ConvNet_CIFAR10_model.dnn'\n",
    "BEST_MODEL_BLOB_NAME = '{}/{}'.format(configuration_with_min_error, MODEL_NAME)\n",
    "print(BEST_MODEL_BLOB_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      " azure blobxfer parameters [v0.12.1]\n",
      "=====================================\n",
      "             platform: Linux-4.4.0-75-generic-x86_64-with-debian-stretch-sid\n",
      "   python interpreter: CPython 2.7.11\n",
      "     package versions: az.common=1.1.5 az.sml=0.20.5 az.stor=0.34.2 crypt=1.9 req=2.18.1\n",
      "      subscription id: None\n",
      "      management cert: None\n",
      "   transfer direction: Azure->local\n",
      "       local resource: psmodels\n",
      "      include pattern: None\n",
      "      remote resource: 4fe7fb06_dockertask-00000_cntk-parametricsweep-job/ConvNet_CIFAR10_model.dnn\n",
      "   max num of workers: 24\n",
      "              timeout: None\n",
      "      storage account: batche2914cdbst\n",
      "              use SAS: False\n",
      "  upload as page blob: False\n",
      "  auto vhd->page blob: False\n",
      " upload to file share: False\n",
      " container/share name: output\n",
      "  container/share URI: https://batche2914cdbst.blob.core.windows.net/output\n",
      "    compute block MD5: False\n",
      "     compute file MD5: True\n",
      "    skip on MD5 match: True\n",
      "   chunk size (bytes): 4194304\n",
      "     create container: True\n",
      "  keep mismatched MD5: False\n",
      "     recursive if dir: True\n",
      "component strip on up: 1\n",
      "        remote delete: False\n",
      "           collate to: disabled\n",
      "      local overwrite: True\n",
      "      encryption mode: disabled\n",
      "         RSA key file: disabled\n",
      "         RSA key type: disabled\n",
      "=======================================\n",
      "\n",
      "script start time: 2017-06-21 09:59:15\n",
      "generating local directory structure and pre-allocating space\n",
      "remote blob: 4fe7fb06_dockertask-00000_cntk-parametricsweep-job/ConvNet_CIFAR10_model.dnn length: 1957354 bytes, md5: Ts3EFfnibh6pKh2QYkQsBw==\n",
      "performing 1 range-gets\n",
      "spawning 1 worker threads\n",
      "xfer progress: [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100.00%     133.43 range-gets/min    \n",
      "\n",
      "1.86667823792 MiB transfered, elapsed 0.449681997299 sec. Throughput = 33.2088586891 Mbit/sec\n",
      "\n",
      "performing finalization (if applicable): HMAC-SHA256: False, MD5: True\n",
      "[MD5: OK, psmodels/4fe7fb06_dockertask-00000_cntk-parametricsweep-job/ConvNet_CIFAR10_model.dnn] Ts3EFfnibh6pKh2QYkQsBw== <L..R> Ts3EFfnibh6pKh2QYkQsBw==\n",
      "finalization complete.\n",
      "\n",
      "script elapsed time: 0.646049022675 sec\n",
      "script end time: 2017-06-21 09:59:16\n"
     ]
    }
   ],
   "source": [
    "blobxfer $storage_account_name output $MODELS_DIR --remoteresource $BEST_MODEL_BLOB_NAME --download --storageaccountkey $storage_account_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1920\r\n",
      "drwxr-xr-x  2 nbuser nbuser    4096 Jun 21 09:59 ./\r\n",
      "drwx------ 11 nbuser nbuser    4096 Jun 21 09:45 ../\r\n",
      "-rw-r--r--  1 nbuser nbuser 1957354 Jun 21 09:59 ConvNet_CIFAR10_model.dnn\r\n"
     ]
    }
   ],
   "source": [
    "!mv $MODELS_DIR/$configuration_with_min_error/$MODEL_NAME $MODELS_DIR\n",
    "!rm -rf $MODELS_DIR/*_$JOB_ID  # optionally remove all of the temporary result json directories/files\n",
    "!ls -alF $MODELS_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model file (`ConvNet_CIFAR10_model.dnn`) is now ready for use.\n",
    "\n",
    "**Note:** We could have created a Batch task that did the model selection for us using task dependencies. The model selection task would be dependent upon all of the parametric sweep training tasks and would only run after those tasks complete successfully. The Batch task could then proceed with the logic above.\n",
    "\n",
    "Please see the advanced notebook that shows how this is accomplished: [Automatic Model Selection from Parametric Sweep with Task Dependencies](06_Advanced_Auto_Model_Selection.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section6'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To delete the job use the command below. Just be aware that this will get rid of all the files created by the job and tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-21 15:27:21,415 INFO - Deleting job: cntk-ps-as-job\n",
      "2017-06-21 15:27:21,415 DEBUG - disabling job cntk-ps-as-job first due to task termination\n",
      "2017-06-21 15:27:21,833 ERROR - cntk-ps-as-job job does not exist\n"
     ]
    }
   ],
   "source": [
    "shipyard jobs del -y --termtasks --wait"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "You can proceed to the [Notebook: Clean Up](05_Clean_Up.ipynb) "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
