{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV - Parameter Sweep\n",
    "In this notebook we will be running a simple parameter sweep on the model we have. We will then pull the results of our sweep and based on the results of our sweep pull the best performing model from blob.\n",
    "\n",
    "* [Setup](#section1)\n",
    "* [Configure job](#section2)\n",
    "* [Submit job](#section3)\n",
    "* [Check results](#section4)\n",
    "* [Download best model](#section5)\n",
    "* [Delete job](#section6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a simple alias for Batch Shipyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%alias shipyard SHIPYARD_CONFIGDIR=config python $HOME/batch-shipyard/shipyard.py %l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that everything is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: shipyard.py [OPTIONS] COMMAND [ARGS]...\r\n",
      "\r\n",
      "  Batch Shipyard: Provision and Execute Docker Workloads on Azure Batch\r\n",
      "\r\n",
      "Options:\r\n",
      "  --version   Show the version and exit.\r\n",
      "  -h, --help  Show this message and exit.\r\n",
      "\r\n",
      "Commands:\r\n",
      "  cert      Certificate actions\r\n",
      "  data      Data actions\r\n",
      "  fs        Filesystem in Azure actions\r\n",
      "  jobs      Jobs actions\r\n",
      "  keyvault  KeyVault actions\r\n",
      "  misc      Miscellaneous actions\r\n",
      "  pool      Pool actions\r\n",
      "  storage   Storage actions\r\n"
     ]
    }
   ],
   "source": [
    "shipyard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the account information we saved earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_json(filename):\n",
    "    with open(filename, 'r') as infile:\n",
    "        return json.load(infile)\n",
    "    \n",
    "account_info = read_json('account_information.json')\n",
    "\n",
    "storage_account_key = account_info['storage_account_key']\n",
    "storage_account_name = account_info['storage_account_name']\n",
    "IMAGE_NAME = account_info['IMAGE_NAME']\n",
    "STORAGE_ALIAS = account_info['STORAGE_ALIAS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous job we ran on a single node we will be running the job on GPU enabled nodes. The difference here is that depending on the number of combinations we will be creating the same number of tasks. Each task will have a different set of parmeters that we will be passing to our model training script. This parameters effect the training of the model and in the end the performance of the model. The model and results of its evaluation are recorded and stored on the node. At the end of the task the results are pulled into the specified storage container.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nbuser/anaconda2_410/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/nbuser/anaconda2_410/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from copy import copy\n",
    "import os\n",
    "import random\n",
    "from sklearn.grid_search import ParameterGrid\n",
    "from toolz import curry, pipe\n",
    "\n",
    "def write_json_to_file(json_dict, filename):\n",
    "    \"\"\" Simple function to write JSON dictionaries to files\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as outfile:\n",
    "        json.dump(json_dict, outfile)\n",
    "\n",
    "def compose_command(num_convolution_layers, minibatch_size, max_epochs=30):\n",
    "    cmd_str = ' '.join((\"source /cntk/activate-cntk;\",\n",
    "                        \"python -u ConvNet_CIFAR10.py\",\n",
    "                        \"--datadir $AZ_BATCH_NODE_SHARED_DIR/data\",\n",
    "                        \"--num_convolution_layers {num_convolution_layers}\",\n",
    "                        \"--minibatch_size {minibatch_size}\",\n",
    "                        \"--max_epochs {max_epochs}\")).format(num_convolution_layers=num_convolution_layers,\n",
    "                                                             minibatch_size=minibatch_size,\n",
    "                                                             max_epochs=max_epochs)\n",
    "    return 'bash -c \"{}\"'.format(cmd_str)\n",
    "\n",
    "@curry\n",
    "def append_parameter(param_name, param_value, data_dict):\n",
    "    data_dict[param_name]=param_value\n",
    "    return data_dict\n",
    "\n",
    "def task_generator(parameters):\n",
    "    for params in ParameterGrid(parameters):\n",
    "        yield pipe(copy(_TASK_TEMPLATE),\n",
    "                   append_parameter('command', compose_command(**params)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the `jobs.json` configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_TASK_TEMPLATE = {\n",
    "    \"image\": IMAGE_NAME,\n",
    "    \"remove_container_after_exit\": True,\n",
    "    \"gpu\": True,\n",
    "    \"resource_files\": [\n",
    "            {\n",
    "                \"file_path\": \"ConvNet_CIFAR10.py\",\n",
    "                \"blob_source\": \"https://batchshipyardexamples.blob.core.windows.net/code/ConvNet_CIFAR10.py\",\n",
    "                \"file_mode\":'0777'\n",
    "            }\n",
    "    ],\n",
    "    \"output_data\": {\n",
    "        \"azure_storage\": [\n",
    "            {\n",
    "                \"storage_account_settings\": STORAGE_ALIAS,\n",
    "                \"container\": \"output\",\n",
    "                \"source\": \"$AZ_BATCH_TASK_DIR/wd/Models\"\n",
    "            },\n",
    "        ]\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of the workshop, we are constraining the parameter search space to just 3 final combinations (4 will be generated but we will remove the first one such that we only need to wait for \"one round\" of processing. In other words, since we have 3 total compute nodes in the pool and 3 tasks, we only have to wait for \"one round.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"num_convolution_layers\": [2, 3],  # this could be expanded to [2, 3, 4] for example\n",
    "    \"minibatch_size\": [32, 64]         # this could be expanded to [32, 64, 128] for example\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tasks for parametric sweep cntk-parametricsweep-job: 3\n"
     ]
    }
   ],
   "source": [
    "JOB_ID = 'cntk-parametricsweep-job'\n",
    "\n",
    "jobs = {\n",
    "    \"job_specifications\": [\n",
    "        {\n",
    "            \"id\": JOB_ID,\n",
    "            \"tasks\": list(task_generator(parameters))    \n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# for purposes of expediency in the workshop, we'll remove one of the tasks to\n",
    "# make 3 total to match the number of compute nodes in our pool\n",
    "del jobs['job_specifications'][0]['tasks'][0]\n",
    "\n",
    "print('number of tasks for parametric sweep {}: {}'.format(JOB_ID, len(jobs['job_specifications'][0]['tasks'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"job_specifications\": [\n",
      "        {\n",
      "            \"id\": \"cntk-parametricsweep-job\", \n",
      "            \"tasks\": [\n",
      "                {\n",
      "                    \"command\": \"bash -c \\\"source /cntk/activate-cntk; python -u ConvNet_CIFAR10.py --datadir $AZ_BATCH_NODE_SHARED_DIR/data --num_convolution_layers 3 --minibatch_size 32 --max_epochs 30\\\"\", \n",
      "                    \"gpu\": true, \n",
      "                    \"image\": \"microsoft/cntk:2.0-gpu-python3.5-cuda8.0-cudnn5.1\", \n",
      "                    \"output_data\": {\n",
      "                        \"azure_storage\": [\n",
      "                            {\n",
      "                                \"container\": \"output\", \n",
      "                                \"source\": \"$AZ_BATCH_TASK_DIR/wd/Models\", \n",
      "                                \"storage_account_settings\": \"mystorageaccount\"\n",
      "                            }\n",
      "                        ]\n",
      "                    }, \n",
      "                    \"remove_container_after_exit\": true, \n",
      "                    \"resource_files\": [\n",
      "                        {\n",
      "                            \"blob_source\": \"https://batchshipyardexamples.blob.core.windows.net/code/ConvNet_CIFAR10.py\", \n",
      "                            \"file_mode\": \"0777\", \n",
      "                            \"file_path\": \"ConvNet_CIFAR10.py\"\n",
      "                        }\n",
      "                    ]\n",
      "                }, \n",
      "                {\n",
      "                    \"command\": \"bash -c \\\"source /cntk/activate-cntk; python -u ConvNet_CIFAR10.py --datadir $AZ_BATCH_NODE_SHARED_DIR/data --num_convolution_layers 2 --minibatch_size 64 --max_epochs 30\\\"\", \n",
      "                    \"gpu\": true, \n",
      "                    \"image\": \"microsoft/cntk:2.0-gpu-python3.5-cuda8.0-cudnn5.1\", \n",
      "                    \"output_data\": {\n",
      "                        \"azure_storage\": [\n",
      "                            {\n",
      "                                \"container\": \"output\", \n",
      "                                \"source\": \"$AZ_BATCH_TASK_DIR/wd/Models\", \n",
      "                                \"storage_account_settings\": \"mystorageaccount\"\n",
      "                            }\n",
      "                        ]\n",
      "                    }, \n",
      "                    \"remove_container_after_exit\": true, \n",
      "                    \"resource_files\": [\n",
      "                        {\n",
      "                            \"blob_source\": \"https://batchshipyardexamples.blob.core.windows.net/code/ConvNet_CIFAR10.py\", \n",
      "                            \"file_mode\": \"0777\", \n",
      "                            \"file_path\": \"ConvNet_CIFAR10.py\"\n",
      "                        }\n",
      "                    ]\n",
      "                }, \n",
      "                {\n",
      "                    \"command\": \"bash -c \\\"source /cntk/activate-cntk; python -u ConvNet_CIFAR10.py --datadir $AZ_BATCH_NODE_SHARED_DIR/data --num_convolution_layers 3 --minibatch_size 64 --max_epochs 30\\\"\", \n",
      "                    \"gpu\": true, \n",
      "                    \"image\": \"microsoft/cntk:2.0-gpu-python3.5-cuda8.0-cudnn5.1\", \n",
      "                    \"output_data\": {\n",
      "                        \"azure_storage\": [\n",
      "                            {\n",
      "                                \"container\": \"output\", \n",
      "                                \"source\": \"$AZ_BATCH_TASK_DIR/wd/Models\", \n",
      "                                \"storage_account_settings\": \"mystorageaccount\"\n",
      "                            }\n",
      "                        ]\n",
      "                    }, \n",
      "                    \"remove_container_after_exit\": true, \n",
      "                    \"resource_files\": [\n",
      "                        {\n",
      "                            \"blob_source\": \"https://batchshipyardexamples.blob.core.windows.net/code/ConvNet_CIFAR10.py\", \n",
      "                            \"file_mode\": \"0777\", \n",
      "                            \"file_path\": \"ConvNet_CIFAR10.py\"\n",
      "                        }\n",
      "                    ]\n",
      "                }\n",
      "            ]\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "write_json_to_file(jobs, os.path.join('config', 'jobs.json'))\n",
    "print(json.dumps(jobs, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit job\n",
    "Check that everything is ok with our pool before we submit our jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-07-12 14:04:36,121 DEBUG - listing nodes for pool gpupool\n",
      "2017-07-12 14:04:36,424 INFO - node_id=tvm-1392786932_1-20170712t132611z [state=ComputeNodeState.idle start_task_exit_code=0 scheduling_state=SchedulingState.enabled ip_address=10.0.0.6 vm_size=standard_nc6 dedicated=True total_tasks_run=0 running_tasks_count=0 total_tasks_succeeded=0]\n",
      "2017-07-12 14:04:36,424 INFO - node_id=tvm-1392786932_2-20170712t132611z [state=ComputeNodeState.idle start_task_exit_code=0 scheduling_state=SchedulingState.enabled ip_address=10.0.0.5 vm_size=standard_nc6 dedicated=True total_tasks_run=0 running_tasks_count=0 total_tasks_succeeded=0]\n",
      "2017-07-12 14:04:36,424 INFO - node_id=tvm-1392786932_3-20170712t132611z [state=ComputeNodeState.idle start_task_exit_code=0 scheduling_state=SchedulingState.enabled ip_address=10.0.0.4 vm_size=standard_nc6 dedicated=True total_tasks_run=2 running_tasks_count=0 total_tasks_succeeded=2]\n"
     ]
    }
   ],
   "source": [
    "shipyard pool listnodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have confirmed everything is working we can execute our job using the command below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-07-12 14:04:40,170 INFO - Adding job cntk-parametricsweep-job to pool gpupool\n",
      "2017-07-12 14:04:40,975 INFO - uploading file /tmp/tmpdpLBXc as u'shipyardtaskrf-cntk-parametricsweep-job/dockertask-00000.shipyard.envlist'\n",
      "2017-07-12 14:04:41,500 INFO - uploading file /tmp/tmp0e9zXa as u'shipyardtaskrf-cntk-parametricsweep-job/dockertask-00001.shipyard.envlist'\n",
      "2017-07-12 14:04:42,225 INFO - uploading file /tmp/tmpVymWG5 as u'shipyardtaskrf-cntk-parametricsweep-job/dockertask-00002.shipyard.envlist'\n",
      "2017-07-12 14:04:42,453 DEBUG - submitting 3 tasks (0 -> 2) to job cntk-parametricsweep-job\n",
      "2017-07-12 14:04:42,750 INFO - submitted all 3 tasks to job cntk-parametricsweep-job\n"
     ]
    }
   ],
   "source": [
    "shipyard jobs add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the command below we can check the status of our job. Only after all tasks have an exit code can we continue with the notebook. Please keep re-running the cell below periodically until you see that all tasks show completed state with an exit code. Continuing on with the notebook without all tasks in the job completing their training execution will result in failure in subsequent cells.\n",
    "\n",
    "You can also view the **heatmap** of this pool on [Azure Portal](https://portal.azure.com) to monitor the progress of this job on the compute nodes under your Batch account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-07-12 14:17:58,284 INFO - job_id=cntk-parametricsweep-job task_id=dockertask-00000 [state=TaskState.completed max_retries=0 retention_time=10675199 days, 2:48:05.477581 pool_id=gpupool node_id=tvm-1392786932_3-20170712t132611z start_time=2017-07-12 14:04:43.585806+00:00 end_time=2017-07-12 14:16:18.403235+00:00 duration=0:11:34.817429 exit_code=0]\r\n",
      "2017-07-12 14:17:58,285 INFO - job_id=cntk-parametricsweep-job task_id=dockertask-00001 [state=TaskState.completed max_retries=0 retention_time=10675199 days, 2:48:05.477581 pool_id=gpupool node_id=tvm-1392786932_2-20170712t132611z start_time=2017-07-12 14:04:43.643825+00:00 end_time=2017-07-12 14:12:44.257550+00:00 duration=0:08:00.613725 exit_code=0]\r\n",
      "2017-07-12 14:17:58,285 INFO - job_id=cntk-parametricsweep-job task_id=dockertask-00002 [state=TaskState.completed max_retries=0 retention_time=10675199 days, 2:48:05.477581 pool_id=gpupool node_id=tvm-1392786932_1-20170712t132611z start_time=2017-07-12 14:04:43.751081+00:00 end_time=2017-07-12 14:13:50.294406+00:00 duration=0:09:06.543325 exit_code=0]\r\n"
     ]
    }
   ],
   "source": [
    "shipyard jobs listtasks --jobid $JOB_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check results\n",
    "The results of our parameter search should now be saved to our output container.\n",
    "\n",
    "**Note:** You will encounter errors if you did not wait for all tasks to complete with an exit code in the previous cell.\n",
    "\n",
    "First let's alias `blobxfer` to aid in downloading our blobs. We will aggregate our results in the `MODELS_DIR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%alias blobxfer python -m blobxfer\n",
    "\n",
    "MODELS_DIR = 'psmodels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      " azure blobxfer parameters [v0.12.1]\n",
      "=====================================\n",
      "             platform: Linux-4.4.0-81-generic-x86_64-with-debian-stretch-sid\n",
      "   python interpreter: CPython 2.7.11\n",
      "     package versions: az.common=1.1.5 az.sml=0.20.5 az.stor=0.34.2 crypt=1.9 req=2.18.1\n",
      "      subscription id: None\n",
      "      management cert: None\n",
      "   transfer direction: Azure->local\n",
      "       local resource: psmodels\n",
      "      include pattern: *_cntk-parametricsweep-job/model_results.json\n",
      "      remote resource: .\n",
      "   max num of workers: 24\n",
      "              timeout: None\n",
      "      storage account: batch0e43a94est\n",
      "              use SAS: False\n",
      "  upload as page blob: False\n",
      "  auto vhd->page blob: False\n",
      " upload to file share: False\n",
      " container/share name: output\n",
      "  container/share URI: https://batch0e43a94est.blob.core.windows.net/output\n",
      "    compute block MD5: False\n",
      "     compute file MD5: True\n",
      "    skip on MD5 match: True\n",
      "   chunk size (bytes): 4194304\n",
      "     create container: True\n",
      "  keep mismatched MD5: False\n",
      "     recursive if dir: True\n",
      "component strip on up: 1\n",
      "        remote delete: False\n",
      "           collate to: disabled\n",
      "      local overwrite: True\n",
      "      encryption mode: disabled\n",
      "         RSA key file: disabled\n",
      "         RSA key type: disabled\n",
      "=======================================\n",
      "\n",
      "script start time: 2017-07-12 14:18:04\n",
      "attempting to copy entire container output to psmodels\n",
      "generating local directory structure and pre-allocating space\n",
      "created local directory: psmodels/59254dc2_dockertask-00000_cntk-parametricsweep-job\n",
      "remote blob: 59254dc2_dockertask-00000_cntk-parametricsweep-job/model_results.json length: 108 bytes, md5: 3TMThHwqrsom6A+zfhG0iw==\n",
      "created local directory: psmodels/ce3e2a24_dockertask-00001_cntk-parametricsweep-job\n",
      "remote blob: ce3e2a24_dockertask-00001_cntk-parametricsweep-job/model_results.json length: 108 bytes, md5: qXiyYWWrmMU8DsbcwLP3gg==\n",
      "created local directory: psmodels/333db632_dockertask-00002_cntk-parametricsweep-job\n",
      "remote blob: 333db632_dockertask-00002_cntk-parametricsweep-job/model_results.json length: 108 bytes, md5: xeKTDjN+r7w40mAH+ZM+MA==\n",
      "performing 3 range-gets\n",
      "spawning 3 worker threads\n",
      "xfer progress: [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100.00%     752.63 range-gets/min    \n",
      "\n",
      "0.000308990478516 MiB transfered, elapsed 0.239157915115 sec. Throughput = 0.0103359482246 Mbit/sec\n",
      "\n",
      "performing finalization (if applicable): HMAC-SHA256: False, MD5: True\n",
      "[MD5: OK, psmodels/59254dc2_dockertask-00000_cntk-parametricsweep-job/model_results.json] 3TMThHwqrsom6A+zfhG0iw== <L..R> 3TMThHwqrsom6A+zfhG0iw==\n",
      "[MD5: OK, psmodels/ce3e2a24_dockertask-00001_cntk-parametricsweep-job/model_results.json] qXiyYWWrmMU8DsbcwLP3gg== <L..R> qXiyYWWrmMU8DsbcwLP3gg==\n",
      "[MD5: OK, psmodels/333db632_dockertask-00002_cntk-parametricsweep-job/model_results.json] xeKTDjN+r7w40mAH+ZM+MA== <L..R> xeKTDjN+r7w40mAH+ZM+MA==\n",
      "finalization complete.\n",
      "\n",
      "script elapsed time: 0.569796085358 sec\n",
      "script end time: 2017-07-12 14:18:04\n"
     ]
    }
   ],
   "source": [
    "blobxfer $storage_account_name output $MODELS_DIR --remoteresource . --download --include \"*_$JOB_ID/model_results.json\" --storageaccountkey $storage_account_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will combine all of the `model_results.json` files into one dictionary for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"333db632_dockertask-00002_cntk-parametricsweep-job\": {\n",
      "        \"parameters\": {\n",
      "            \"max_epochs\": 30, \n",
      "            \"minibatch_size\": 64, \n",
      "            \"num_convolution_layers\": 3\n",
      "        }, \n",
      "        \"test_metric\": 0.2295\n",
      "    }, \n",
      "    \"59254dc2_dockertask-00000_cntk-parametricsweep-job\": {\n",
      "        \"parameters\": {\n",
      "            \"max_epochs\": 30, \n",
      "            \"minibatch_size\": 32, \n",
      "            \"num_convolution_layers\": 3\n",
      "        }, \n",
      "        \"test_metric\": 0.2163\n",
      "    }, \n",
      "    \"ce3e2a24_dockertask-00001_cntk-parametricsweep-job\": {\n",
      "        \"parameters\": {\n",
      "            \"max_epochs\": 30, \n",
      "            \"minibatch_size\": 64, \n",
      "            \"num_convolution_layers\": 2\n",
      "        }, \n",
      "        \"test_metric\": 0.2403\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def scandir(basedir):\n",
    "    for root, dirs, files in os.walk(basedir):\n",
    "        for f in files:\n",
    "            yield os.path.join(root, f) \n",
    "\n",
    "results_dict = {}\n",
    "for model in scandir(MODELS_DIR):\n",
    "    if not model.endswith('.json'):\n",
    "        continue\n",
    "    key = model.split(os.sep)[1]\n",
    "    results_dict[key] = read_json(model)\n",
    "    \n",
    "print(json.dumps(results_dict, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the aggregated results dictionary, we select the one with the smallest error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task with smallest error: 59254dc2_dockertask-00000_cntk-parametricsweep-job (0.2163)\n"
     ]
    }
   ],
   "source": [
    "tuple_min_error = min(results_dict.iteritems(), key=lambda x: x[1]['test_metric'])\n",
    "configuration_with_min_error = tuple_min_error[0]\n",
    "print('task with smallest error: {} ({})'.format(configuration_with_min_error, tuple_min_error[1]['test_metric']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download best model\n",
    "Now we'll download the corresponding best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59254dc2_dockertask-00000_cntk-parametricsweep-job/ConvNet_CIFAR10_model.dnn\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'ConvNet_CIFAR10_model.dnn'\n",
    "BEST_MODEL_BLOB_NAME = '{}/{}'.format(configuration_with_min_error, MODEL_NAME)\n",
    "print(BEST_MODEL_BLOB_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      " azure blobxfer parameters [v0.12.1]\n",
      "=====================================\n",
      "             platform: Linux-4.4.0-81-generic-x86_64-with-debian-stretch-sid\n",
      "   python interpreter: CPython 2.7.11\n",
      "     package versions: az.common=1.1.5 az.sml=0.20.5 az.stor=0.34.2 crypt=1.9 req=2.18.1\n",
      "      subscription id: None\n",
      "      management cert: None\n",
      "   transfer direction: Azure->local\n",
      "       local resource: psmodels\n",
      "      include pattern: None\n",
      "      remote resource: 59254dc2_dockertask-00000_cntk-parametricsweep-job/ConvNet_CIFAR10_model.dnn\n",
      "   max num of workers: 24\n",
      "              timeout: None\n",
      "      storage account: batch0e43a94est\n",
      "              use SAS: False\n",
      "  upload as page blob: False\n",
      "  auto vhd->page blob: False\n",
      " upload to file share: False\n",
      " container/share name: output\n",
      "  container/share URI: https://batch0e43a94est.blob.core.windows.net/output\n",
      "    compute block MD5: False\n",
      "     compute file MD5: True\n",
      "    skip on MD5 match: True\n",
      "   chunk size (bytes): 4194304\n",
      "     create container: True\n",
      "  keep mismatched MD5: False\n",
      "     recursive if dir: True\n",
      "component strip on up: 1\n",
      "        remote delete: False\n",
      "           collate to: disabled\n",
      "      local overwrite: True\n",
      "      encryption mode: disabled\n",
      "         RSA key file: disabled\n",
      "         RSA key type: disabled\n",
      "=======================================\n",
      "\n",
      "script start time: 2017-07-12 14:18:18\n",
      "generating local directory structure and pre-allocating space\n",
      "remote blob: 59254dc2_dockertask-00000_cntk-parametricsweep-job/ConvNet_CIFAR10_model.dnn length: 1957354 bytes, md5: DJA/YB03FRZXaUXqS41PYA==\n",
      "performing 1 range-gets\n",
      "spawning 1 worker threads\n",
      "xfer progress: [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100.00%     128.95 range-gets/min    \n",
      "\n",
      "1.86667823792 MiB transfered, elapsed 0.465287208557 sec. Throughput = 32.095070805 Mbit/sec\n",
      "\n",
      "performing finalization (if applicable): HMAC-SHA256: False, MD5: True\n",
      "[MD5: OK, psmodels/59254dc2_dockertask-00000_cntk-parametricsweep-job/ConvNet_CIFAR10_model.dnn] DJA/YB03FRZXaUXqS41PYA== <L..R> DJA/YB03FRZXaUXqS41PYA==\n",
      "finalization complete.\n",
      "\n",
      "script elapsed time: 0.664788007736 sec\n",
      "script end time: 2017-07-12 14:18:18\n"
     ]
    }
   ],
   "source": [
    "blobxfer $storage_account_name output $MODELS_DIR --remoteresource $BEST_MODEL_BLOB_NAME --download --storageaccountkey $storage_account_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1920\r\n",
      "drwxr-xr-x  2 nbuser nbuser    4096 Jul 12 14:18 ./\r\n",
      "drwx------ 15 nbuser nbuser    4096 Jul 12 14:11 ../\r\n",
      "-rw-r--r--  1 nbuser nbuser 1957354 Jul 12 14:18 ConvNet_CIFAR10_model.dnn\r\n"
     ]
    }
   ],
   "source": [
    "!mv $MODELS_DIR/$configuration_with_min_error/$MODEL_NAME $MODELS_DIR\n",
    "!rm -rf $MODELS_DIR/*_$JOB_ID  # optionally remove all of the temporary result json directories/files\n",
    "!ls -alF $MODELS_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model file (`ConvNet_CIFAR10_model.dnn`) is now ready for use.\n",
    "\n",
    "**Note:** We could have created a Batch task that did the model selection for us using task dependencies. The model selection task would be dependent upon all of the parametric sweep training tasks and would only run after those tasks complete successfully. The Batch task could then proceed with the logic above.\n",
    "\n",
    "Please see the advanced notebook that shows how this is accomplished: [Automatic Model Selection from Parametric Sweep with Task Dependencies](05_Advanced_Auto_Model_Selection.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section6'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To delete the job use the command below. Just be aware that this will get rid of all the files created by the job and tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-07-12 14:18:23,129 INFO - Deleting job: cntk-parametricsweep-job\n",
      "2017-07-12 14:18:23,129 DEBUG - disabling job cntk-parametricsweep-job first due to task termination\n",
      "2017-07-12 14:18:23,974 DEBUG - Skipping termination of completed task dockertask-00000 on job cntk-parametricsweep-job\n",
      "2017-07-12 14:18:24,182 DEBUG - Skipping termination of completed task dockertask-00001 on job cntk-parametricsweep-job\n",
      "2017-07-12 14:18:24,378 DEBUG - Skipping termination of completed task dockertask-00002 on job cntk-parametricsweep-job\n",
      "2017-07-12 14:18:24,737 DEBUG - waiting for job cntk-parametricsweep-job to delete\n",
      "2017-07-12 14:18:56,482 INFO - job cntk-parametricsweep-job does not exist\n"
     ]
    }
   ],
   "source": [
    "shipyard jobs del -y --termtasks --wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
