{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains a tutorial on how to train a Deep Learning model on GPUs using Azure Batch shipyard. In this example we will be training a Deep Learning model built using CNTK but Batch Shipyard can be used to train a model built in any framework.\n",
    "\n",
    "This notebook assumes that nothing has been set up previously and will create everything from scratch. The necessary steps are broken up into the following sections:\n",
    "* [Setup](#section1)\n",
    "* [Azure account login](#section2)\n",
    "* [Create Azure resources](#section3)\n",
    "* [Transfer model files](#section4)\n",
    "* [Batch Shiyard configuration](#section5)\n",
    "* [Using Batch Shipyard](#section6)\n",
    "* [Delete everything](#section7)\n",
    "\n",
    "\n",
    "In this specific example Batch shipyard will spin up a node with GPU support and load our Docker image that has the necessary libraries installed. Pull in the Python files that contain our model into the node and execute them. The output of the execution will then be streamed back for us to see. The two files used are [prepare_cifar_data.py](model/prepare_cifar_data.py) and [ConvNet_CIFAR10.py](model/ConvNet_CIFAR10.py). With the former downloading the data and conerting it to the appropriate format and the second constructing, training and evaluating the model.\n",
    "\n",
    "\n",
    "\n",
    "<a id='installation'></a>\n",
    "### Installation\n",
    "If you do not have Batch Shipyard the Azure CLI or Blobxfer installed please follow the instructions below to install them before continuing with this tutorial.  \n",
    "[Batch Shipyard](https://github.com/Azure/batch-shipyard/blob/master/docs/01-batch-shipyard-installation.md)    \n",
    "[Azure CLI](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli)  \n",
    "[Blobxfer](https://github.com/Azure/blobxfer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We asume here that the shipyard command is in your PATH and can be executed within the notebook with the !shipyard command. If not please add it to your PATH."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the various name definitions for the resources needed to run batch jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feel free to modify these\n",
    "group_name = 'batchcntkrg'\n",
    "batch_account_name = \"batchcntkacc\"\n",
    "storage_account_name = \"batchcntkstr\"\n",
    "location = 'eastus' # We are setting everything up in East US\n",
    "code_share_name = \"code\"\n",
    "\n",
    "# Leve these alone for now\n",
    "STORAGE_ALIAS = \"mystorageaccount\"\n",
    "IMAGE_NAME = \"microsoft/cntk:2.0.rc1-gpu-python3.5-cuda8.0-cudnn5.1\" # The latest CNTK image\n",
    "STORAGE_ENDPOINT = \"core.windows.net\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure account login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command below will initiate a login to your Azure account. It will pop up with an url to go to where you will enter a one off code and log into your Azure account using your browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!az login -o table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you hve multiple subscriptions you can select the one you need with the command below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selected_subscription = '\"My Team\"'\n",
    "!az account set --subscription $selected_subscription"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Azure resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Group\n",
    "Azure encourages the use of groups to organise all the Azure components you deploy that way it is easier to find them but also we can deleted a number of resources simply by deleting the Group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location    Name\n",
      "----------  -----------\n",
      "eastus      batchcntkrg\n",
      "CPU times: user 58 ms, sys: 24.8 ms, total: 82.7 ms\n",
      "Wall time: 3.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!az group create -n $group_name -l $location -o table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Batch account\n",
    "In this section we define an ARM template to create our batch and storage accounts. Once we have created the accounts we can the use the Azure CLI to query them and obtain the batch_account_key, batch_service_url and storage_account_key which we will need for our Batch Shipyard configuration files later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "template_dict = {\n",
    "    \"$schema\": \"https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\",\n",
    "    \"contentVersion\": \"1.0.0.0\",\n",
    "    \"parameters\": {\n",
    "        \"batchAccounts_name\": {\n",
    "            \"defaultValue\": batch_account_name,\n",
    "            \"type\": \"String\"\n",
    "        },\n",
    "        \"storageAccounts_name\": {\n",
    "            \"defaultValue\": storage_account_name,\n",
    "            \"type\": \"String\"\n",
    "        }\n",
    "    },\n",
    "    \"variables\": {},\n",
    "    \"resources\": [\n",
    "        {\n",
    "            \"type\": \"Microsoft.Batch/batchAccounts\",\n",
    "            \"name\": \"[parameters('batchAccounts_name')]\",\n",
    "            \"apiVersion\": \"2015-12-01\",\n",
    "            \"location\": location,\n",
    "            \"properties\": {\n",
    "                \"autoStorage\": {\n",
    "                    \"storageAccountId\": \"[resourceId('Microsoft.Storage/storageAccounts', parameters('storageAccounts_name'))]\"\n",
    "                }\n",
    "            },\n",
    "            \"resources\": [],\n",
    "            \"dependsOn\": [\n",
    "                \"[resourceId('Microsoft.Storage/storageAccounts', parameters('storageAccounts_name'))]\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"Microsoft.Storage/storageAccounts\",\n",
    "            \"sku\": {\n",
    "                \"name\": \"Standard_LRS\",\n",
    "                \"tier\": \"Standard\"\n",
    "            },\n",
    "            \"kind\": \"Storage\",\n",
    "            \"name\": \"[parameters('storageAccounts_name')]\",\n",
    "            \"apiVersion\": \"2016-01-01\",\n",
    "            \"location\": location,\n",
    "            \"tags\": {},\n",
    "            \"properties\": {},\n",
    "            \"resources\": [],\n",
    "            \"dependsOn\": []\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "template_filename = 'template.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(template_filename, 'w') as outfile:\n",
    "    json.dump(template_dict, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate the template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "  \"error\": null,\r\n",
      "  \"properties\": {\r\n",
      "    \"correlationId\": \"2ec7bbea-75af-4f53-bfc6-88457a3b5e0f\",\r\n",
      "    \"debugSetting\": null,\r\n",
      "    \"dependencies\": [\r\n",
      "      {\r\n",
      "        \"dependsOn\": [\r\n",
      "          {\r\n",
      "            \"id\": \"/subscriptions/edf507a2-6235-46c5-b560-fd463ba2e771/resourceGroups/batchcntkrg/providers/Microsoft.Storage/storageAccounts/batchcntkstr\",\r\n",
      "            \"resourceGroup\": \"batchcntkrg\",\r\n",
      "            \"resourceName\": \"batchcntkstr\",\r\n",
      "            \"resourceType\": \"Microsoft.Storage/storageAccounts\"\r\n",
      "          }\r\n",
      "        ],\r\n",
      "        \"id\": \"/subscriptions/edf507a2-6235-46c5-b560-fd463ba2e771/resourceGroups/batchcntkrg/providers/Microsoft.Batch/batchAccounts/batchcntkacc\",\r\n",
      "        \"resourceGroup\": \"batchcntkrg\",\r\n",
      "        \"resourceName\": \"batchcntkacc\",\r\n",
      "        \"resourceType\": \"Microsoft.Batch/batchAccounts\"\r\n",
      "      }\r\n",
      "    ],\r\n",
      "    \"mode\": \"Incremental\",\r\n",
      "    \"outputs\": null,\r\n",
      "    \"parameters\": {\r\n",
      "      \"batchAccounts_name\": {\r\n",
      "        \"type\": \"String\",\r\n",
      "        \"value\": \"batchcntkacc\"\r\n",
      "      },\r\n",
      "      \"storageAccounts_name\": {\r\n",
      "        \"type\": \"String\",\r\n",
      "        \"value\": \"batchcntkstr\"\r\n",
      "      }\r\n",
      "    },\r\n",
      "    \"parametersLink\": null,\r\n",
      "    \"providers\": [\r\n",
      "      {\r\n",
      "        \"id\": null,\r\n",
      "        \"namespace\": \"Microsoft.Batch\",\r\n",
      "        \"registrationState\": null,\r\n",
      "        \"resourceTypes\": [\r\n",
      "          {\r\n",
      "            \"aliases\": null,\r\n",
      "            \"apiVersions\": null,\r\n",
      "            \"locations\": [\r\n",
      "              \"eastus\"\r\n",
      "            ],\r\n",
      "            \"properties\": null,\r\n",
      "            \"resourceType\": \"batchAccounts\"\r\n",
      "          }\r\n",
      "        ]\r\n",
      "      },\r\n",
      "      {\r\n",
      "        \"id\": null,\r\n",
      "        \"namespace\": \"Microsoft.Storage\",\r\n",
      "        \"registrationState\": null,\r\n",
      "        \"resourceTypes\": [\r\n",
      "          {\r\n",
      "            \"aliases\": null,\r\n",
      "            \"apiVersions\": null,\r\n",
      "            \"locations\": [\r\n",
      "              \"eastus\"\r\n",
      "            ],\r\n",
      "            \"properties\": null,\r\n",
      "            \"resourceType\": \"storageAccounts\"\r\n",
      "          }\r\n",
      "        ]\r\n",
      "      }\r\n",
      "    ],\r\n",
      "    \"provisioningState\": \"Succeeded\",\r\n",
      "    \"template\": null,\r\n",
      "    \"templateLink\": null,\r\n",
      "    \"timestamp\": \"2017-04-10T09:08:45.101443+00:00\"\r\n",
      "  }\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!az group deployment validate --template-file $template_filename -g $group_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see \"Succeeded\" in the provisioningState field in the JSON above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we deploy the template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"/subscriptions/edf507a2-6235-46c5-b560-fd463ba2e771/resourceGroups/batchcntkrg/providers/Microsoft.Resources/deployments/template\",\n",
      "  \"name\": \"template\",\n",
      "  \"properties\": {\n",
      "    \"correlationId\": \"6b45de5c-2ae9-4dda-ae63-d2ffde46d961\",\n",
      "    \"debugSetting\": null,\n",
      "    \"dependencies\": [\n",
      "      {\n",
      "        \"dependsOn\": [\n",
      "          {\n",
      "            \"id\": \"/subscriptions/edf507a2-6235-46c5-b560-fd463ba2e771/resourceGroups/batchcntkrg/providers/Microsoft.Storage/storageAccounts/batchcntkstr\",\n",
      "            \"resourceGroup\": \"batchcntkrg\",\n",
      "            \"resourceName\": \"batchcntkstr\",\n",
      "            \"resourceType\": \"Microsoft.Storage/storageAccounts\"\n",
      "          }\n",
      "        ],\n",
      "        \"id\": \"/subscriptions/edf507a2-6235-46c5-b560-fd463ba2e771/resourceGroups/batchcntkrg/providers/Microsoft.Batch/batchAccounts/batchcntkacc\",\n",
      "        \"resourceGroup\": \"batchcntkrg\",\n",
      "        \"resourceName\": \"batchcntkacc\",\n",
      "        \"resourceType\": \"Microsoft.Batch/batchAccounts\"\n",
      "      }\n",
      "    ],\n",
      "    \"mode\": \"Incremental\",\n",
      "    \"outputs\": null,\n",
      "    \"parameters\": {\n",
      "      \"batchAccounts_name\": {\n",
      "        \"type\": \"String\",\n",
      "        \"value\": \"batchcntkacc\"\n",
      "      },\n",
      "      \"storageAccounts_name\": {\n",
      "        \"type\": \"String\",\n",
      "        \"value\": \"batchcntkstr\"\n",
      "      }\n",
      "    },\n",
      "    \"parametersLink\": null,\n",
      "    \"providers\": [\n",
      "      {\n",
      "        \"id\": null,\n",
      "        \"namespace\": \"Microsoft.Batch\",\n",
      "        \"registrationState\": null,\n",
      "        \"resourceTypes\": [\n",
      "          {\n",
      "            \"aliases\": null,\n",
      "            \"apiVersions\": null,\n",
      "            \"locations\": [\n",
      "              \"eastus\"\n",
      "            ],\n",
      "            \"properties\": null,\n",
      "            \"resourceType\": \"batchAccounts\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"id\": null,\n",
      "        \"namespace\": \"Microsoft.Storage\",\n",
      "        \"registrationState\": null,\n",
      "        \"resourceTypes\": [\n",
      "          {\n",
      "            \"aliases\": null,\n",
      "            \"apiVersions\": null,\n",
      "            \"locations\": [\n",
      "              \"eastus\"\n",
      "            ],\n",
      "            \"properties\": null,\n",
      "            \"resourceType\": \"storageAccounts\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    ],\n",
      "    \"provisioningState\": \"Succeeded\",\n",
      "    \"template\": null,\n",
      "    \"templateLink\": null,\n",
      "    \"timestamp\": \"2017-04-10T09:09:57.365596+00:00\"\n",
      "  },\n",
      "  \"resourceGroup\": \"batchcntkrg\"\n",
      "}\n",
      "CPU times: user 1.27 s, sys: 467 ms, total: 1.74 s\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!az group deployment create --template-file $template_filename -g $group_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we should see \"Succeeded\" in the provisioningState field in the JSON above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we retrieve the batch_account_key, batch_service_url and storage_account_key which we will need for the Batch Shipyard configuration files further down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json_data = !az batch account keys list -n $batch_account_name -g $group_name\n",
    "batch_account_key = json.loads(''.join(json_data))['primary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json_data = !az batch account list -g $group_name\n",
    "batch_service_url = 'https://'+json.loads(''.join(json_data))[0]['accountEndpoint']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json_data = !az storage account keys list -n $storage_account_name -g $group_name\n",
    "storage_account_key = json.loads(''.join(json_data))[0]['value']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer files\n",
    "Here we will create a share in the storage account where we will transfer the necessary files to create and execute our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32minfo\u001b[39m:    Executing command \u001b[1mstorage share create\u001b[22m\n",
      "+\n",
      "+\n",
      "\u001b[90mdata\u001b[39m:    \u001b[36m{\u001b[39m\n",
      "\u001b[90mdata\u001b[39m:    \u001b[36m    \u001b[1mname\u001b[22m: \u001b[32m'code'\u001b[36m,\u001b[39m\n",
      "\u001b[90mdata\u001b[39m:    \u001b[36m    \u001b[1mmetadata\u001b[22m: {},\u001b[39m\n",
      "\u001b[90mdata\u001b[39m:    \u001b[36m    \u001b[1metag\u001b[22m: \u001b[32m'\"0x8D47FF2F446FB17\"'\u001b[36m,\u001b[39m\n",
      "\u001b[90mdata\u001b[39m:    \u001b[36m    \u001b[1mlastModified\u001b[22m: \u001b[32m'Mon, 10 Apr 2017 09:21:22 GMT'\u001b[36m,\u001b[39m\n",
      "\u001b[90mdata\u001b[39m:    \u001b[36m    \u001b[1mrequestId\u001b[22m: \u001b[32m'7607fda2-001a-0065-43db-b155b9000000'\u001b[36m,\u001b[39m\n",
      "\u001b[90mdata\u001b[39m:    \u001b[36m    \u001b[1mquota\u001b[22m: \u001b[32m'5120'\u001b[36m,\u001b[39m\n",
      "\u001b[90mdata\u001b[39m:    \u001b[36m    \u001b[1mshareUsage\u001b[22m: \u001b[32m'0'\u001b[36m\u001b[39m\n",
      "\u001b[90mdata\u001b[39m:    \u001b[36m}\u001b[39m\u001b[39m\n",
      "\u001b[32minfo\u001b[39m:    \u001b[1mstorage share create\u001b[22m command \u001b[1m\u001b[32mOK\u001b[39m\u001b[22m\n",
      "CPU times: user 49.4 ms, sys: 16.2 ms, total: 65.6 ms\n",
      "Wall time: 2.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!azure storage share create $code_share_name -a $storage_account_name -k $storage_account_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "localfolder = 'model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      " azure blobxfer parameters [v0.12.1]\n",
      "=====================================\n",
      "             platform: Linux-3.10.0-327.36.3.el7.x86_64-x86_64-with-centos-7.2.1511-Core\n",
      "   python interpreter: CPython 3.5.3\n",
      "     package versions: az.common=1.1.4 az.sml=0.20.5 az.stor=0.33.0 crypt=1.8.1 req=2.13.0\n",
      "      subscription id: None\n",
      "      management cert: None\n",
      "   transfer direction: local->Azure\n",
      "       local resource: model\n",
      "      include pattern: None\n",
      "      remote resource: None\n",
      "   max num of workers: 12\n",
      "              timeout: None\n",
      "      storage account: batchcntkstr\n",
      "              use SAS: False\n",
      "  upload as page blob: False\n",
      "  auto vhd->page blob: False\n",
      " upload to file share: True\n",
      " container/share name: code\n",
      "  container/share URI: https://batchcntkstr.file.core.windows.net/code\n",
      "    compute block MD5: False\n",
      "     compute file MD5: True\n",
      "    skip on MD5 match: True\n",
      "   chunk size (bytes): 4194304\n",
      "     create container: True\n",
      "  keep mismatched MD5: False\n",
      "     recursive if dir: True\n",
      "component strip on up: 1\n",
      "        remote delete: False\n",
      "           collate to: disabled\n",
      "      local overwrite: True\n",
      "      encryption mode: disabled\n",
      "         RSA key file: disabled\n",
      "         RSA key type: disabled\n",
      "=======================================\n",
      "\n",
      "script start time: 2017-04-11 08:28:51\n",
      "computing file md5 on: model/ConvNet_CIFAR10.py\n",
      "  >> 4pa+1/eCjNs7rltnvWf81w== <L..R> ij++Ej//sGtjzVyVg4jnzQ== ConvNet_CIFAR10.py MISMATCH: re-upload\n",
      "computing file md5 on: model/prepare_cifar_data.py\n",
      "  >> RW4Sbj2aAQNwKmoJcbQNdA== <L..R> RW4Sbj2aAQNwKmoJcbQNdA== prepare_cifar_data.py match: skip\n",
      "creating file share, if needed: code\n",
      "initializing files on fileshare\n",
      "detected 0 empty files to upload\n",
      "performing 1 put ranges and 1 set file properties\n",
      "spawning 1 worker threads\n",
      "xfer progress: [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100.00%     156.38 ranges/min    \n",
      "\n",
      "0.005625724792480469 MiB transfered, elapsed 0.38368797302246094 sec. Throughput = 0.11729791263800997 Mbit/sec\n",
      "\n",
      "\n",
      "script elapsed time: 1.3011884689331055 sec\n",
      "script end time: 2017-04-11 08:28:53\n",
      "CPU times: user 35.6 ms, sys: 22.4 ms, total: 58.1 ms\n",
      "Wall time: 1.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!blobxfer $storage_account_name $code_share_name $localfolder --fileshare --upload --storageaccountkey $storage_account_key \n",
    "# Transfers all files in local folder to the share"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Shiyard configuration\n",
    "In order to execute a job on Batch Shipyard you need a minimum of 4 configuration files. These are\n",
    "* [credentials](#credentials)\n",
    "* [configuration](#configuration)\n",
    "* [pool](#pool)\n",
    "* [jobs](#jobs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='credentials'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credentials\n",
    "Here we define all the credentials necessary for Batch Shipyard to run our job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "credentials = {\n",
    "    \"credentials\": {\n",
    "        \"batch\": {\n",
    "            \"account_key\": batch_account_key,\n",
    "            \"account_service_url\": batch_service_url\n",
    "        },\n",
    "        \"storage\": {\n",
    "            STORAGE_ALIAS : {\n",
    "                    \"account\": storage_account_name,\n",
    "                    \"account_key\": storage_account_key,\n",
    "                    \"endpoint\": STORAGE_ENDPOINT\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='configuration'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "The config mainly contains the configuration for Batch Shipyard. Here we simply define the storage alias that Batch Shipyard should use as well as the image name to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"batch_shipyard\": {\n",
    "        \"storage_account_settings\": STORAGE_ALIAS\n",
    "    },\n",
    "    \"global_resources\": {\n",
    "        \"docker_images\": [\n",
    "            IMAGE_NAME\n",
    "        ]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pool'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pool\n",
    "This is where we define the properties of the compute pool we wish to create. The configuration below creates a pool that is made up of a single NC6 VM running Ubuntu. If you wish to run a job that uses GPUs then you need to use a VM from the NC or NV series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pool={\n",
    "    \"pool_specification\": {\n",
    "        \"id\": \"singlegpu\",\n",
    "        \"vm_size\": \"STANDARD_NC6\",\n",
    "        \"vm_count\": 1,\n",
    "        \"publisher\": \"Canonical\",\n",
    "        \"offer\": \"UbuntuServer\",\n",
    "        \"sku\": \"16.04-LTS\",\n",
    "        \"ssh\": {\n",
    "            \"username\": \"docker\"\n",
    "        },\n",
    "        \"reboot_on_start_task_failed\": False,\n",
    "        \"block_until_all_global_resources_loaded\": True,\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='jobs'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jobs\n",
    "In the dictonary below we define the properties of the job we wish to execute. You can see thta we have specified that the image to use is the one we defined at the beginning of this notebook. Another interesting note is thta we specify the gpu switch to true since we want the job to use the GPU. Finally the command is as follows\n",
    "\n",
    "```\n",
    "source /cntk/activate-cntk\n",
    "python $AZ_BATCH_NODE_SHARED_DIR/code/process_cifar_data.py\n",
    "python $AZ_BATCH_NODE_SHARED_DIR/code/cntk_cifar10.py\"\n",
    "```\n",
    "\n",
    "Which in essence activate the CNTK Anaconda environment then runs the the process_cifar_data.py script. Once this is done it will run the cntk_cifar10.py script which will train and evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "command = 'bash -c \"source /cntk/activate-cntk; \\\n",
    "           python $AZ_BATCH_NODE_SHARED_DIR/code/prepare_cifar_data.py; \\\n",
    "           python $AZ_BATCH_NODE_SHARED_DIR/code/ConvNet_CIFAR10.py\"'\n",
    "\n",
    "jobs = {\n",
    "    \"job_specifications\": [\n",
    "        {\n",
    "            \"id\": \"cntkjob\",\n",
    "            \"tasks\": [\n",
    "                {\n",
    "                    \"id\": \"run_cifar10\",# This should be changed per task\n",
    "                    \"image\": IMAGE_NAME,\n",
    "                    \"remove_container_after_exit\": True,\n",
    "                    \"command\": command,\n",
    "                    \"gpu\": True,\n",
    "                }\n",
    "            ],\n",
    "            \"input_data\": {\n",
    "                \"azure_storage\": [\n",
    "                    {\n",
    "                        \"storage_account_settings\": STORAGE_ALIAS,\n",
    "                        \"file_share\": code_share_name,\n",
    "                        \"blobxfer_extra_options\": None,\n",
    "                        \"destination\":\"$AZ_BATCH_NODE_SHARED_DIR/code\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!rm -r config\n",
    "!mkdir config # Create config file where we will store all our Batch Shipyard configuration files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_json_to_file(json_dict, filename):\n",
    "    \"\"\" Simple function to write JSON dictionaries to files\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as outfile:\n",
    "        json.dump(json_dict, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_json_to_file(credentials, path.join('config', 'credentials.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_json_to_file(config, path.join('config', 'config.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_json_to_file(pool, path.join('config', 'pool.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_json_to_file(jobs, path.join('config', 'jobs.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section6'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Batch Shipyard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we do anything we need to create the pool. This can take a little bit of time so be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1e+03 ns, total: 4 µs\n",
      "Wall time: 8.11 µs\n",
      "2017-04-11 08:07:18.362 INFO - creating container: shipyardtor-batchcntkacc-singlegpu\n",
      "2017-04-11 08:07:19.057 INFO - creating table: shipyardtorrentinfo\n",
      "2017-04-11 08:07:19.290 INFO - creating table: shipyardimages\n",
      "2017-04-11 08:07:19.338 INFO - creating container: shipyardrf-batchcntkacc-singlegpu\n",
      "2017-04-11 08:07:19.386 INFO - creating queue: shipyardgr-batchcntkacc-singlegpu\n",
      "2017-04-11 08:07:19.671 INFO - creating table: shipyardregistry\n",
      "2017-04-11 08:07:19.721 INFO - creating table: shipyarddht\n",
      "2017-04-11 08:07:19.770 INFO - creating container: shipyardremotefs\n",
      "2017-04-11 08:07:19.814 INFO - creating table: shipyardgr\n",
      "2017-04-11 08:07:19.864 INFO - deleting blobs: shipyardtor-batchcntkacc-singlegpu\n",
      "2017-04-11 08:07:19.937 DEBUG - clearing table (pk=batchcntkacc$singlegpu): shipyardtorrentinfo\n",
      "2017-04-11 08:07:20.008 DEBUG - clearing table (pk=batchcntkacc$singlegpu): shipyardimages\n",
      "2017-04-11 08:07:20.057 INFO - deleting blobs: shipyardrf-batchcntkacc-singlegpu\n",
      "2017-04-11 08:07:20.100 INFO - clearing queue: shipyardgr-batchcntkacc-singlegpu\n",
      "2017-04-11 08:07:20.143 DEBUG - clearing table (pk=batchcntkacc$singlegpu): shipyardperf\n",
      "2017-04-11 08:07:20.192 DEBUG - clearing table (pk=batchcntkacc$singlegpu): shipyardregistry\n",
      "2017-04-11 08:07:20.240 DEBUG - clearing table (pk=batchcntkacc$singlegpu): shipyarddht\n",
      "2017-04-11 08:07:20.288 DEBUG - clearing table (pk=batchcntkacc$singlegpu): shipyardgr\n",
      "2017-04-11 08:07:20.400 INFO - adding global resource: docker:microsoft/cntk:2.0.rc1-gpu-python3.5-cuda8.0-cudnn5.1\n",
      "2017-04-11 08:07:20.494 DEBUG - no virtual network settings specified\n",
      "2017-04-11 08:07:21.074 INFO - uploading file /home/mat/repos/cntk_batch/batch-shipyard/scripts/shipyard_nodeprep.sh as 'shipyard_nodeprep.sh'\n",
      "2017-04-11 08:07:21.363 INFO - uploading file /home/mat/repos/cntk_batch/batch-shipyard/scripts/docker_jp_block.sh as 'docker_jp_block.sh'\n",
      "2017-04-11 08:07:21.639 INFO - uploading file /home/mat/repos/cntk_batch/batch-shipyard/scripts/shipyard_blobxfer.sh as 'shipyard_blobxfer.sh'\n",
      "2017-04-11 08:07:21.922 INFO - uploading file /home/mat/repos/cntk_batch/batch-shipyard/resources/nvidia-driver.run as 'nvidia-driver.run'\n",
      "2017-04-11 08:07:24.659 INFO - uploading file /home/mat/repos/cntk_batch/batch-shipyard/resources/nvidia-docker.deb as 'nvidia-docker.deb'\n",
      "2017-04-11 08:07:24.792 INFO - Attempting to create pool: singlegpu\n",
      "2017-04-11 08:07:24.947 INFO - Created pool: singlegpu\n",
      "2017-04-11 08:07:24.948 INFO - waiting for all nodes in pool singlegpu to reach one of: frozenset({<ComputeNodeState.start_task_failed: 'startTaskFailed'>, <ComputeNodeState.idle: 'idle'>, <ComputeNodeState.unusable: 'unusable'>})\n",
      "2017-04-11 08:07:45.473 DEBUG - waiting for 1 nodes to reach desired state\n",
      "2017-04-11 08:08:15.891 DEBUG - waiting for 1 nodes to reach desired state\n",
      "2017-04-11 08:08:46.374 DEBUG - waiting for 1 nodes to reach desired state\n",
      "2017-04-11 08:09:16.804 DEBUG - waiting for 1 nodes to reach desired state\n",
      "2017-04-11 08:09:47.254 DEBUG - waiting for 1 nodes to reach desired state\n",
      "2017-04-11 08:09:47.254 DEBUG - tvm-325613345_1-20170411t080923z: ComputeNodeState.creating\n",
      "2017-04-11 08:10:17.808 DEBUG - waiting for 1 nodes to reach desired state\n",
      "2017-04-11 08:10:17.809 DEBUG - tvm-325613345_1-20170411t080923z: ComputeNodeState.creating\n",
      "2017-04-11 08:10:48.448 DEBUG - waiting for 1 nodes to reach desired state\n",
      "2017-04-11 08:10:48.448 DEBUG - tvm-325613345_1-20170411t080923z: ComputeNodeState.creating\n",
      "2017-04-11 08:11:19.322 DEBUG - waiting for 1 nodes to reach desired state\n",
      "2017-04-11 08:11:19.322 DEBUG - tvm-325613345_1-20170411t080923z: ComputeNodeState.creating\n",
      "2017-04-11 08:11:49.820 DEBUG - waiting for 1 nodes to reach desired state\n",
      "2017-04-11 08:11:49.820 DEBUG - tvm-325613345_1-20170411t080923z: ComputeNodeState.creating\n",
      "2017-04-11 08:12:20.785 DEBUG - waiting for 1 nodes to reach desired state\n",
      "2017-04-11 08:12:20.786 DEBUG - tvm-325613345_1-20170411t080923z: ComputeNodeState.creating\n",
      "2017-04-11 08:12:51.602 DEBUG - waiting for 1 nodes to reach desired state\n",
      "2017-04-11 08:12:51.602 DEBUG - tvm-325613345_1-20170411t080923z: ComputeNodeState.waiting_for_start_task\n",
      "2017-04-11 08:13:22.109 DEBUG - waiting for 1 nodes to reach desired state\n",
      "2017-04-11 08:13:22.110 DEBUG - tvm-325613345_1-20170411t080923z: ComputeNodeState.waiting_for_start_task\n",
      "2017-04-11 08:13:52.530 DEBUG - waiting for 1 nodes to reach desired state\n",
      "2017-04-11 08:13:52.531 DEBUG - tvm-325613345_1-20170411t080923z: ComputeNodeState.waiting_for_start_task\n",
      "2017-04-11 08:14:23.101 DEBUG - waiting for 1 nodes to reach desired state\n",
      "2017-04-11 08:14:23.101 DEBUG - tvm-325613345_1-20170411t080923z: ComputeNodeState.waiting_for_start_task\n",
      "2017-04-11 08:14:53.773 DEBUG - waiting for 1 nodes to reach desired state\n",
      "2017-04-11 08:14:53.773 DEBUG - tvm-325613345_1-20170411t080923z: ComputeNodeState.waiting_for_start_task\n",
      "2017-04-11 08:15:24.228 DEBUG - waiting for 1 nodes to reach desired state\n",
      "2017-04-11 08:15:24.228 DEBUG - tvm-325613345_1-20170411t080923z: ComputeNodeState.waiting_for_start_task\n",
      "2017-04-11 08:15:54.701 DEBUG - waiting for 1 nodes to reach desired state\n",
      "2017-04-11 08:15:54.701 DEBUG - tvm-325613345_1-20170411t080923z: ComputeNodeState.waiting_for_start_task\n",
      "2017-04-11 08:16:25.335 DEBUG - waiting for 1 nodes to reach desired state\n",
      "2017-04-11 08:16:25.336 DEBUG - tvm-325613345_1-20170411t080923z: ComputeNodeState.waiting_for_start_task\n",
      "2017-04-11 08:16:55.777 DEBUG - waiting for 1 nodes to reach desired state\n",
      "2017-04-11 08:16:55.777 DEBUG - tvm-325613345_1-20170411t080923z: ComputeNodeState.waiting_for_start_task\n",
      "2017-04-11 08:17:26.954 DEBUG - waiting for 1 nodes to reach desired state\n",
      "2017-04-11 08:17:26.954 DEBUG - tvm-325613345_1-20170411t080923z: ComputeNodeState.waiting_for_start_task\n",
      "2017-04-11 08:17:57.462 DEBUG - waiting for 1 nodes to reach desired state\n",
      "2017-04-11 08:17:57.462 DEBUG - tvm-325613345_1-20170411t080923z: ComputeNodeState.waiting_for_start_task\n",
      "2017-04-11 08:18:27.916 DEBUG - waiting for 1 nodes to reach desired state\n",
      "2017-04-11 08:18:27.916 DEBUG - tvm-325613345_1-20170411t080923z: ComputeNodeState.waiting_for_start_task\n",
      "2017-04-11 08:18:58.435 DEBUG - waiting for 1 nodes to reach desired state\n",
      "2017-04-11 08:18:58.436 DEBUG - tvm-325613345_1-20170411t080923z: ComputeNodeState.waiting_for_start_task\n",
      "2017-04-11 08:19:28.948 DEBUG - waiting for 1 nodes to reach desired state\n",
      "2017-04-11 08:19:28.948 DEBUG - tvm-325613345_1-20170411t080923z: ComputeNodeState.waiting_for_start_task\n",
      "2017-04-11 08:19:59.408 DEBUG - waiting for 1 nodes to reach desired state\n",
      "2017-04-11 08:19:59.409 DEBUG - tvm-325613345_1-20170411t080923z: ComputeNodeState.waiting_for_start_task\n",
      "2017-04-11 08:20:09.670 DEBUG - listing nodes for pool singlegpu\n",
      "2017-04-11 08:20:09.670 INFO - node_id=tvm-325613345_1-20170411t080923z [state=ComputeNodeState.idle start_task_exit_code=0 scheduling_state=SchedulingState.enabled ip_address=10.0.0.4 vm_size=standard_nc6 total_tasks_run=0 running_tasks_count=0 total_tasks_succeeded=0]\n",
      "2017-04-11 08:20:09.672 INFO - generating ssh key pair to path: .\n",
      "Generating public/private rsa key pair.\n",
      "Your identification has been saved in id_rsa_shipyard.\n",
      "Your public key has been saved in id_rsa_shipyard.pub.\n",
      "The key fingerprint is:\n",
      "57:44:99:05:71:d0:b8:3c:d4:72:98:f0:4d:cb:e2:b0 mat@msdemo\n",
      "The key's randomart image is:\n",
      "+--[ RSA 2048]----+\n",
      "|          .o*&+  |\n",
      "|           oO=+. |\n",
      "|          .o+++  |\n",
      "|           =+.   |\n",
      "|        S E ..   |\n",
      "|         .       |\n",
      "|                 |\n",
      "|                 |\n",
      "|                 |\n",
      "+-----------------+\n",
      "2017-04-11 08:20:09.751 INFO - adding user docker to node tvm-325613345_1-20170411t080923z in pool singlegpu, expiry=2017-05-11 08:20:09.751350\n",
      "Datetime with no tzinfo will be considered UTC.\n",
      "2017-04-11 08:20:10.707 INFO - node tvm-325613345_1-20170411t080923z: ip 40.71.226.171 port 50000\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!shipyard pool add --yes --configdir config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the pool is created we can confirm everything by running the command below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-04-11 08:29:09.968 INFO - pool_id=singlegpu [state=PoolState.active allocation_state=AllocationState.steady vm_size=standard_nc6, vm_count=1 target_vm_count=1]\r\n"
     ]
    }
   ],
   "source": [
    "!shipyard pool list --configdir config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have confirmed everything is working we can execute our job using the command below. The tail switch at the end will stream stdout from the node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-04-11 08:29:13.648 INFO - Adding job cntkjob to pool singlegpu\n",
      "2017-04-11 08:29:14.168 DEBUG - remote file is the same for shipyardtaskrf-cntkjob/run_cifar10.shipyard.envlist, skipping\n",
      "2017-04-11 08:29:14.168 INFO - Adding task: run_cifar10\n",
      "2017-04-11 08:29:14.418 DEBUG - attempting to stream file stdout.txt from job=cntkjob task=run_cifar10\n",
      "\n",
      "************************************************************\n",
      "CNTK is activated.\n",
      "\n",
      "Please checkout tutorials and examples here:\n",
      "  /cntk/Tutorials\n",
      "  /cntk/Examples\n",
      "\n",
      "To deactivate the environment run\n",
      "\n",
      "  source /root/anaconda3/bin/deactivate\n",
      "\n",
      "************************************************************\n",
      "Downloading http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "Done.\n",
      "Extracting files...\n",
      "Done.\n",
      "Preparing train set...\n",
      "Done.\n",
      "Preparing test set...\n",
      "Done.\n",
      "Writing train text file...\n",
      "Done.\n",
      "Writing test text file...\n",
      "Done.\n",
      "Converting train data to png images...\n",
      "Done.\n",
      "Converting test data to png images...\n",
      "Done.\n",
      "Setting up input variables\n",
      "Creating NN model\n",
      "Training 1195594 parameters in 14 parameter tensors.\n",
      "\n",
      "Starting training\n",
      "Finished Epoch[1 of 30]: [Training] loss = 1.943535 * 50000, metric = 71.96% * 50000 19.817s (2523.1 samples/s);\n",
      "Finished Epoch[2 of 30]: [Training] loss = 1.524170 * 50000, metric = 55.13% * 50000 15.728s (3179.0 samples/s);\n",
      "Finished Epoch[3 of 30]: [Training] loss = 1.294059 * 50000, metric = 45.84% * 50000 15.738s (3177.0 samples/s);\n",
      "Finished Epoch[4 of 30]: [Training] loss = 1.122736 * 50000, metric = 39.19% * 50000 15.747s (3175.2 samples/s);\n",
      "Finished Epoch[5 of 30]: [Training] loss = 1.008447 * 50000, metric = 34.41% * 50000 15.727s (3179.2 samples/s);\n",
      "Finished Epoch[6 of 30]: [Training] loss = 0.895551 * 50000, metric = 30.44% * 50000 15.736s (3177.4 samples/s);\n",
      "Finished Epoch[7 of 30]: [Training] loss = 0.832379 * 50000, metric = 28.04% * 50000 15.736s (3177.4 samples/s);\n",
      "Finished Epoch[8 of 30]: [Training] loss = 0.779092 * 50000, metric = 26.20% * 50000 15.733s (3178.0 samples/s);\n",
      "Finished Epoch[9 of 30]: [Training] loss = 0.742882 * 50000, metric = 24.85% * 50000 15.729s (3178.8 samples/s);\n",
      "Finished Epoch[10 of 30]: [Training] loss = 0.707100 * 50000, metric = 23.63% * 50000 15.738s (3177.0 samples/s);\n",
      "Finished Epoch[11 of 30]: [Training] loss = 0.523840 * 50000, metric = 17.49% * 50000 15.732s (3178.2 samples/s);\n",
      "Finished Epoch[12 of 30]: [Training] loss = 0.488219 * 50000, metric = 16.29% * 50000 15.734s (3177.8 samples/s);\n",
      "Finished Epoch[13 of 30]: [Training] loss = 0.465934 * 50000, metric = 15.50% * 50000 15.741s (3176.4 samples/s);\n",
      "Finished Epoch[14 of 30]: [Training] loss = 0.447816 * 50000, metric = 15.06% * 50000 15.749s (3174.8 samples/s);\n",
      "Finished Epoch[15 of 30]: [Training] loss = 0.426678 * 50000, metric = 14.13% * 50000 15.737s (3177.2 samples/s);\n",
      "Finished Epoch[16 of 30]: [Training] loss = 0.415694 * 50000, metric = 13.79% * 50000 15.787s (3167.2 samples/s);\n",
      "Finished Epoch[17 of 30]: [Training] loss = 0.397565 * 50000, metric = 13.37% * 50000 15.791s (3166.4 samples/s);\n",
      "Finished Epoch[18 of 30]: [Training] loss = 0.385472 * 50000, metric = 12.66% * 50000 15.768s (3171.0 samples/s);\n",
      "Finished Epoch[19 of 30]: [Training] loss = 0.377150 * 50000, metric = 12.58% * 50000 15.752s (3174.2 samples/s);\n",
      "Finished Epoch[20 of 30]: [Training] loss = 0.359074 * 50000, metric = 11.94% * 50000 15.758s (3173.0 samples/s);\n",
      "Finished Epoch[21 of 30]: [Training] loss = 0.282116 * 50000, metric = 9.35% * 50000 15.786s (3167.4 samples/s);\n",
      "Finished Epoch[22 of 30]: [Training] loss = 0.255382 * 50000, metric = 8.22% * 50000 15.783s (3168.0 samples/s);\n",
      "Finished Epoch[23 of 30]: [Training] loss = 0.241718 * 50000, metric = 7.91% * 50000 15.795s (3165.6 samples/s);\n",
      "Finished Epoch[24 of 30]: [Training] loss = 0.235032 * 50000, metric = 7.71% * 50000 15.796s (3165.4 samples/s);\n",
      "Finished Epoch[25 of 30]: [Training] loss = 0.226643 * 50000, metric = 7.37% * 50000 15.810s (3162.6 samples/s);\n",
      "Finished Epoch[26 of 30]: [Training] loss = 0.218251 * 50000, metric = 7.18% * 50000 15.808s (3163.0 samples/s);\n",
      "Finished Epoch[27 of 30]: [Training] loss = 0.213856 * 50000, metric = 6.97% * 50000 15.818s (3161.0 samples/s);\n",
      "Finished Epoch[28 of 30]: [Training] loss = 0.204678 * 50000, metric = 6.57% * 50000 15.821s (3160.4 samples/s);\n",
      "Finished Epoch[29 of 30]: [Training] loss = 0.199867 * 50000, metric = 6.44% * 50000 15.823s (3160.0 samples/s);\n",
      "Finished Epoch[30 of 30]: [Training] loss = 0.198118 * 50000, metric = 6.38% * 50000 15.843s (3156.0 samples/s);\n",
      "Starting testing\n",
      "\n",
      "Final Results: Minibatch[1-626]: errs = 18.47% * 10000\n",
      "\n",
      "\n",
      "CPU times: user 14.8 s, sys: 4.74 s, total: 19.5 s\n",
      "Wall time: 12min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!shipyard jobs add --configdir config --tail stdout.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we simply want to review what happened we can execute the command below. Of course when you delete the job all this information is also deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-04-11 08:48:53.086 DEBUG - attempting to stream file stdout.txt from job=cntkjob task=run_cifar10\n",
      "\n",
      "************************************************************\n",
      "CNTK is activated.\n",
      "\n",
      "Please checkout tutorials and examples here:\n",
      "  /cntk/Tutorials\n",
      "  /cntk/Examples\n",
      "\n",
      "To deactivate the environment run\n",
      "\n",
      "  source /root/anaconda3/bin/deactivate\n",
      "\n",
      "************************************************************\n",
      "Downloading http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "Done.\n",
      "Extracting files...\n",
      "Done.\n",
      "Preparing train set...\n",
      "Done.\n",
      "Preparing test set...\n",
      "Done.\n",
      "Writing train text file...\n",
      "Done.\n",
      "Writing test text file...\n",
      "Done.\n",
      "Converting train data to png images...\n",
      "Done.\n",
      "Converting test data to png images...\n",
      "Done.\n",
      "Setting up input variables\n",
      "Creating NN model\n",
      "Training 1195594 parameters in 14 parameter tensors.\n",
      "\n",
      "Starting training\n",
      "Finished Epoch[1 of 30]: [Training] loss = 1.943535 * 50000, metric = 71.96% * 50000 19.817s (2523.1 samples/s);\n",
      "Finished Epoch[2 of 30]: [Training] loss = 1.524170 * 50000, metric = 55.13% * 50000 15.728s (3179.0 samples/s);\n",
      "Finished Epoch[3 of 30]: [Training] loss = 1.294059 * 50000, metric = 45.84% * 50000 15.738s (3177.0 samples/s);\n",
      "Finished Epoch[4 of 30]: [Training] loss = 1.122736 * 50000, metric = 39.19% * 50000 15.747s (3175.2 samples/s);\n",
      "Finished Epoch[5 of 30]: [Training] loss = 1.008447 * 50000, metric = 34.41% * 50000 15.727s (3179.2 samples/s);\n",
      "Finished Epoch[6 of 30]: [Training] loss = 0.895551 * 50000, metric = 30.44% * 50000 15.736s (3177.4 samples/s);\n",
      "Finished Epoch[7 of 30]: [Training] loss = 0.832379 * 50000, metric = 28.04% * 50000 15.736s (3177.4 samples/s);\n",
      "Finished Epoch[8 of 30]: [Training] loss = 0.779092 * 50000, metric = 26.20% * 50000 15.733s (3178.0 samples/s);\n",
      "Finished Epoch[9 of 30]: [Training] loss = 0.742882 * 50000, metric = 24.85% * 50000 15.729s (3178.8 samples/s);\n",
      "Finished Epoch[10 of 30]: [Training] loss = 0.707100 * 50000, metric = 23.63% * 50000 15.738s (3177.0 samples/s);\n",
      "Finished Epoch[11 of 30]: [Training] loss = 0.523840 * 50000, metric = 17.49% * 50000 15.732s (3178.2 samples/s);\n",
      "Finished Epoch[12 of 30]: [Training] loss = 0.488219 * 50000, metric = 16.29% * 50000 15.734s (3177.8 samples/s);\n",
      "Finished Epoch[13 of 30]: [Training] loss = 0.465934 * 50000, metric = 15.50% * 50000 15.741s (3176.4 samples/s);\n",
      "Finished Epoch[14 of 30]: [Training] loss = 0.447816 * 50000, metric = 15.06% * 50000 15.749s (3174.8 samples/s);\n",
      "Finished Epoch[15 of 30]: [Training] loss = 0.426678 * 50000, metric = 14.13% * 50000 15.737s (3177.2 samples/s);\n",
      "Finished Epoch[16 of 30]: [Training] loss = 0.415694 * 50000, metric = 13.79% * 50000 15.787s (3167.2 samples/s);\n",
      "Finished Epoch[17 of 30]: [Training] loss = 0.397565 * 50000, metric = 13.37% * 50000 15.791s (3166.4 samples/s);\n",
      "Finished Epoch[18 of 30]: [Training] loss = 0.385472 * 50000, metric = 12.66% * 50000 15.768s (3171.0 samples/s);\n",
      "Finished Epoch[19 of 30]: [Training] loss = 0.377150 * 50000, metric = 12.58% * 50000 15.752s (3174.2 samples/s);\n",
      "Finished Epoch[20 of 30]: [Training] loss = 0.359074 * 50000, metric = 11.94% * 50000 15.758s (3173.0 samples/s);\n",
      "Finished Epoch[21 of 30]: [Training] loss = 0.282116 * 50000, metric = 9.35% * 50000 15.786s (3167.4 samples/s);\n",
      "Finished Epoch[22 of 30]: [Training] loss = 0.255382 * 50000, metric = 8.22% * 50000 15.783s (3168.0 samples/s);\n",
      "Finished Epoch[23 of 30]: [Training] loss = 0.241718 * 50000, metric = 7.91% * 50000 15.795s (3165.6 samples/s);\n",
      "Finished Epoch[24 of 30]: [Training] loss = 0.235032 * 50000, metric = 7.71% * 50000 15.796s (3165.4 samples/s);\n",
      "Finished Epoch[25 of 30]: [Training] loss = 0.226643 * 50000, metric = 7.37% * 50000 15.810s (3162.6 samples/s);\n",
      "Finished Epoch[26 of 30]: [Training] loss = 0.218251 * 50000, metric = 7.18% * 50000 15.808s (3163.0 samples/s);\n",
      "Finished Epoch[27 of 30]: [Training] loss = 0.213856 * 50000, metric = 6.97% * 50000 15.818s (3161.0 samples/s);\n",
      "Finished Epoch[28 of 30]: [Training] loss = 0.204678 * 50000, metric = 6.57% * 50000 15.821s (3160.4 samples/s);\n",
      "Finished Epoch[29 of 30]: [Training] loss = 0.199867 * 50000, metric = 6.44% * 50000 15.823s (3160.0 samples/s);\n",
      "Finished Epoch[30 of 30]: [Training] loss = 0.198118 * 50000, metric = 6.38% * 50000 15.843s (3156.0 samples/s);\n",
      "Starting testing\n",
      "\n",
      "Final Results: Minibatch[1-626]: errs = 18.47% * 10000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!shipyard data stream --configdir config --filespec cntkjob,run_cifar10,stdout.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If something goes wrong you can run the following command to get the stderr output from the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-04-11 08:48:58.680 DEBUG - attempting to stream file stderr.txt from job=cntkjob task=run_cifar10\n",
      "-------------------------------------------------------------------\n",
      "Build info: \n",
      "\n",
      "\t\tBuilt time: Apr  3 2017 07:05:06\n",
      "\t\tLast modified date: Mon Apr  3 05:11:23 2017\n",
      "\t\tBuild type: release\n",
      "\t\tBuild target: GPU\n",
      "\t\tWith 1bit-SGD: no\n",
      "\t\tWith ASGD: yes\n",
      "\t\tMath lib: mkl\n",
      "\t\tCUDA_PATH: /usr/local/cuda-8.0\n",
      "\t\tCUB_PATH: /usr/local/cub-1.4.1\n",
      "\t\tCUDNN_PATH: /usr/local/cudnn-5.1\n",
      "\t\tBuild Branch: HEAD\n",
      "\t\tBuild SHA1: 7661e81777360d3222a26dcb969973ce1d4c513f\n",
      "\t\tBuilt by Source/CNTK/buildinfo.h$$0 on ef88a481c30f\n",
      "\t\tBuild Path: /home/philly/jenkins/workspace/CNTK-Build-Linux\n",
      "\t\tMPI distribution: Open MPI\n",
      "\t\tMPI version: 1.10.3\n",
      "-------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!shipyard data stream --configdir config --filespec cntkjob,run_cifar10,stderr.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To delete the job use the command below. Just beware that this will get rid of all the files created by the job and tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-04-11 08:49:51.208 INFO - Deleting job: cntkjob\n",
      "2017-04-11 08:49:51.784 DEBUG - waiting for job cntkjob to delete\n"
     ]
    }
   ],
   "source": [
    "!shipyard jobs del -y --configdir config --wait"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deallocate the VM simply execute the command below. If you do not the VM will be running idle and you will continue to incur charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-04-11 08:50:25.623 INFO - Deleting pool: singlegpu\n",
      "2017-04-11 08:50:25.941 DEBUG - clearing table (pk=batchcntkacc$singlegpu): shipyardtorrentinfo\n",
      "2017-04-11 08:50:26.418 DEBUG - clearing table (pk=batchcntkacc$singlegpu): shipyardimages\n",
      "2017-04-11 08:50:26.540 DEBUG - clearing table (pk=batchcntkacc$singlegpu): shipyardgr\n",
      "2017-04-11 08:50:26.666 DEBUG - clearing table (pk=batchcntkacc$singlegpu): shipyardregistry\n",
      "2017-04-11 08:50:26.789 DEBUG - clearing table (pk=batchcntkacc$singlegpu): shipyarddht\n",
      "2017-04-11 08:50:26.857 DEBUG - clearing table (pk=batchcntkacc$singlegpu): shipyardperf\n",
      "2017-04-11 08:50:26.912 DEBUG - deleting queue: shipyardgr-batchcntkacc-singlegpu\n",
      "2017-04-11 08:50:27.493 DEBUG - deleting container: shipyardrf-batchcntkacc-singlegpu\n",
      "2017-04-11 08:50:27.814 DEBUG - deleting container: shipyardtor-batchcntkacc-singlegpu\n",
      "2017-04-11 08:50:27.874 DEBUG - waiting for pool singlegpu to delete\n"
     ]
    }
   ],
   "source": [
    "!shipyard pool del -y --configdir config --wait"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section6'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete everything\n",
    "Once you have deleted the pool all that remains is the storage account and the Batch account. You will only incur a cost for the storage account but if you wish to delete everything execute the commands below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mStarting long running operation 'Starting group delete'\u001b[0m\n",
      "\u001b[32mLong running operation 'Starting group delete' completed with result None\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!az group delete -n $group_name --yes --verbose"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:ship3]",
   "language": "python",
   "name": "conda-env-ship3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
