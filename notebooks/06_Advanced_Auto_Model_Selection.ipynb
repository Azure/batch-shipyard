{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI - Automatic Model Selection from Parametric Sweep using Task Dependencies\n",
    "In this notebook we will be taking the example from the [Parametric Sweep](04_Parameter_Sweep.ipynb) notebook and automating the entire chain using task dependencies in a single Azure Batch job.\n",
    "\n",
    "* [Setup](#section1)\n",
    "* [Configure job](#section2)\n",
    "* [Submit job](#section3)\n",
    "* [Download best model](#section4)\n",
    "* [Delete job](#section5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a simple alias for Batch Shipyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%alias shipyard SHIPYARD_CONFIGDIR=config python $HOME/batch-shipyard/shipyard.py %l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that everything is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: shipyard.py [OPTIONS] COMMAND [ARGS]...\r\n",
      "\r\n",
      "  Batch Shipyard: Provision and Execute Docker Workloads on Azure Batch\r\n",
      "\r\n",
      "Options:\r\n",
      "  --version   Show the version and exit.\r\n",
      "  -h, --help  Show this message and exit.\r\n",
      "\r\n",
      "Commands:\r\n",
      "  cert      Certificate actions\r\n",
      "  data      Data actions\r\n",
      "  fs        Filesystem in Azure actions\r\n",
      "  jobs      Jobs actions\r\n",
      "  keyvault  KeyVault actions\r\n",
      "  misc      Miscellaneous actions\r\n",
      "  pool      Pool actions\r\n",
      "  storage   Storage actions\r\n"
     ]
    }
   ],
   "source": [
    "shipyard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the account information we saved earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_json(filename):\n",
    "    with open(filename, 'r') as infile:\n",
    "        return json.load(infile)\n",
    "    \n",
    "account_info = read_json('account_information.json')\n",
    "\n",
    "storage_account_key = account_info['storage_account_key']\n",
    "storage_account_name = account_info['storage_account_name']\n",
    "IMAGE_NAME = account_info['IMAGE_NAME']\n",
    "STORAGE_ALIAS = account_info['STORAGE_ALIAS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous job we ran on a single node we will be running the job on GPU enabled nodes. The difference here is that depending on the number of combinations we will be creating the same number of tasks. Each task will have a different set of parmeters that we will be passing to our model training script. This parameters effect the training of the model and in the end the performance of the model. The model and results of its evaluation are recorded and stored on the node. At the end of the task the results are pulled into the specified storage container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "import os\n",
    "import random\n",
    "from sklearn.grid_search import ParameterGrid\n",
    "from toolz import curry, pipe\n",
    "\n",
    "def write_json_to_file(json_dict, filename):\n",
    "    \"\"\" Simple function to write JSON dictionaries to files\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as outfile:\n",
    "        json.dump(json_dict, outfile)\n",
    "\n",
    "def compose_command(num_convolution_layers, minibatch_size, max_epochs=30):\n",
    "    cmd_str = ' '.join((\"source /cntk/activate-cntk;\",\n",
    "                        \"python -u ConvNet_CIFAR10.py\",\n",
    "                        \"--datadir $AZ_BATCH_NODE_SHARED_DIR/data\",\n",
    "                        \"--num_convolution_layers {num_convolution_layers}\",\n",
    "                        \"--minibatch_size {minibatch_size}\",\n",
    "                        \"--max_epochs {max_epochs}\")).format(num_convolution_layers=num_convolution_layers,\n",
    "                                                             minibatch_size=minibatch_size,\n",
    "                                                             max_epochs=max_epochs)\n",
    "    return 'bash -c \"{}\"'.format(cmd_str)\n",
    "\n",
    "@curry\n",
    "def append_parameter(param_name, param_value, data_dict):\n",
    "    data_dict[param_name]=param_value\n",
    "    return data_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the task template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_TASK_TEMPLATE = {\n",
    "    \"image\": IMAGE_NAME,\n",
    "    \"remove_container_after_exit\": True,\n",
    "    \"gpu\": True,\n",
    "    \"resource_files\": [\n",
    "            {\n",
    "                \"file_path\": \"ConvNet_CIFAR10.py\",\n",
    "                \"blob_source\": \"https://batchshipyardexamples.blob.core.windows.net/code/ConvNet_CIFAR10.py\",\n",
    "                \"file_mode\":'0777'\n",
    "            }\n",
    "    ],\n",
    "    \"output_data\": {\n",
    "        \"azure_storage\": [\n",
    "            {\n",
    "                \"storage_account_settings\": STORAGE_ALIAS,\n",
    "                \"container\": \"output\",\n",
    "                \"source\": \"$AZ_BATCH_TASK_DIR/wd/Models\"\n",
    "            },\n",
    "        ]\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we modify the `task_generator` to append an additional parameter to each task definition with a specific `id` numbered from `0`. This is done so we can reference each dependent task in the selection task that must run after each training task has completed successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def task_generator(parameters):\n",
    "    id = 0\n",
    "    for params in ParameterGrid(parameters):\n",
    "        yield pipe(copy(_TASK_TEMPLATE),\n",
    "                   append_parameter('command', compose_command(**params)),\n",
    "                   append_parameter('id', str(id)))\n",
    "        id += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the `jobs.json` configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"num_convolution_layers\": [2, 3],\n",
    "    \"minibatch_size\": [32, 64]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tasks for parametric sweep cntk-ps-as-job: 4\n"
     ]
    }
   ],
   "source": [
    "JOB_ID = 'cntk-ps-as-job'\n",
    "\n",
    "jobs = {\n",
    "    \"job_specifications\": [\n",
    "        {\n",
    "            \"id\": JOB_ID,\n",
    "            \"tasks\": list(task_generator(parameters))    \n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "num_parameter_sweep_tasks = len(jobs['job_specifications'][0]['tasks'])\n",
    "print('number of tasks for parametric sweep {}: {}'.format(JOB_ID, num_parameter_sweep_tasks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create the Python program to run that performs the best model selection. Note that this code is nearly similar to the code for selecting the best model locally in the [Parameter sweep notebook](04_Parameter_Sweep.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing autoselect.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile autoselect.py\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def read_json(filename):\n",
    "    with open(filename, 'r') as infile:\n",
    "        return json.load(infile)\n",
    "\n",
    "def scandir(basedir):\n",
    "    for root, dirs, files in os.walk(basedir):\n",
    "        for f in files:\n",
    "            yield os.path.join(root, f) \n",
    "\n",
    "MODELS_DIR = os.path.join('wd', 'Models')\n",
    "            \n",
    "results_dict = {}\n",
    "for model in scandir(MODELS_DIR):\n",
    "    if not model.endswith('.json'):\n",
    "        continue\n",
    "    key = model.split(os.sep)[2]  # due to MODELS_DIR path change\n",
    "    results_dict[key] = read_json(model)\n",
    "\n",
    "# use items() instead of iteritems() as this will be run in python3\n",
    "tuple_min_error = min(results_dict.items(), key=lambda x: x[1]['test_metric'])\n",
    "configuration_with_min_error = tuple_min_error[0]\n",
    "print('task with smallest error: {} ({})'.format(configuration_with_min_error, tuple_min_error[1]['test_metric']))\n",
    "\n",
    "# copy best model to wd\n",
    "MODEL_NAME = 'ConvNet_CIFAR10_model.dnn'\n",
    "shutil.copy(os.path.join(MODELS_DIR, configuration_with_min_error, MODEL_NAME), '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to prepare the file to be uploaded to the Azure Storage account to be referenced in the task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 12\r\n",
      "drwxr-xr-x  2 nbuser nbuser 4096 Jun 21 15:25 ./\r\n",
      "drwx------ 19 nbuser nbuser 4096 Jun 21 15:25 ../\r\n",
      "-rw-r--r--  1 nbuser nbuser 1001 Jun 21 15:25 autoselect.py\r\n"
     ]
    }
   ],
   "source": [
    "INPUT_CONTAINER = 'input-autoselect'\n",
    "OUTPUT_CONTAINER = 'output-autoselect'\n",
    "UPLOAD_DIR = 'autoselect_upload'\n",
    "\n",
    "!rm -rf $UPLOAD_DIR\n",
    "!mkdir -p $UPLOAD_DIR\n",
    "!mv autoselect.py $UPLOAD_DIR\n",
    "!ls -alF $UPLOAD_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alias `blobxfer` and upload it to `INPUT_CONTAINER`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%alias blobxfer python -m blobxfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      " azure blobxfer parameters [v0.12.1]\n",
      "=====================================\n",
      "             platform: Linux-4.4.0-75-generic-x86_64-with-debian-stretch-sid\n",
      "   python interpreter: CPython 2.7.11\n",
      "     package versions: az.common=1.1.5 az.sml=0.20.5 az.stor=0.34.2 crypt=1.9 req=2.18.1\n",
      "      subscription id: None\n",
      "      management cert: None\n",
      "   transfer direction: local->Azure\n",
      "       local resource: autoselect_upload\n",
      "      include pattern: None\n",
      "      remote resource: None\n",
      "   max num of workers: 24\n",
      "              timeout: None\n",
      "      storage account: batcha3dc41fdst\n",
      "              use SAS: False\n",
      "  upload as page blob: False\n",
      "  auto vhd->page blob: False\n",
      " upload to file share: False\n",
      " container/share name: input-autoselect\n",
      "  container/share URI: https://batcha3dc41fdst.blob.core.windows.net/input-autoselect\n",
      "    compute block MD5: False\n",
      "     compute file MD5: True\n",
      "    skip on MD5 match: True\n",
      "   chunk size (bytes): 4194304\n",
      "     create container: True\n",
      "  keep mismatched MD5: False\n",
      "     recursive if dir: True\n",
      "component strip on up: 1\n",
      "        remote delete: False\n",
      "           collate to: disabled\n",
      "      local overwrite: True\n",
      "      encryption mode: disabled\n",
      "         RSA key file: disabled\n",
      "         RSA key type: disabled\n",
      "=======================================\n",
      "\n",
      "script start time: 2017-06-21 15:25:41\n",
      "computing file md5 on: autoselect_upload/autoselect.py\n",
      "  >> md5: A0V93+uyerutTDe7jckLLA==\n",
      "creating container, if needed: input-autoselect\n",
      "detected 0 empty files to upload\n",
      "performing 1 put blocks/blobs and 1 put block lists\n",
      "spawning 1 worker threads\n",
      "xfer progress: [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100.00%     596.83 blocks/min    \n",
      "\n",
      "0.000954627990723 MiB transfered, elapsed 0.10052895546 sec. Throughput = 0.0759684002571 Mbit/sec\n",
      "\n",
      "\n",
      "script elapsed time: 0.471256971359 sec\n",
      "script end time: 2017-06-21 15:25:41\n"
     ]
    }
   ],
   "source": [
    "blobxfer $storage_account_name $INPUT_CONTAINER $UPLOAD_DIR --upload --storageaccountkey $storage_account_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll append the `auto-model-selection` task which depends on the prior training tasks. The important properties here are `depends_on_range` which specifies a range of task ids the `auto-model-selection` task depends on. Additionally, this task requires data from the prior run task which is specified in `input_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_input_data_spec(job_id, task_id):\n",
    "    return {\n",
    "        \"job_id\": job_id,\n",
    "        \"task_id\": task_id,\n",
    "        \"include\": [\"wd/Models/*_{}_{}/*\".format(task_id, job_id)]\n",
    "    }\n",
    "\n",
    "input_data = []\n",
    "for x in range(0, num_parameter_sweep_tasks):\n",
    "    input_data.append(generate_input_data_spec(JOB_ID, str(x)))\n",
    "\n",
    "model_selection_task = {\n",
    "    \"id\": \"auto-model-selection\",\n",
    "    \"command\": 'bash -c \"source /cntk/activate-cntk; python -u autoselect.py\"',\n",
    "    \"depends_on_range\": [0, num_parameter_sweep_tasks - 1],\n",
    "    \"image\": IMAGE_NAME,\n",
    "    \"remove_container_after_exit\": True,\n",
    "    \"input_data\": {\n",
    "        \"azure_batch\": input_data,\n",
    "        \"azure_storage\": [\n",
    "            {\n",
    "                \"storage_account_settings\": STORAGE_ALIAS,\n",
    "                \"container\": INPUT_CONTAINER\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"output_data\": {\n",
    "        \"azure_storage\": [\n",
    "            {\n",
    "                \"storage_account_settings\": STORAGE_ALIAS,\n",
    "                \"container\": OUTPUT_CONTAINER,\n",
    "                \"include\": [\"*wd/ConvNet_CIFAR10_model.dnn\"],\n",
    "                \"blobxfer_extra_options\": \"--delete --strip-components 2\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# append auto-model-selection task to jobs\n",
    "jobs['job_specifications'][0]['tasks'].append(model_selection_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"job_specifications\": [\n",
      "        {\n",
      "            \"id\": \"cntk-ps-as-job\", \n",
      "            \"tasks\": [\n",
      "                {\n",
      "                    \"command\": \"bash -c \\\"source /cntk/activate-cntk; python -u ConvNet_CIFAR10.py --datadir $AZ_BATCH_NODE_SHARED_DIR/data --num_convolution_layers 2 --minibatch_size 32 --max_epochs 30\\\"\", \n",
      "                    \"gpu\": true, \n",
      "                    \"id\": \"0\", \n",
      "                    \"image\": \"microsoft/cntk:2.0-gpu-python3.5-cuda8.0-cudnn5.1\", \n",
      "                    \"output_data\": {\n",
      "                        \"azure_storage\": [\n",
      "                            {\n",
      "                                \"container\": \"output\", \n",
      "                                \"source\": \"$AZ_BATCH_TASK_DIR/wd/Models\", \n",
      "                                \"storage_account_settings\": \"mystorageaccount\"\n",
      "                            }\n",
      "                        ]\n",
      "                    }, \n",
      "                    \"remove_container_after_exit\": true, \n",
      "                    \"resource_files\": [\n",
      "                        {\n",
      "                            \"blob_source\": \"https://batchshipyardexamples.blob.core.windows.net/code/ConvNet_CIFAR10.py\", \n",
      "                            \"file_mode\": \"0777\", \n",
      "                            \"file_path\": \"ConvNet_CIFAR10.py\"\n",
      "                        }\n",
      "                    ]\n",
      "                }, \n",
      "                {\n",
      "                    \"command\": \"bash -c \\\"source /cntk/activate-cntk; python -u ConvNet_CIFAR10.py --datadir $AZ_BATCH_NODE_SHARED_DIR/data --num_convolution_layers 3 --minibatch_size 32 --max_epochs 30\\\"\", \n",
      "                    \"gpu\": true, \n",
      "                    \"id\": \"1\", \n",
      "                    \"image\": \"microsoft/cntk:2.0-gpu-python3.5-cuda8.0-cudnn5.1\", \n",
      "                    \"output_data\": {\n",
      "                        \"azure_storage\": [\n",
      "                            {\n",
      "                                \"container\": \"output\", \n",
      "                                \"source\": \"$AZ_BATCH_TASK_DIR/wd/Models\", \n",
      "                                \"storage_account_settings\": \"mystorageaccount\"\n",
      "                            }\n",
      "                        ]\n",
      "                    }, \n",
      "                    \"remove_container_after_exit\": true, \n",
      "                    \"resource_files\": [\n",
      "                        {\n",
      "                            \"blob_source\": \"https://batchshipyardexamples.blob.core.windows.net/code/ConvNet_CIFAR10.py\", \n",
      "                            \"file_mode\": \"0777\", \n",
      "                            \"file_path\": \"ConvNet_CIFAR10.py\"\n",
      "                        }\n",
      "                    ]\n",
      "                }, \n",
      "                {\n",
      "                    \"command\": \"bash -c \\\"source /cntk/activate-cntk; python -u ConvNet_CIFAR10.py --datadir $AZ_BATCH_NODE_SHARED_DIR/data --num_convolution_layers 2 --minibatch_size 64 --max_epochs 30\\\"\", \n",
      "                    \"gpu\": true, \n",
      "                    \"id\": \"2\", \n",
      "                    \"image\": \"microsoft/cntk:2.0-gpu-python3.5-cuda8.0-cudnn5.1\", \n",
      "                    \"output_data\": {\n",
      "                        \"azure_storage\": [\n",
      "                            {\n",
      "                                \"container\": \"output\", \n",
      "                                \"source\": \"$AZ_BATCH_TASK_DIR/wd/Models\", \n",
      "                                \"storage_account_settings\": \"mystorageaccount\"\n",
      "                            }\n",
      "                        ]\n",
      "                    }, \n",
      "                    \"remove_container_after_exit\": true, \n",
      "                    \"resource_files\": [\n",
      "                        {\n",
      "                            \"blob_source\": \"https://batchshipyardexamples.blob.core.windows.net/code/ConvNet_CIFAR10.py\", \n",
      "                            \"file_mode\": \"0777\", \n",
      "                            \"file_path\": \"ConvNet_CIFAR10.py\"\n",
      "                        }\n",
      "                    ]\n",
      "                }, \n",
      "                {\n",
      "                    \"command\": \"bash -c \\\"source /cntk/activate-cntk; python -u ConvNet_CIFAR10.py --datadir $AZ_BATCH_NODE_SHARED_DIR/data --num_convolution_layers 3 --minibatch_size 64 --max_epochs 30\\\"\", \n",
      "                    \"gpu\": true, \n",
      "                    \"id\": \"3\", \n",
      "                    \"image\": \"microsoft/cntk:2.0-gpu-python3.5-cuda8.0-cudnn5.1\", \n",
      "                    \"output_data\": {\n",
      "                        \"azure_storage\": [\n",
      "                            {\n",
      "                                \"container\": \"output\", \n",
      "                                \"source\": \"$AZ_BATCH_TASK_DIR/wd/Models\", \n",
      "                                \"storage_account_settings\": \"mystorageaccount\"\n",
      "                            }\n",
      "                        ]\n",
      "                    }, \n",
      "                    \"remove_container_after_exit\": true, \n",
      "                    \"resource_files\": [\n",
      "                        {\n",
      "                            \"blob_source\": \"https://batchshipyardexamples.blob.core.windows.net/code/ConvNet_CIFAR10.py\", \n",
      "                            \"file_mode\": \"0777\", \n",
      "                            \"file_path\": \"ConvNet_CIFAR10.py\"\n",
      "                        }\n",
      "                    ]\n",
      "                }, \n",
      "                {\n",
      "                    \"command\": \"bash -c \\\"source /cntk/activate-cntk; python -u autoselect.py\\\"\", \n",
      "                    \"depends_on_range\": [\n",
      "                        0, \n",
      "                        3\n",
      "                    ], \n",
      "                    \"id\": \"auto-model-selection\", \n",
      "                    \"image\": \"microsoft/cntk:2.0-gpu-python3.5-cuda8.0-cudnn5.1\", \n",
      "                    \"input_data\": {\n",
      "                        \"azure_batch\": [\n",
      "                            {\n",
      "                                \"include\": [\n",
      "                                    \"wd/Models/*_0_cntk-ps-as-job/*\"\n",
      "                                ], \n",
      "                                \"job_id\": \"cntk-ps-as-job\", \n",
      "                                \"task_id\": \"0\"\n",
      "                            }, \n",
      "                            {\n",
      "                                \"include\": [\n",
      "                                    \"wd/Models/*_1_cntk-ps-as-job/*\"\n",
      "                                ], \n",
      "                                \"job_id\": \"cntk-ps-as-job\", \n",
      "                                \"task_id\": \"1\"\n",
      "                            }, \n",
      "                            {\n",
      "                                \"include\": [\n",
      "                                    \"wd/Models/*_2_cntk-ps-as-job/*\"\n",
      "                                ], \n",
      "                                \"job_id\": \"cntk-ps-as-job\", \n",
      "                                \"task_id\": \"2\"\n",
      "                            }, \n",
      "                            {\n",
      "                                \"include\": [\n",
      "                                    \"wd/Models/*_3_cntk-ps-as-job/*\"\n",
      "                                ], \n",
      "                                \"job_id\": \"cntk-ps-as-job\", \n",
      "                                \"task_id\": \"3\"\n",
      "                            }\n",
      "                        ], \n",
      "                        \"azure_storage\": [\n",
      "                            {\n",
      "                                \"container\": \"input-autoselect\", \n",
      "                                \"storage_account_settings\": \"mystorageaccount\"\n",
      "                            }\n",
      "                        ]\n",
      "                    }, \n",
      "                    \"output_data\": {\n",
      "                        \"azure_storage\": [\n",
      "                            {\n",
      "                                \"blobxfer_extra_options\": \"--delete --strip-components 2\", \n",
      "                                \"container\": \"output-autoselect\", \n",
      "                                \"include\": [\n",
      "                                    \"*wd/ConvNet_CIFAR10_model.dnn\"\n",
      "                                ], \n",
      "                                \"storage_account_settings\": \"mystorageaccount\"\n",
      "                            }\n",
      "                        ]\n",
      "                    }, \n",
      "                    \"remove_container_after_exit\": true\n",
      "                }\n",
      "            ]\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "write_json_to_file(jobs, os.path.join('config', 'jobs.json'))\n",
    "print(json.dumps(jobs, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit job\n",
    "Check that everything is ok with our pool before we submit our jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-21 10:00:47,791 INFO - pool_id=gpupool [state=PoolState.active allocation_state=AllocationState.steady vm_size=standard_nc6 node_agent=batch.node.ubuntu 16.04 vm_dedicated_count=3 target_vm_dedicated_count=3 vm_low_priority_count=0 target_vm_low_priority_count=0]\r\n"
     ]
    }
   ],
   "source": [
    "shipyard pool list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have confirmed everything is working we can execute our job using the command below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-21 15:27:29,203 INFO - Adding job cntk-ps-as-job to pool gpupool\n",
      "2017-06-21 15:27:29,794 INFO - uploading file /tmp/tmpUzvZEY as u'shipyardtaskrf-cntk-ps-as-job/0.shipyard.envlist'\n",
      "2017-06-21 15:27:30,060 INFO - uploading file /tmp/tmpMQ8DhB as u'shipyardtaskrf-cntk-ps-as-job/1.shipyard.envlist'\n",
      "2017-06-21 15:27:30,301 INFO - uploading file /tmp/tmpPE2ZcQ as u'shipyardtaskrf-cntk-ps-as-job/2.shipyard.envlist'\n",
      "2017-06-21 15:27:30,558 INFO - uploading file /tmp/tmpawAWLm as u'shipyardtaskrf-cntk-ps-as-job/3.shipyard.envlist'\n",
      "2017-06-21 15:27:31,015 DEBUG - submitting 5 tasks (0 -> 4) to job cntk-ps-as-job\n",
      "2017-06-21 15:27:31,309 INFO - submitted all 5 tasks to job cntk-ps-as-job\n"
     ]
    }
   ],
   "source": [
    "shipyard jobs add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the command below we can check the status of our jobs. Once all jobs have an exit code we can continue. You can also view the **heatmap** of this pool on [Azure Portal](https://portal.azure.com) to monitor the progress of this job on the compute nodes under your Batch account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-21 10:19:56,818 INFO - job_id=cntk-ps-as-job task_id=0 [state=TaskState.completed max_retries=0 retention_time=10675199 days, 2:48:05.477581 pool_id=gpupool node_id=tvm-4283973576_2-20170621t092540z start_time=2017-06-21 10:00:57.299984+00:00 end_time=2017-06-21 10:11:14.302835+00:00 duration=0:10:17.002851 exit_code=0]\r\n",
      "2017-06-21 10:19:56,818 INFO - job_id=cntk-ps-as-job task_id=1 [state=TaskState.completed max_retries=0 retention_time=10675199 days, 2:48:05.477581 pool_id=gpupool node_id=tvm-4283973576_3-20170621t092540z start_time=2017-06-21 10:00:56.712625+00:00 end_time=2017-06-21 10:12:26.286121+00:00 duration=0:11:29.573496 exit_code=0]\r\n",
      "2017-06-21 10:19:56,819 INFO - job_id=cntk-ps-as-job task_id=2 [state=TaskState.completed max_retries=0 retention_time=10675199 days, 2:48:05.477581 pool_id=gpupool node_id=tvm-4283973576_1-20170621t092540z start_time=2017-06-21 10:10:02.125950+00:00 end_time=2017-06-21 10:18:01.573246+00:00 duration=0:07:59.447296 exit_code=0]\r\n",
      "2017-06-21 10:19:56,819 INFO - job_id=cntk-ps-as-job task_id=3 [state=TaskState.completed max_retries=0 retention_time=10675199 days, 2:48:05.477581 pool_id=gpupool node_id=tvm-4283973576_1-20170621t092540z start_time=2017-06-21 10:00:56.978430+00:00 end_time=2017-06-21 10:10:01.172899+00:00 duration=0:09:04.194469 exit_code=0]\r\n",
      "2017-06-21 10:19:56,819 INFO - job_id=cntk-ps-as-job task_id=auto-model-selection [state=TaskState.completed max_retries=0 retention_time=10675199 days, 2:48:05.477581 pool_id=gpupool node_id=tvm-4283973576_3-20170621t092540z start_time=2017-06-21 10:18:15.072862+00:00 end_time=2017-06-21 10:18:24.864115+00:00 duration=0:00:09.791253 exit_code=0]\r\n"
     ]
    }
   ],
   "source": [
    "shipyard jobs listtasks --jobid $JOB_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download best model\n",
    "The best performing model from the parametric sweep job should now be saved to our `OUTPUT_CONTAINER` container by the `auto-model-selection` task. Let's save this model in `MODELS_DIR`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MODELS_DIR = 'auto-selected-model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the best performing model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      " azure blobxfer parameters [v0.12.1]\n",
      "=====================================\n",
      "             platform: Linux-4.4.0-75-generic-x86_64-with-debian-stretch-sid\n",
      "   python interpreter: CPython 2.7.11\n",
      "     package versions: az.common=1.1.5 az.sml=0.20.5 az.stor=0.34.2 crypt=1.9 req=2.18.1\n",
      "      subscription id: None\n",
      "      management cert: None\n",
      "   transfer direction: Azure->local\n",
      "       local resource: auto-selected-model\n",
      "      include pattern: None\n",
      "      remote resource: .\n",
      "   max num of workers: 24\n",
      "              timeout: None\n",
      "      storage account: batche2914cdbst\n",
      "              use SAS: False\n",
      "  upload as page blob: False\n",
      "  auto vhd->page blob: False\n",
      " upload to file share: False\n",
      " container/share name: output-autoselect\n",
      "  container/share URI: https://batche2914cdbst.blob.core.windows.net/output-autoselect\n",
      "    compute block MD5: False\n",
      "     compute file MD5: True\n",
      "    skip on MD5 match: True\n",
      "   chunk size (bytes): 4194304\n",
      "     create container: True\n",
      "  keep mismatched MD5: False\n",
      "     recursive if dir: True\n",
      "component strip on up: 1\n",
      "        remote delete: False\n",
      "           collate to: disabled\n",
      "      local overwrite: True\n",
      "      encryption mode: disabled\n",
      "         RSA key file: disabled\n",
      "         RSA key type: disabled\n",
      "=======================================\n",
      "\n",
      "script start time: 2017-06-21 10:20:07\n",
      "attempting to copy entire container output-autoselect to auto-selected-model\n",
      "generating local directory structure and pre-allocating space\n",
      "created local directory: auto-selected-model\n",
      "remote blob: ConvNet_CIFAR10_model.dnn length: 4802199 bytes, md5: LUj5QwGxbKuMXXKMvYbsHA==\n",
      "performing 2 range-gets\n",
      "spawning 2 worker threads\n",
      "xfer progress: [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100.00%     133.50 range-gets/min    \n",
      "\n",
      "4.57973384857 MiB transfered, elapsed 0.8988468647 sec. Throughput = 40.7609707809 Mbit/sec\n",
      "\n",
      "performing finalization (if applicable): HMAC-SHA256: False, MD5: True\n",
      "[MD5: OK, auto-selected-model/ConvNet_CIFAR10_model.dnn] LUj5QwGxbKuMXXKMvYbsHA== <L..R> LUj5QwGxbKuMXXKMvYbsHA==\n",
      "finalization complete.\n",
      "\n",
      "script elapsed time: 1.16924500465 sec\n",
      "script end time: 2017-06-21 10:20:08\n"
     ]
    }
   ],
   "source": [
    "blobxfer $storage_account_name $OUTPUT_CONTAINER $MODELS_DIR --remoteresource . --download --storageaccountkey $storage_account_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model file (`ConvNet_CIFAR10_model.dnn`) is now ready for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4700\r\n",
      "drwxr-xr-x  2 nbuser nbuser    4096 Jun 21 10:20 ./\r\n",
      "drwx------ 13 nbuser nbuser    4096 Jun 21 10:20 ../\r\n",
      "-rw-r--r--  1 nbuser nbuser 4802199 Jun 21 10:20 ConvNet_CIFAR10_model.dnn\r\n"
     ]
    }
   ],
   "source": [
    "!ls -alF $MODELS_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To delete the job use the command below. Just be aware that this will get rid of all the files created by the job and tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-21 15:30:24,436 INFO - Deleting job: cntk-ps-as-job\n",
      "2017-06-21 15:30:24,436 DEBUG - disabling job cntk-ps-as-job first due to task termination\n",
      "2017-06-21 15:30:26,047 INFO - Terminating task: 0\n",
      "Warning: Permanently added '[52.186.123.38]:50001' (ECDSA) to the list of known hosts.\n",
      "cntk-ps-as-job-0\n",
      "Connection to 52.186.123.38 closed.\n",
      "2017-06-21 15:30:27,948 INFO - Terminating task: 1\n",
      "Warning: Permanently added '[52.186.123.38]:50002' (ECDSA) to the list of known hosts.\n",
      "cntk-ps-as-job-1\n",
      "Connection to 52.186.123.38 closed.\n",
      "2017-06-21 15:30:29,564 INFO - Terminating task: 2\n",
      "2017-06-21 15:30:29,949 INFO - Terminating task: 3\n",
      "Warning: Permanently added '[52.186.123.38]:50000' (ECDSA) to the list of known hosts.\n",
      "cntk-ps-as-job-3\n",
      "Connection to 52.186.123.38 closed.\n",
      "2017-06-21 15:30:32,604 INFO - Terminating task: auto-model-selection\n",
      "2017-06-21 15:30:33,063 DEBUG - waiting for task 0 in job cntk-ps-as-job to terminate\n",
      "2017-06-21 15:30:33,251 DEBUG - waiting for task 1 in job cntk-ps-as-job to terminate\n",
      "2017-06-21 15:30:33,434 DEBUG - waiting for task 2 in job cntk-ps-as-job to terminate\n",
      "2017-06-21 15:30:33,598 DEBUG - waiting for task 3 in job cntk-ps-as-job to terminate\n",
      "2017-06-21 15:30:33,805 DEBUG - waiting for task auto-model-selection in job cntk-ps-as-job to terminate\n",
      "2017-06-21 15:30:34,176 DEBUG - waiting for job cntk-ps-as-job to delete\n",
      "2017-06-21 15:31:05,454 INFO - job cntk-ps-as-job does not exist\n"
     ]
    }
   ],
   "source": [
    "shipyard jobs del -y --termtasks --wait"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "You can proceed to the [Notebook: Clean Up](05_Clean_Up.ipynb) if you are done for now, or proceed to one of the following additional Notebooks:\n",
    "* [Notebook: Tensorboard Visualization](07_Advanced_Tensorboard.ipynb) - note this requires running this notebook on your own machine\n",
    "* [Notebook: Parallel and Distributed](08_Advanced_Parallel_and_Distributed.ipynb)\n",
    "* [Notebook: Keras with TensorFlow](09_Keras_Single_GPU_Training_With_Tensorflow.ipynb)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
