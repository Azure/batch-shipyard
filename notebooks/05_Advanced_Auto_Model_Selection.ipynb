{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V - Automatic Model Selection from Parametric Sweep using Task Dependencies\n",
    "In this notebook we will be taking the example from the [Parametric Sweep](04_Parameter_Sweep.ipynb) notebook and automating the entire chain using task dependencies in a single Azure Batch job.\n",
    "\n",
    "* [Setup](#section1)\n",
    "* [Configure job](#section2)\n",
    "* [Submit job](#section3)\n",
    "* [Download best model](#section4)\n",
    "* [Delete job](#section5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a simple alias for Batch Shipyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%alias shipyard SHIPYARD_CONFIGDIR=config python $HOME/batch-shipyard/shipyard.py %l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that everything is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: shipyard.py [OPTIONS] COMMAND [ARGS]...\r\n",
      "\r\n",
      "  Batch Shipyard: Provision and Execute Docker Workloads on Azure Batch\r\n",
      "\r\n",
      "Options:\r\n",
      "  --version   Show the version and exit.\r\n",
      "  -h, --help  Show this message and exit.\r\n",
      "\r\n",
      "Commands:\r\n",
      "  cert      Certificate actions\r\n",
      "  data      Data actions\r\n",
      "  fs        Filesystem in Azure actions\r\n",
      "  jobs      Jobs actions\r\n",
      "  keyvault  KeyVault actions\r\n",
      "  misc      Miscellaneous actions\r\n",
      "  pool      Pool actions\r\n",
      "  storage   Storage actions\r\n"
     ]
    }
   ],
   "source": [
    "shipyard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the account information we saved earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_json(filename):\n",
    "    with open(filename, 'r') as infile:\n",
    "        return json.load(infile)\n",
    "    \n",
    "account_info = read_json('account_information.json')\n",
    "\n",
    "storage_account_key = account_info['storage_account_key']\n",
    "storage_account_name = account_info['storage_account_name']\n",
    "IMAGE_NAME = account_info['IMAGE_NAME']\n",
    "STORAGE_ALIAS = account_info['STORAGE_ALIAS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous job we ran on a single node we will be running the job on GPU enabled nodes. The difference here is that depending on the number of combinations we will be creating the same number of tasks. Each task will have a different set of parmeters that we will be passing to our model training script. This parameters effect the training of the model and in the end the performance of the model. The model and results of its evaluation are recorded and stored on the node. At the end of the task the results are pulled into the specified storage container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nbuser/anaconda2_410/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/nbuser/anaconda2_410/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from copy import copy\n",
    "import os\n",
    "import random\n",
    "from sklearn.grid_search import ParameterGrid\n",
    "from toolz import curry, pipe\n",
    "\n",
    "def write_json_to_file(json_dict, filename):\n",
    "    \"\"\" Simple function to write JSON dictionaries to files\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as outfile:\n",
    "        json.dump(json_dict, outfile)\n",
    "\n",
    "def compose_command(num_convolution_layers, minibatch_size, max_epochs=30):\n",
    "    cmd_str = ' '.join((\"source /cntk/activate-cntk;\",\n",
    "                        \"python -u ConvNet_CIFAR10.py\",\n",
    "                        \"--datadir $AZ_BATCH_NODE_SHARED_DIR/data\",\n",
    "                        \"--num_convolution_layers {num_convolution_layers}\",\n",
    "                        \"--minibatch_size {minibatch_size}\",\n",
    "                        \"--max_epochs {max_epochs}\")).format(num_convolution_layers=num_convolution_layers,\n",
    "                                                             minibatch_size=minibatch_size,\n",
    "                                                             max_epochs=max_epochs)\n",
    "    return 'bash -c \"{}\"'.format(cmd_str)\n",
    "\n",
    "@curry\n",
    "def append_parameter(param_name, param_value, data_dict):\n",
    "    data_dict[param_name]=param_value\n",
    "    return data_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the task template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_TASK_TEMPLATE = {\n",
    "    \"image\": IMAGE_NAME,\n",
    "    \"remove_container_after_exit\": True,\n",
    "    \"gpu\": True,\n",
    "    \"resource_files\": [\n",
    "            {\n",
    "                \"file_path\": \"ConvNet_CIFAR10.py\",\n",
    "                \"blob_source\": \"https://batchshipyardexamples.blob.core.windows.net/code/ConvNet_CIFAR10.py\",\n",
    "                \"file_mode\":'0777'\n",
    "            }\n",
    "    ],\n",
    "    \"output_data\": {\n",
    "        \"azure_storage\": [\n",
    "            {\n",
    "                \"storage_account_settings\": STORAGE_ALIAS,\n",
    "                \"container\": \"output\",\n",
    "                \"source\": \"$AZ_BATCH_TASK_DIR/wd/Models\"\n",
    "            },\n",
    "        ]\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we modify the `task_generator` to append an additional parameter to each task definition with a specific `id` numbered from `0`. This is done so we can reference each dependent task in the selection task that must run after each training task has completed successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def task_generator(parameters):\n",
    "    id = 0\n",
    "    for params in ParameterGrid(parameters):\n",
    "        yield pipe(copy(_TASK_TEMPLATE),\n",
    "                   append_parameter('command', compose_command(**params)),\n",
    "                   append_parameter('id', str(id)))\n",
    "        id += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the `jobs.json` configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"num_convolution_layers\": [2, 3],\n",
    "    \"minibatch_size\": [32, 64]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tasks for parametric sweep cntk-ps-as-job: 4\n"
     ]
    }
   ],
   "source": [
    "JOB_ID = 'cntk-ps-as-job'\n",
    "\n",
    "jobs = {\n",
    "    \"job_specifications\": [\n",
    "        {\n",
    "            \"id\": JOB_ID,\n",
    "            \"tasks\": list(task_generator(parameters))    \n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "num_parameter_sweep_tasks = len(jobs['job_specifications'][0]['tasks'])\n",
    "print('number of tasks for parametric sweep {}: {}'.format(JOB_ID, num_parameter_sweep_tasks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create the Python program to run that performs the best model selection. Note that this code is nearly similar to the code for selecting the best model locally in the [Parameter sweep notebook](04_Parameter_Sweep.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing autoselect.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile autoselect.py\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def read_json(filename):\n",
    "    with open(filename, 'r') as infile:\n",
    "        return json.load(infile)\n",
    "\n",
    "def scandir(basedir):\n",
    "    for root, dirs, files in os.walk(basedir):\n",
    "        for f in files:\n",
    "            yield os.path.join(root, f) \n",
    "\n",
    "MODELS_DIR = os.path.join('wd', 'Models')\n",
    "            \n",
    "results_dict = {}\n",
    "for model in scandir(MODELS_DIR):\n",
    "    if not model.endswith('.json'):\n",
    "        continue\n",
    "    key = model.split(os.sep)[2]  # due to MODELS_DIR path change\n",
    "    results_dict[key] = read_json(model)\n",
    "\n",
    "# use items() instead of iteritems() as this will be run in python3\n",
    "tuple_min_error = min(results_dict.items(), key=lambda x: x[1]['test_metric'])\n",
    "configuration_with_min_error = tuple_min_error[0]\n",
    "print('task with smallest error: {} ({})'.format(configuration_with_min_error, tuple_min_error[1]['test_metric']))\n",
    "\n",
    "# copy best model to wd\n",
    "MODEL_NAME = 'ConvNet_CIFAR10_model.dnn'\n",
    "shutil.copy(os.path.join(MODELS_DIR, configuration_with_min_error, MODEL_NAME), '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to prepare the file to be uploaded to the Azure Storage account to be referenced in the task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 12\r\n",
      "drwxr-xr-x  2 nbuser nbuser 4096 Jul 12 14:20 ./\r\n",
      "drwx------ 16 nbuser nbuser 4096 Jul 12 14:20 ../\r\n",
      "-rw-r--r--  1 nbuser nbuser 1001 Jul 12 14:20 autoselect.py\r\n"
     ]
    }
   ],
   "source": [
    "INPUT_CONTAINER = 'input-autoselect'\n",
    "OUTPUT_CONTAINER = 'output-autoselect'\n",
    "UPLOAD_DIR = 'autoselect_upload'\n",
    "\n",
    "!rm -rf $UPLOAD_DIR\n",
    "!mkdir -p $UPLOAD_DIR\n",
    "!mv autoselect.py $UPLOAD_DIR\n",
    "!ls -alF $UPLOAD_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alias `blobxfer` and upload it to `INPUT_CONTAINER`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%alias blobxfer python -m blobxfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      " azure blobxfer parameters [v0.12.1]\n",
      "=====================================\n",
      "             platform: Linux-4.4.0-81-generic-x86_64-with-debian-stretch-sid\n",
      "   python interpreter: CPython 2.7.11\n",
      "     package versions: az.common=1.1.5 az.sml=0.20.5 az.stor=0.34.2 crypt=1.9 req=2.18.1\n",
      "      subscription id: None\n",
      "      management cert: None\n",
      "   transfer direction: local->Azure\n",
      "       local resource: autoselect_upload\n",
      "      include pattern: None\n",
      "      remote resource: None\n",
      "   max num of workers: 24\n",
      "              timeout: None\n",
      "      storage account: batch0e43a94est\n",
      "              use SAS: False\n",
      "  upload as page blob: False\n",
      "  auto vhd->page blob: False\n",
      " upload to file share: False\n",
      " container/share name: input-autoselect\n",
      "  container/share URI: https://batch0e43a94est.blob.core.windows.net/input-autoselect\n",
      "    compute block MD5: False\n",
      "     compute file MD5: True\n",
      "    skip on MD5 match: True\n",
      "   chunk size (bytes): 4194304\n",
      "     create container: True\n",
      "  keep mismatched MD5: False\n",
      "     recursive if dir: True\n",
      "component strip on up: 1\n",
      "        remote delete: False\n",
      "           collate to: disabled\n",
      "      local overwrite: True\n",
      "      encryption mode: disabled\n",
      "         RSA key file: disabled\n",
      "         RSA key type: disabled\n",
      "=======================================\n",
      "\n",
      "script start time: 2017-07-12 14:21:02\n",
      "computing file md5 on: autoselect_upload/autoselect.py\n",
      "  >> md5: A0V93+uyerutTDe7jckLLA==\n",
      "creating container, if needed: input-autoselect\n",
      "detected 0 empty files to upload\n",
      "performing 1 put blocks/blobs and 1 put block lists\n",
      "spawning 1 worker threads\n",
      "xfer progress: [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100.00%     574.91 blocks/min    \n",
      "\n",
      "0.000954627990723 MiB transfered, elapsed 0.104362010956 sec. Throughput = 0.0731781982336 Mbit/sec\n",
      "\n",
      "\n",
      "script elapsed time: 0.343563079834 sec\n",
      "script end time: 2017-07-12 14:21:02\n"
     ]
    }
   ],
   "source": [
    "blobxfer $storage_account_name $INPUT_CONTAINER $UPLOAD_DIR --upload --storageaccountkey $storage_account_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll append the `auto-model-selection` task which depends on the prior training tasks. The important properties here are `depends_on_range` which specifies a range of task ids the `auto-model-selection` task depends on. Additionally, this task requires data from the prior run task which is specified in `input_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_input_data_spec(job_id, task_id):\n",
    "    return {\n",
    "        \"job_id\": job_id,\n",
    "        \"task_id\": task_id,\n",
    "        \"include\": [\"wd/Models/*_{}_{}/*\".format(task_id, job_id)]\n",
    "    }\n",
    "\n",
    "input_data = []\n",
    "for x in range(0, num_parameter_sweep_tasks):\n",
    "    input_data.append(generate_input_data_spec(JOB_ID, str(x)))\n",
    "\n",
    "model_selection_task = {\n",
    "    \"id\": \"auto-model-selection\",\n",
    "    \"command\": 'bash -c \"source /cntk/activate-cntk; python -u autoselect.py\"',\n",
    "    \"depends_on_range\": [0, num_parameter_sweep_tasks - 1],\n",
    "    \"image\": IMAGE_NAME,\n",
    "    \"remove_container_after_exit\": True,\n",
    "    \"input_data\": {\n",
    "        \"azure_batch\": input_data,\n",
    "        \"azure_storage\": [\n",
    "            {\n",
    "                \"storage_account_settings\": STORAGE_ALIAS,\n",
    "                \"container\": INPUT_CONTAINER\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"output_data\": {\n",
    "        \"azure_storage\": [\n",
    "            {\n",
    "                \"storage_account_settings\": STORAGE_ALIAS,\n",
    "                \"container\": OUTPUT_CONTAINER,\n",
    "                \"include\": [\"*wd/ConvNet_CIFAR10_model.dnn\"],\n",
    "                \"blobxfer_extra_options\": \"--delete --strip-components 2\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# append auto-model-selection task to jobs\n",
    "jobs['job_specifications'][0]['tasks'].append(model_selection_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"job_specifications\": [\n",
      "        {\n",
      "            \"id\": \"cntk-ps-as-job\", \n",
      "            \"tasks\": [\n",
      "                {\n",
      "                    \"command\": \"bash -c \\\"source /cntk/activate-cntk; python -u ConvNet_CIFAR10.py --datadir $AZ_BATCH_NODE_SHARED_DIR/data --num_convolution_layers 2 --minibatch_size 32 --max_epochs 30\\\"\", \n",
      "                    \"gpu\": true, \n",
      "                    \"id\": \"0\", \n",
      "                    \"image\": \"microsoft/cntk:2.0-gpu-python3.5-cuda8.0-cudnn5.1\", \n",
      "                    \"output_data\": {\n",
      "                        \"azure_storage\": [\n",
      "                            {\n",
      "                                \"container\": \"output\", \n",
      "                                \"source\": \"$AZ_BATCH_TASK_DIR/wd/Models\", \n",
      "                                \"storage_account_settings\": \"mystorageaccount\"\n",
      "                            }\n",
      "                        ]\n",
      "                    }, \n",
      "                    \"remove_container_after_exit\": true, \n",
      "                    \"resource_files\": [\n",
      "                        {\n",
      "                            \"blob_source\": \"https://batchshipyardexamples.blob.core.windows.net/code/ConvNet_CIFAR10.py\", \n",
      "                            \"file_mode\": \"0777\", \n",
      "                            \"file_path\": \"ConvNet_CIFAR10.py\"\n",
      "                        }\n",
      "                    ]\n",
      "                }, \n",
      "                {\n",
      "                    \"command\": \"bash -c \\\"source /cntk/activate-cntk; python -u ConvNet_CIFAR10.py --datadir $AZ_BATCH_NODE_SHARED_DIR/data --num_convolution_layers 3 --minibatch_size 32 --max_epochs 30\\\"\", \n",
      "                    \"gpu\": true, \n",
      "                    \"id\": \"1\", \n",
      "                    \"image\": \"microsoft/cntk:2.0-gpu-python3.5-cuda8.0-cudnn5.1\", \n",
      "                    \"output_data\": {\n",
      "                        \"azure_storage\": [\n",
      "                            {\n",
      "                                \"container\": \"output\", \n",
      "                                \"source\": \"$AZ_BATCH_TASK_DIR/wd/Models\", \n",
      "                                \"storage_account_settings\": \"mystorageaccount\"\n",
      "                            }\n",
      "                        ]\n",
      "                    }, \n",
      "                    \"remove_container_after_exit\": true, \n",
      "                    \"resource_files\": [\n",
      "                        {\n",
      "                            \"blob_source\": \"https://batchshipyardexamples.blob.core.windows.net/code/ConvNet_CIFAR10.py\", \n",
      "                            \"file_mode\": \"0777\", \n",
      "                            \"file_path\": \"ConvNet_CIFAR10.py\"\n",
      "                        }\n",
      "                    ]\n",
      "                }, \n",
      "                {\n",
      "                    \"command\": \"bash -c \\\"source /cntk/activate-cntk; python -u ConvNet_CIFAR10.py --datadir $AZ_BATCH_NODE_SHARED_DIR/data --num_convolution_layers 2 --minibatch_size 64 --max_epochs 30\\\"\", \n",
      "                    \"gpu\": true, \n",
      "                    \"id\": \"2\", \n",
      "                    \"image\": \"microsoft/cntk:2.0-gpu-python3.5-cuda8.0-cudnn5.1\", \n",
      "                    \"output_data\": {\n",
      "                        \"azure_storage\": [\n",
      "                            {\n",
      "                                \"container\": \"output\", \n",
      "                                \"source\": \"$AZ_BATCH_TASK_DIR/wd/Models\", \n",
      "                                \"storage_account_settings\": \"mystorageaccount\"\n",
      "                            }\n",
      "                        ]\n",
      "                    }, \n",
      "                    \"remove_container_after_exit\": true, \n",
      "                    \"resource_files\": [\n",
      "                        {\n",
      "                            \"blob_source\": \"https://batchshipyardexamples.blob.core.windows.net/code/ConvNet_CIFAR10.py\", \n",
      "                            \"file_mode\": \"0777\", \n",
      "                            \"file_path\": \"ConvNet_CIFAR10.py\"\n",
      "                        }\n",
      "                    ]\n",
      "                }, \n",
      "                {\n",
      "                    \"command\": \"bash -c \\\"source /cntk/activate-cntk; python -u ConvNet_CIFAR10.py --datadir $AZ_BATCH_NODE_SHARED_DIR/data --num_convolution_layers 3 --minibatch_size 64 --max_epochs 30\\\"\", \n",
      "                    \"gpu\": true, \n",
      "                    \"id\": \"3\", \n",
      "                    \"image\": \"microsoft/cntk:2.0-gpu-python3.5-cuda8.0-cudnn5.1\", \n",
      "                    \"output_data\": {\n",
      "                        \"azure_storage\": [\n",
      "                            {\n",
      "                                \"container\": \"output\", \n",
      "                                \"source\": \"$AZ_BATCH_TASK_DIR/wd/Models\", \n",
      "                                \"storage_account_settings\": \"mystorageaccount\"\n",
      "                            }\n",
      "                        ]\n",
      "                    }, \n",
      "                    \"remove_container_after_exit\": true, \n",
      "                    \"resource_files\": [\n",
      "                        {\n",
      "                            \"blob_source\": \"https://batchshipyardexamples.blob.core.windows.net/code/ConvNet_CIFAR10.py\", \n",
      "                            \"file_mode\": \"0777\", \n",
      "                            \"file_path\": \"ConvNet_CIFAR10.py\"\n",
      "                        }\n",
      "                    ]\n",
      "                }, \n",
      "                {\n",
      "                    \"command\": \"bash -c \\\"source /cntk/activate-cntk; python -u autoselect.py\\\"\", \n",
      "                    \"depends_on_range\": [\n",
      "                        0, \n",
      "                        3\n",
      "                    ], \n",
      "                    \"id\": \"auto-model-selection\", \n",
      "                    \"image\": \"microsoft/cntk:2.0-gpu-python3.5-cuda8.0-cudnn5.1\", \n",
      "                    \"input_data\": {\n",
      "                        \"azure_batch\": [\n",
      "                            {\n",
      "                                \"include\": [\n",
      "                                    \"wd/Models/*_0_cntk-ps-as-job/*\"\n",
      "                                ], \n",
      "                                \"job_id\": \"cntk-ps-as-job\", \n",
      "                                \"task_id\": \"0\"\n",
      "                            }, \n",
      "                            {\n",
      "                                \"include\": [\n",
      "                                    \"wd/Models/*_1_cntk-ps-as-job/*\"\n",
      "                                ], \n",
      "                                \"job_id\": \"cntk-ps-as-job\", \n",
      "                                \"task_id\": \"1\"\n",
      "                            }, \n",
      "                            {\n",
      "                                \"include\": [\n",
      "                                    \"wd/Models/*_2_cntk-ps-as-job/*\"\n",
      "                                ], \n",
      "                                \"job_id\": \"cntk-ps-as-job\", \n",
      "                                \"task_id\": \"2\"\n",
      "                            }, \n",
      "                            {\n",
      "                                \"include\": [\n",
      "                                    \"wd/Models/*_3_cntk-ps-as-job/*\"\n",
      "                                ], \n",
      "                                \"job_id\": \"cntk-ps-as-job\", \n",
      "                                \"task_id\": \"3\"\n",
      "                            }\n",
      "                        ], \n",
      "                        \"azure_storage\": [\n",
      "                            {\n",
      "                                \"container\": \"input-autoselect\", \n",
      "                                \"storage_account_settings\": \"mystorageaccount\"\n",
      "                            }\n",
      "                        ]\n",
      "                    }, \n",
      "                    \"output_data\": {\n",
      "                        \"azure_storage\": [\n",
      "                            {\n",
      "                                \"blobxfer_extra_options\": \"--delete --strip-components 2\", \n",
      "                                \"container\": \"output-autoselect\", \n",
      "                                \"include\": [\n",
      "                                    \"*wd/ConvNet_CIFAR10_model.dnn\"\n",
      "                                ], \n",
      "                                \"storage_account_settings\": \"mystorageaccount\"\n",
      "                            }\n",
      "                        ]\n",
      "                    }, \n",
      "                    \"remove_container_after_exit\": true\n",
      "                }\n",
      "            ]\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "write_json_to_file(jobs, os.path.join('config', 'jobs.json'))\n",
    "print(json.dumps(jobs, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit job\n",
    "Check that everything is ok with our pool before we submit our jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-07-12 14:21:08,577 INFO - pool_id=gpupool [state=PoolState.active allocation_state=AllocationState.steady vm_size=standard_nc6 node_agent=batch.node.ubuntu 16.04 vm_dedicated_count=3 target_vm_dedicated_count=3 vm_low_priority_count=0 target_vm_low_priority_count=0]\r\n"
     ]
    }
   ],
   "source": [
    "shipyard pool list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have confirmed everything is working we can execute our job using the command below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-07-12 14:21:10,362 INFO - Adding job cntk-ps-as-job to pool gpupool\n",
      "2017-07-12 14:21:10,846 INFO - uploading file /tmp/tmp8KpIIY as u'shipyardtaskrf-cntk-ps-as-job/0.shipyard.envlist'\n",
      "2017-07-12 14:21:11,120 INFO - uploading file /tmp/tmpBqjmZv as u'shipyardtaskrf-cntk-ps-as-job/1.shipyard.envlist'\n",
      "2017-07-12 14:21:11,472 INFO - uploading file /tmp/tmpjDD8L3 as u'shipyardtaskrf-cntk-ps-as-job/2.shipyard.envlist'\n",
      "2017-07-12 14:21:11,759 INFO - uploading file /tmp/tmpddfhZs as u'shipyardtaskrf-cntk-ps-as-job/3.shipyard.envlist'\n",
      "2017-07-12 14:21:12,181 DEBUG - submitting 5 tasks (0 -> 4) to job cntk-ps-as-job\n",
      "2017-07-12 14:21:12,523 INFO - submitted all 5 tasks to job cntk-ps-as-job\n"
     ]
    }
   ],
   "source": [
    "shipyard jobs add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the command below we can check the status of our jobs. Once all jobs have an exit code we can continue. You can also view the **heatmap** of this pool on [Azure Portal](https://portal.azure.com) to monitor the progress of this job on the compute nodes under your Batch account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-07-12 14:41:12,347 INFO - job_id=cntk-ps-as-job task_id=0 [state=TaskState.completed max_retries=0 retention_time=10675199 days, 2:48:05.477581 pool_id=gpupool node_id=tvm-1392786932_3-20170712t132611z start_time=2017-07-12 14:21:13.202438+00:00 end_time=2017-07-12 14:31:34.253864+00:00 duration=0:10:21.051426 exit_code=0]\r\n",
      "2017-07-12 14:41:12,347 INFO - job_id=cntk-ps-as-job task_id=1 [state=TaskState.completed max_retries=0 retention_time=10675199 days, 2:48:05.477581 pool_id=gpupool node_id=tvm-1392786932_1-20170712t132611z start_time=2017-07-12 14:21:13.471622+00:00 end_time=2017-07-12 14:32:40.606897+00:00 duration=0:11:27.135275 exit_code=0]\r\n",
      "2017-07-12 14:41:12,348 INFO - job_id=cntk-ps-as-job task_id=2 [state=TaskState.completed max_retries=0 retention_time=10675199 days, 2:48:05.477581 pool_id=gpupool node_id=tvm-1392786932_2-20170712t132611z start_time=2017-07-12 14:30:19.609022+00:00 end_time=2017-07-12 14:38:19.982432+00:00 duration=0:08:00.373410 exit_code=0]\r\n",
      "2017-07-12 14:41:12,348 INFO - job_id=cntk-ps-as-job task_id=3 [state=TaskState.completed max_retries=0 retention_time=10675199 days, 2:48:05.477581 pool_id=gpupool node_id=tvm-1392786932_2-20170712t132611z start_time=2017-07-12 14:21:13.376340+00:00 end_time=2017-07-12 14:30:18.085398+00:00 duration=0:09:04.709058 exit_code=0]\r\n",
      "2017-07-12 14:41:12,348 INFO - job_id=cntk-ps-as-job task_id=auto-model-selection [state=TaskState.completed max_retries=0 retention_time=10675199 days, 2:48:05.477581 pool_id=gpupool node_id=tvm-1392786932_3-20170712t132611z start_time=2017-07-12 14:38:21.712752+00:00 end_time=2017-07-12 14:38:31.078846+00:00 duration=0:00:09.366094 exit_code=0]\r\n"
     ]
    }
   ],
   "source": [
    "shipyard jobs listtasks --jobid $JOB_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download best model\n",
    "The best performing model from the parametric sweep job should now be saved to our `OUTPUT_CONTAINER` container by the `auto-model-selection` task. Let's save this model in `MODELS_DIR`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MODELS_DIR = 'auto-selected-model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the best performing model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      " azure blobxfer parameters [v0.12.1]\n",
      "=====================================\n",
      "             platform: Linux-4.4.0-81-generic-x86_64-with-debian-stretch-sid\n",
      "   python interpreter: CPython 2.7.11\n",
      "     package versions: az.common=1.1.5 az.sml=0.20.5 az.stor=0.34.2 crypt=1.9 req=2.18.1\n",
      "      subscription id: None\n",
      "      management cert: None\n",
      "   transfer direction: Azure->local\n",
      "       local resource: auto-selected-model\n",
      "      include pattern: None\n",
      "      remote resource: .\n",
      "   max num of workers: 24\n",
      "              timeout: None\n",
      "      storage account: batch0e43a94est\n",
      "              use SAS: False\n",
      "  upload as page blob: False\n",
      "  auto vhd->page blob: False\n",
      " upload to file share: False\n",
      " container/share name: output-autoselect\n",
      "  container/share URI: https://batch0e43a94est.blob.core.windows.net/output-autoselect\n",
      "    compute block MD5: False\n",
      "     compute file MD5: True\n",
      "    skip on MD5 match: True\n",
      "   chunk size (bytes): 4194304\n",
      "     create container: True\n",
      "  keep mismatched MD5: False\n",
      "     recursive if dir: True\n",
      "component strip on up: 1\n",
      "        remote delete: False\n",
      "           collate to: disabled\n",
      "      local overwrite: True\n",
      "      encryption mode: disabled\n",
      "         RSA key file: disabled\n",
      "         RSA key type: disabled\n",
      "=======================================\n",
      "\n",
      "script start time: 2017-07-12 14:41:22\n",
      "attempting to copy entire container output-autoselect to auto-selected-model\n",
      "generating local directory structure and pre-allocating space\n",
      "created local directory: auto-selected-model\n",
      "remote blob: ConvNet_CIFAR10_model.dnn length: 1957354 bytes, md5: 2bjofmNlQvXgP6Hpp4vUwQ==\n",
      "performing 1 range-gets\n",
      "spawning 1 worker threads\n",
      "xfer progress: [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100.00%     125.38 range-gets/min    \n",
      "\n",
      "1.86667823792 MiB transfered, elapsed 0.478538990021 sec. Throughput = 31.2062887554 Mbit/sec\n",
      "\n",
      "performing finalization (if applicable): HMAC-SHA256: False, MD5: True\n",
      "[MD5: OK, auto-selected-model/ConvNet_CIFAR10_model.dnn] 2bjofmNlQvXgP6Hpp4vUwQ== <L..R> 2bjofmNlQvXgP6Hpp4vUwQ==\n",
      "finalization complete.\n",
      "\n",
      "script elapsed time: 0.728971004486 sec\n",
      "script end time: 2017-07-12 14:41:22\n"
     ]
    }
   ],
   "source": [
    "blobxfer $storage_account_name $OUTPUT_CONTAINER $MODELS_DIR --remoteresource . --download --storageaccountkey $storage_account_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model file (`ConvNet_CIFAR10_model.dnn`) is now ready for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1920\r\n",
      "drwxr-xr-x  2 nbuser nbuser    4096 Jul 12 14:41 ./\r\n",
      "drwx------ 17 nbuser nbuser    4096 Jul 12 14:41 ../\r\n",
      "-rw-r--r--  1 nbuser nbuser 1957354 Jul 12 14:41 ConvNet_CIFAR10_model.dnn\r\n"
     ]
    }
   ],
   "source": [
    "!ls -alF $MODELS_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To delete the job use the command below. Just be aware that this will get rid of all the files created by the job and tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-07-12 14:42:46,163 INFO - Deleting job: cntk-ps-as-job\n",
      "2017-07-12 14:42:46,163 DEBUG - disabling job cntk-ps-as-job first due to task termination\n",
      "2017-07-12 14:42:47,330 DEBUG - Skipping termination of completed task 0 on job cntk-ps-as-job\n",
      "2017-07-12 14:42:47,540 DEBUG - Skipping termination of completed task 1 on job cntk-ps-as-job\n",
      "2017-07-12 14:42:47,736 DEBUG - Skipping termination of completed task 2 on job cntk-ps-as-job\n",
      "2017-07-12 14:42:47,944 DEBUG - Skipping termination of completed task 3 on job cntk-ps-as-job\n",
      "2017-07-12 14:42:48,142 DEBUG - Skipping termination of completed task auto-model-selection on job cntk-ps-as-job\n",
      "2017-07-12 14:42:48,616 DEBUG - waiting for job cntk-ps-as-job to delete\n",
      "2017-07-12 14:43:56,158 INFO - job cntk-ps-as-job does not exist\n"
     ]
    }
   ],
   "source": [
    "shipyard jobs del -y --termtasks --wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
