{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II - Single GPU Training\n",
    "In the previous notebok we set up our pool of GPU nodes. In this notebook we are going to get one of the nodes in the pool to train a deep learning model for small number of epochs. The model and results of the training will be then loaded into blob storage for later retrieval.\n",
    "\n",
    "* [Setup](#section1)\n",
    "* [Configure job](#section2)\n",
    "* [Submit job](#section3)\n",
    "* [Delete job](#section4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a simple alias for Batch Shipyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%alias shipyard SHIPYARD_CONFIGDIR=config python $HOME/batch-shipyard/shipyard.py %l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that everything is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: shipyard.py [OPTIONS] COMMAND [ARGS]...\r\n",
      "\r\n",
      "  Batch Shipyard: Provision and Execute Docker Workloads on Azure Batch\r\n",
      "\r\n",
      "Options:\r\n",
      "  --version   Show the version and exit.\r\n",
      "  -h, --help  Show this message and exit.\r\n",
      "\r\n",
      "Commands:\r\n",
      "  cert      Certificate actions\r\n",
      "  data      Data actions\r\n",
      "  fs        Filesystem in Azure actions\r\n",
      "  jobs      Jobs actions\r\n",
      "  keyvault  KeyVault actions\r\n",
      "  misc      Miscellaneous actions\r\n",
      "  pool      Pool actions\r\n",
      "  storage   Storage actions\r\n"
     ]
    }
   ],
   "source": [
    "shipyard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get some variables stored in the Setup notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_json(filename):\n",
    "    with open(filename, 'r') as infile:\n",
    "        return json.load(infile)\n",
    "    \n",
    "account_info = read_json('account_information.json')\n",
    "\n",
    "IMAGE_NAME = account_info['IMAGE_NAME']\n",
    "STORAGE_ALIAS = account_info['STORAGE_ALIAS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "## Configure job\n",
    "In the dictonary below we define the properties of the job we wish to execute. You can see that we have specified that the image to use is the one we defined at the beginning of this notebook. Another interesting note is that we specify the gpu switch to true since we want the job to use the GPU. Finally the command is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "source /cntk/activate-cntk\n",
    "python ConvNet_CIFAR10.py\n",
    "```\n",
    "\n",
    "Which in essence activates the CNTK Anaconda environment then runs the **ConvNet_CIFAR10.py** script which will train and evaluate the model.\n",
    "\n",
    "In the jobs json below, `resource_files` contains the script to train our CNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TASK_ID = 'run_cifar10' # This should be changed per task\n",
    "\n",
    "JOB_ID = 'cntk-training-job'\n",
    "\n",
    "COMMAND = 'bash -c \"source /cntk/activate-cntk; python -u ConvNet_CIFAR10.py --datadir $AZ_BATCH_NODE_SHARED_DIR/data\"'\n",
    "\n",
    "jobs = {\n",
    "    \"job_specifications\": [\n",
    "        {\n",
    "            \"id\": JOB_ID,\n",
    "            \"tasks\": [\n",
    "                {\n",
    "                    \"id\": TASK_ID,\n",
    "                    \"image\": IMAGE_NAME,\n",
    "                    \"remove_container_after_exit\": True,\n",
    "                    \"command\": COMMAND,\n",
    "                    \"gpu\": True,\n",
    "                    \"resource_files\": [\n",
    "                        {\n",
    "                            \"file_path\": \"ConvNet_CIFAR10.py\",\n",
    "                            \"blob_source\": \"https://batchshipyardexamples.blob.core.windows.net/code/ConvNet_CIFAR10.py\",\n",
    "                            \"file_mode\":'0777'\n",
    "                        }\n",
    "                    ],\n",
    "                    \"output_data\": {\n",
    "                        \"azure_storage\": [\n",
    "                            {\n",
    "                                \"storage_account_settings\": STORAGE_ALIAS,\n",
    "                                \"container\": \"output\",\n",
    "                                \"source\": \"$AZ_BATCH_TASK_WORKING_DIR/Models\"\n",
    "                            },\n",
    "                        ]\n",
    "                    },\n",
    "                }\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Write the jobs configuration to the `jobs.json` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"job_specifications\": [\n",
      "        {\n",
      "            \"id\": \"cntk-training-job\", \n",
      "            \"tasks\": [\n",
      "                {\n",
      "                    \"command\": \"bash -c \\\"source /cntk/activate-cntk; python -u ConvNet_CIFAR10.py --datadir $AZ_BATCH_NODE_SHARED_DIR/data\\\"\", \n",
      "                    \"gpu\": true, \n",
      "                    \"id\": \"run_cifar10\", \n",
      "                    \"image\": \"microsoft/cntk:2.0-gpu-python3.5-cuda8.0-cudnn5.1\", \n",
      "                    \"output_data\": {\n",
      "                        \"azure_storage\": [\n",
      "                            {\n",
      "                                \"container\": \"output\", \n",
      "                                \"source\": \"$AZ_BATCH_TASK_WORKING_DIR/Models\", \n",
      "                                \"storage_account_settings\": \"mystorageaccount\"\n",
      "                            }\n",
      "                        ]\n",
      "                    }, \n",
      "                    \"remove_container_after_exit\": true, \n",
      "                    \"resource_files\": [\n",
      "                        {\n",
      "                            \"blob_source\": \"https://batchshipyardexamples.blob.core.windows.net/code/ConvNet_CIFAR10.py\", \n",
      "                            \"file_mode\": \"0777\", \n",
      "                            \"file_path\": \"ConvNet_CIFAR10.py\"\n",
      "                        }\n",
      "                    ]\n",
      "                }\n",
      "            ]\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def write_json_to_file(json_dict, filename):\n",
    "    \"\"\" Simple function to write JSON dictionaries to files\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as outfile:\n",
    "        json.dump(json_dict, outfile)\n",
    "\n",
    "write_json_to_file(jobs, os.path.join('config', 'jobs.json'))\n",
    "print(json.dumps(jobs, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit job\n",
    "Check that everything is ok with our pool before we submit our jobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-21 14:51:25,822 DEBUG - listing nodes for pool gpupool\n",
      "2017-06-21 14:51:26,064 INFO - node_id=tvm-4283973576_1-20170621t143056z [state=ComputeNodeState.idle start_task_exit_code=0 scheduling_state=SchedulingState.enabled ip_address=10.0.0.4 vm_size=standard_nc6 dedicated=True total_tasks_run=0 running_tasks_count=0 total_tasks_succeeded=0]\n",
      "2017-06-21 14:51:26,064 INFO - node_id=tvm-4283973576_2-20170621t143056z [state=ComputeNodeState.waiting_for_start_task start_task_exit_code=None scheduling_state=SchedulingState.enabled ip_address=10.0.0.6 vm_size=standard_nc6 dedicated=True total_tasks_run=0 running_tasks_count=0 total_tasks_succeeded=0]\n",
      "2017-06-21 14:51:26,064 INFO - node_id=tvm-4283973576_3-20170621t143056z [state=ComputeNodeState.idle start_task_exit_code=0 scheduling_state=SchedulingState.enabled ip_address=10.0.0.5 vm_size=standard_nc6 dedicated=True total_tasks_run=0 running_tasks_count=0 total_tasks_succeeded=0]\n"
     ]
    }
   ],
   "source": [
    "shipyard pool listnodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have confirmed everything is working we can execute our job using the command below. The tail switch at the end will stream stdout from the node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-21 14:52:02,783 INFO - Adding job cntk-training-job to pool gpupool\n",
      "2017-06-21 14:52:03,263 INFO - uploading file /tmp/tmp7vnk_W as u'shipyardtaskrf-cntk-training-job/run_cifar10.shipyard.envlist'\n",
      "2017-06-21 14:52:03,616 DEBUG - submitting 1 tasks (0 -> 0) to job cntk-training-job\n",
      "2017-06-21 14:52:03,856 INFO - submitted all 1 tasks to job cntk-training-job\n",
      "2017-06-21 14:52:03,856 DEBUG - attempting to stream file stdout.txt from job=cntk-training-job task=run_cifar10\n",
      "\n",
      "************************************************************\n",
      "CNTK is activated.\n",
      "\n",
      "Please checkout tutorials and examples here:\n",
      "  /cntk/Tutorials\n",
      "  /cntk/Examples\n",
      "\n",
      "To deactivate the environment run\n",
      "\n",
      "  source /root/anaconda3/bin/deactivate\n",
      "\n",
      "************************************************************\n",
      "CCreating NN model\n",
      "Learning rate per sample: 0.0015625\n",
      "Momentum per sample: 0.0\n",
      "Finished Epoch[1 of 20]: [Training] loss = 2.130596 * 50048, metric = 80.71% * 50048 17.279s (2896.5 samples/s);\n",
      "Finished Epoch[2 of 20]: [Training] loss = 1.875394 * 49984, metric = 70.96% * 49984 15.732s (3177.2 samples/s);\n",
      "Finished Epoch[3 of 20]: [Training] loss = 1.737077 * 49984, metric = 64.24% * 49984 15.686s (3186.5 samples/s);\n",
      "Finished Epoch[4 of 20]: [Training] loss = 1.611311 * 49984, metric = 58.72% * 49984 15.777s (3168.2 samples/s);\n",
      "Finished Epoch[5 of 20]: [Training] loss = 1.487197 * 50048, metric = 53.91% * 50048 15.797s (3168.2 samples/s);\n",
      "Finished Epoch[6 of 20]: [Training] loss = 1.403237 * 49984, metric = 50.05% * 49984 15.732s (3177.2 samples/s);\n",
      "Finished Epoch[7 of 20]: [Training] loss = 1.307949 * 49984, metric = 46.33% * 49984 15.835s (3156.6 samples/s);\n",
      "Finished Epoch[8 of 20]: [Training] loss = 1.242379 * 49984, metric = 43.46% * 49984 15.750s (3173.6 samples/s);\n",
      "Finished Epoch[9 of 20]: [Training] loss = 1.184103 * 50048, metric = 41.09% * 50048 15.791s (3169.4 samples/s);\n",
      "Finished Epoch[10 of 20]: [Training] loss = 1.106218 * 49984, metric = 38.10% * 49984 15.702s (3183.3 samples/s);\n",
      "Learning rate per sample: 0.00046875\n",
      "Finished Epoch[11 of 20]: [Training] loss = 0.919987 * 49984, metric = 31.74% * 49984 15.774s (3168.8 samples/s);\n",
      "Finished Epoch[12 of 20]: [Training] loss = 0.891014 * 49984, metric = 30.37% * 49984 15.818s (3159.9 samples/s);\n",
      "Finished Epoch[13 of 20]: [Training] loss = 0.866919 * 50048, metric = 29.71% * 50048 15.850s (3157.6 samples/s);\n",
      "Finished Epoch[14 of 20]: [Training] loss = 0.850517 * 49984, metric = 28.93% * 49984 15.769s (3169.8 samples/s);\n",
      "Finished Epoch[15 of 20]: [Training] loss = 0.835393 * 49984, metric = 28.21% * 49984 15.759s (3171.8 samples/s);\n",
      "Finished Epoch[16 of 20]: [Training] loss = 0.827972 * 49984, metric = 28.09% * 49984 15.721s (3179.4 samples/s);\n",
      "Finished Epoch[17 of 20]: [Training] loss = 0.817293 * 50048, metric = 27.72% * 50048 15.830s (3161.6 samples/s);\n",
      "Finished Epoch[18 of 20]: [Training] loss = 0.801408 * 49984, metric = 27.28% * 49984 15.770s (3169.6 samples/s);\n",
      "Finished Epoch[19 of 20]: [Training] loss = 0.792335 * 49984, metric = 27.00% * 49984 15.757s (3172.2 samples/s);\n",
      "Finished Epoch[20 of 20]: [Training] loss = 0.778641 * 49984, metric = 26.04% * 49984 15.850s (3153.6 samples/s);\n",
      "Finished Evaluation [1]: Minibatch[1-625]: metric = 25.43% * 10000;\n",
      "=====================================\n",
      " azure blobxfer parameters [v0.12.1]\n",
      "=====================================\n",
      "             platform: Linux-4.4.0-81-generic-x86_64-with\n",
      "   python interpreter: CPython 3.5.2\n",
      "     package versions: az.common=1.1.4 az.sml=0.20.5 az.stor=0.33.0 crypt=1.6 req=2.12.3\n",
      "      subscription id: None\n",
      "      management cert: None\n",
      "   transfer direction: local->Azure\n",
      "       local resource: .\n",
      "      include pattern: None\n",
      "      remote resource: None\n",
      "   max num of workers: 18\n",
      "              timeout: None\n",
      "      storage account: batcha3dc41fdst\n",
      "              use SAS: True\n",
      "  upload as page blob: False\n",
      "  auto vhd->page blob: False\n",
      " upload to file share: False\n",
      " container/share name: output\n",
      "  container/share URI: https://batcha3dc41fdst.blob.core.windows.net/output\n",
      "    compute block MD5: False\n",
      "     compute file MD5: True\n",
      "    skip on MD5 match: True\n",
      "   chunk size (bytes): 4194304\n",
      "     create container: False\n",
      "  keep mismatched MD5: False\n",
      "     recursive if dir: True\n",
      "component strip on up: 1\n",
      "        remote delete: False\n",
      "           collate to: disabled\n",
      "      local overwrite: True\n",
      "      encryption mode: disabled\n",
      "         RSA key file: disabled\n",
      "         RSA key type: disabled\n",
      "=======================================\n",
      "\n",
      "script start time: 2017-06-21 14:57:27\n",
      "computing file md5 on: ./ConvNet_CIFAR10_model.dnn\n",
      "  >> md5: l6Jr8T1oVnRWVvCVaQYjjw==\n",
      "computing file md5 on: ./ConvNet_CIFAR10_model.dnn.ckp\n",
      "  >> md5: HNHPOg7Mw67IGe65IW8nQg==\n",
      "computing file md5 on: ./416c6fd4_run_cifar10_cntk-training-job/ConvNet_CIFAR10_model.dnn\n",
      "  >> md5: DJGAHm8yMoyz+hLNXS91IQ==\n",
      "computing file md5 on: ./416c6fd4_run_cifar10_cntk-training-job/model_results.json\n",
      "  >> md5: EDSAPyp+UV+gjDcP4yB1BA==\n",
      "detected 0 empty files to upload\n",
      "performing 7 put blocks/blobs and 4 put block lists\n",
      "spawning 7 worker threads\n",
      "\n",
      "\n",
      "13.727113723754883 MiB transfered, elapsed 0.4027082920074463 sec. Throughput = 272.6959239965401 Mbit/sec\n",
      "\n",
      "\n",
      "script elapsed time: 0.5573785305023193 sec\n",
      "script end time: 2017-06-21 14:57:27\n",
      "\n"
     ]
    }
   ],
   "source": [
    "shipyard jobs add --tail stdout.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also retrieve this `stdout.txt` data independently of `--tail` above by using the `data stream` command. Note that when you delete the job all this information is also deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-21 09:42:14,883 DEBUG - attempting to stream file stdout.txt from job=cntk-training-job task=run_cifar10\n",
      "\n",
      "************************************************************\n",
      "CNTK is activated.\n",
      "\n",
      "Please checkout tutorials and examples here:\n",
      "  /cntk/Tutorials\n",
      "  /cntk/Examples\n",
      "\n",
      "To deactivate the environment run\n",
      "\n",
      "  source /root/anaconda3/bin/deactivate\n",
      "\n",
      "************************************************************\n",
      "Creating NN model\n",
      "Learning rate per sample: 0.0015625\n",
      "Momentum per sample: 0.0\n",
      "Finished Epoch[1 of 20]: [Training] loss = 2.111319 * 50048, metric = 80.16% * 50048 17.290s (2894.6 samples/s);\n",
      "Finished Epoch[2 of 20]: [Training] loss = 1.851379 * 49984, metric = 69.47% * 49984 15.721s (3179.4 samples/s);\n",
      "Finished Epoch[3 of 20]: [Training] loss = 1.678905 * 49984, metric = 61.83% * 49984 15.727s (3178.2 samples/s);\n",
      "Finished Epoch[4 of 20]: [Training] loss = 1.542636 * 49984, metric = 56.11% * 49984 15.700s (3183.7 samples/s);\n",
      "Finished Epoch[5 of 20]: [Training] loss = 1.418185 * 50048, metric = 51.03% * 50048 15.786s (3170.4 samples/s);\n",
      "Finished Epoch[6 of 20]: [Training] loss = 1.322249 * 49984, metric = 46.94% * 49984 15.661s (3191.6 samples/s);\n",
      "Finished Epoch[7 of 20]: [Training] loss = 1.245542 * 49984, metric = 43.71% * 49984 15.830s (3157.5 samples/s);\n",
      "Finished Epoch[8 of 20]: [Training] loss = 1.169137 * 49984, metric = 40.56% * 49984 15.697s (3184.3 samples/s);\n",
      "Finished Epoch[9 of 20]: [Training] loss = 1.127274 * 50048, metric = 38.74% * 50048 15.869s (3153.8 samples/s);\n",
      "Finished Epoch[10 of 20]: [Training] loss = 1.073771 * 49984, metric = 36.45% * 49984 15.772s (3169.2 samples/s);\n",
      "Learning rate per sample: 0.00046875\n",
      "Finished Epoch[11 of 20]: [Training] loss = 0.884580 * 49984, metric = 30.31% * 49984 15.731s (3177.4 samples/s);\n",
      "Finished Epoch[12 of 20]: [Training] loss = 0.857084 * 49984, metric = 29.06% * 49984 15.797s (3164.1 samples/s);\n",
      "Finished Epoch[13 of 20]: [Training] loss = 0.831888 * 50048, metric = 28.42% * 50048 15.807s (3166.2 samples/s);\n",
      "Finished Epoch[14 of 20]: [Training] loss = 0.822159 * 49984, metric = 28.02% * 49984 15.752s (3173.2 samples/s);\n",
      "Finished Epoch[15 of 20]: [Training] loss = 0.810429 * 49984, metric = 27.38% * 49984 15.807s (3162.1 samples/s);\n",
      "Finished Epoch[16 of 20]: [Training] loss = 0.801221 * 49984, metric = 27.12% * 49984 15.789s (3165.7 samples/s);\n",
      "Finished Epoch[17 of 20]: [Training] loss = 0.786630 * 50048, metric = 26.36% * 50048 15.825s (3162.6 samples/s);\n",
      "Finished Epoch[18 of 20]: [Training] loss = 0.778694 * 49984, metric = 26.24% * 49984 15.861s (3151.4 samples/s);\n",
      "Finished Epoch[19 of 20]: [Training] loss = 0.767496 * 49984, metric = 25.76% * 49984 15.765s (3170.6 samples/s);\n",
      "Finished Epoch[20 of 20]: [Training] loss = 0.757048 * 49984, metric = 25.46% * 49984 15.755s (3172.6 samples/s);\n",
      "Finished Evaluation [1]: Minibatch[1-625]: metric = 25.20% * 10000;\n",
      "=====================================\n",
      " azure blobxfer parameters [v0.12.1]\n",
      "=====================================\n",
      "             platform: Linux-4.4.0-81-generic-x86_64-with\n",
      "   python interpreter: CPython 3.5.2\n",
      "     package versions: az.common=1.1.4 az.sml=0.20.5 az.stor=0.33.0 crypt=1.6 req=2.12.3\n",
      "      subscription id: None\n",
      "      management cert: None\n",
      "   transfer direction: local->Azure\n",
      "       local resource: .\n",
      "      include pattern: None\n",
      "      remote resource: None\n",
      "   max num of workers: 18\n",
      "              timeout: None\n",
      "      storage account: batche2914cdbst\n",
      "              use SAS: True\n",
      "  upload as page blob: False\n",
      "  auto vhd->page blob: False\n",
      " upload to file share: False\n",
      " container/share name: output\n",
      "  container/share URI: https://batche2914cdbst.blob.core.windows.net/output\n",
      "    compute block MD5: False\n",
      "     compute file MD5: True\n",
      "    skip on MD5 match: True\n",
      "   chunk size (bytes): 4194304\n",
      "     create container: False\n",
      "  keep mismatched MD5: False\n",
      "     recursive if dir: True\n",
      "component strip on up: 1\n",
      "        remote delete: False\n",
      "           collate to: disabled\n",
      "      local overwrite: True\n",
      "      encryption mode: disabled\n",
      "         RSA key file: disabled\n",
      "         RSA key type: disabled\n",
      "=======================================\n",
      "\n",
      "script start time: 2017-06-21 09:41:19\n",
      "computing file md5 on: ./ConvNet_CIFAR10_model.dnn.ckp\n",
      "  >> md5: MrVV5tn/XFIzq5tuEbYtbQ==\n",
      "computing file md5 on: ./ConvNet_CIFAR10_model.dnn\n",
      "  >> md5: p4S3vl9biDdHI92d71Jf8w==\n",
      "computing file md5 on: ./d4c7c9c8_run_cifar10_cntk-training-job/ConvNet_CIFAR10_model.dnn\n",
      "  >> md5: TCDm4fvxAzXhjCX3MW10BA==\n",
      "computing file md5 on: ./d4c7c9c8_run_cifar10_cntk-training-job/model_results.json\n",
      "  >> md5: XZGD6HvVsIWl5VWrE8T6ag==\n",
      "detected 0 empty files to upload\n",
      "performing 7 put blocks/blobs and 4 put block lists\n",
      "spawning 7 worker threads\n",
      "\n",
      "\n",
      "13.727112770080566 MiB transfered, elapsed 0.3568391799926758 sec. Throughput = 307.74900380305365 Mbit/sec\n",
      "\n",
      "\n",
      "script elapsed time: 0.45090413093566895 sec\n",
      "script end time: 2017-06-21 09:41:19\n",
      "\n"
     ]
    }
   ],
   "source": [
    "shipyard data stream --filespec $JOB_ID,$TASK_ID,stdout.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If something goes wrong you can run the following command to get the stderr output from the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-21 09:42:19,202 DEBUG - attempting to stream file stderr.txt from job=cntk-training-job task=run_cifar10\n",
      "INFO:__main__:Processing /mnt/batch/tasks/shared/data/train_map.txt...\n",
      "INFO:__main__:Processing /mnt/batch/tasks/shared/data/test_map.txt...\n",
      "INFO:__main__:Running network with: \n",
      "                2 convolution layers\n",
      "                64  minibatch size\n",
      "                for 20 epochs\n",
      "Selected GPU[0] Tesla K80 as the process wide default device.\n",
      "ping [requestnodes (before change)]: 1 nodes pinging each other\n",
      "ping [requestnodes (after change)]: 1 nodes pinging each other\n",
      "requestnodes [MPIWrapperMpi]: using 1 out of 1 MPI nodes on a single host (1 requested); we (0) are in (participating)\n",
      "ping [mpihelper]: 1 nodes pinging each other\n",
      "-------------------------------------------------------------------\n",
      "Build info: \n",
      "\n",
      "\t\tBuilt time: May 31 2017 17:14:18\n",
      "\t\tLast modified date: Sun May 21 16:00:04 2017\n",
      "\t\tBuild type: release\n",
      "\t\tBuild target: GPU\n",
      "\t\tWith 1bit-SGD: no\n",
      "\t\tWith ASGD: yes\n",
      "\t\tMath lib: mkl\n",
      "\t\tCUDA_PATH: /usr/local/cuda-8.0\n",
      "\t\tCUB_PATH: /usr/local/cub-1.4.1\n",
      "\t\tCUDNN_PATH: /usr/local/cudnn-5.1\n",
      "\t\tBuild Branch: HEAD\n",
      "\t\tBuild SHA1: ade8bc05d30e61160da729aee078e22f8bd4fced\n",
      "\t\tBuilt by Source/CNTK/buildinfo.h$$0 on 2a77e259aee8\n",
      "\t\tBuild Path: /home/philly/jenkins/workspace/CNTK-Build-Linux\n",
      "\t\tMPI distribution: Open MPI\n",
      "\t\tMPI version: 1.10.3\n",
      "-------------------------------------------------------------------\n",
      "INFO:__main__:Saving results {'parameters': {'minibatch_size': 64, 'num_convolution_layers': 2, 'max_epochs': 20}, 'test_metric': 0.252}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "shipyard data stream --filespec $JOB_ID,$TASK_ID,stderr.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To delete the job use the command below. Just be aware that this will get rid of all the files created by the job and tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-21 14:59:08,640 INFO - Deleting job: cntk-training-job\n",
      "2017-06-21 14:59:08,640 DEBUG - disabling job cntk-training-job first due to task termination\n",
      "2017-06-21 14:59:09,569 DEBUG - Skipping termination of completed task run_cifar10 on job cntk-training-job\n",
      "2017-06-21 14:59:09,886 DEBUG - waiting for job cntk-training-job to delete\n",
      "2017-06-21 14:59:42,427 INFO - job cntk-training-job does not exist\n"
     ]
    }
   ],
   "source": [
    "shipyard jobs del -y --termtasks --wait"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Next notebook: Scoring](03_Scoring_model.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
