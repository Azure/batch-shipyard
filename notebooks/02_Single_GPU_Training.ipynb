{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II - Single GPU Training\n",
    "In the previous notebok we set up our pool of GPU nodes. In this notebook we are going to get one of the nodes in the pool to train a deep learning model for small number of epochs. The model and results of the training will be then loaded into blob storage for later retrieval.\n",
    "\n",
    "* [Setup](#section1)\n",
    "* [Configure job](#section2)\n",
    "* [Submit job](#section3)\n",
    "* [Delete job](#section4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a simple alias for Batch Shipyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%alias shipyard SHIPYARD_CONFIGDIR=config python $HOME/batch-shipyard/shipyard.py %l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that everything is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: shipyard.py [OPTIONS] COMMAND [ARGS]...\r\n",
      "\r\n",
      "  Batch Shipyard: Provision and Execute Docker Workloads on Azure Batch\r\n",
      "\r\n",
      "Options:\r\n",
      "  --version   Show the version and exit.\r\n",
      "  -h, --help  Show this message and exit.\r\n",
      "\r\n",
      "Commands:\r\n",
      "  cert      Certificate actions\r\n",
      "  data      Data actions\r\n",
      "  fs        Filesystem in Azure actions\r\n",
      "  jobs      Jobs actions\r\n",
      "  keyvault  KeyVault actions\r\n",
      "  misc      Miscellaneous actions\r\n",
      "  pool      Pool actions\r\n",
      "  storage   Storage actions\r\n"
     ]
    }
   ],
   "source": [
    "shipyard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get some variables stored in the Setup notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_json(filename):\n",
    "    with open(filename, 'r') as infile:\n",
    "        return json.load(infile)\n",
    "    \n",
    "account_info = read_json('account_information.json')\n",
    "\n",
    "IMAGE_NAME = account_info['IMAGE_NAME']\n",
    "STORAGE_ALIAS = account_info['STORAGE_ALIAS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "## Configure job\n",
    "In the dictonary below we define the properties of the job we wish to execute. You can see that we have specified that the image to use is the one we defined at the beginning of this notebook. Another interesting note is that we specify the gpu switch to true since we want the job to use the GPU. Finally the command is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "source /cntk/activate-cntk\n",
    "python ConvNet_CIFAR10.py\n",
    "```\n",
    "\n",
    "Which in essence activates the CNTK Anaconda environment then runs the **ConvNet_CIFAR10.py** script which will train and evaluate the model.\n",
    "\n",
    "In the jobs json below, `resource_files` contains the script to train our CNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TASK_ID = 'run_cifar10' # This should be changed per task\n",
    "\n",
    "JOB_ID = 'cntk-training-job'\n",
    "\n",
    "COMMAND = 'bash -c \"source /cntk/activate-cntk; python -u ConvNet_CIFAR10.py --datadir $AZ_BATCH_NODE_SHARED_DIR/data\"'\n",
    "\n",
    "jobs = {\n",
    "    \"job_specifications\": [\n",
    "        {\n",
    "            \"id\": JOB_ID,\n",
    "            \"tasks\": [\n",
    "                {\n",
    "                    \"id\": TASK_ID,\n",
    "                    \"image\": IMAGE_NAME,\n",
    "                    \"remove_container_after_exit\": True,\n",
    "                    \"command\": COMMAND,\n",
    "                    \"gpu\": True,\n",
    "                    \"resource_files\": [\n",
    "                        {\n",
    "                            \"file_path\": \"ConvNet_CIFAR10.py\",\n",
    "                            \"blob_source\": \"https://batchshipyardexamples.blob.core.windows.net/code/ConvNet_CIFAR10.py\",\n",
    "                            \"file_mode\":'0777'\n",
    "                        }\n",
    "                    ],\n",
    "                    \"output_data\": {\n",
    "                        \"azure_storage\": [\n",
    "                            {\n",
    "                                \"storage_account_settings\": STORAGE_ALIAS,\n",
    "                                \"container\": \"output\",\n",
    "                                \"source\": \"$AZ_BATCH_TASK_WORKING_DIR/Models\"\n",
    "                            },\n",
    "                        ]\n",
    "                    },\n",
    "                }\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Write the jobs configuration to the `jobs.json` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"job_specifications\": [\n",
      "        {\n",
      "            \"id\": \"cntk-training-job\", \n",
      "            \"tasks\": [\n",
      "                {\n",
      "                    \"command\": \"bash -c \\\"source /cntk/activate-cntk; python -u ConvNet_CIFAR10.py --datadir $AZ_BATCH_NODE_SHARED_DIR/data\\\"\", \n",
      "                    \"gpu\": true, \n",
      "                    \"id\": \"run_cifar10\", \n",
      "                    \"image\": \"microsoft/cntk:2.0-gpu-python3.5-cuda8.0-cudnn5.1\", \n",
      "                    \"output_data\": {\n",
      "                        \"azure_storage\": [\n",
      "                            {\n",
      "                                \"container\": \"output\", \n",
      "                                \"source\": \"$AZ_BATCH_TASK_WORKING_DIR/Models\", \n",
      "                                \"storage_account_settings\": \"mystorageaccount\"\n",
      "                            }\n",
      "                        ]\n",
      "                    }, \n",
      "                    \"remove_container_after_exit\": true, \n",
      "                    \"resource_files\": [\n",
      "                        {\n",
      "                            \"blob_source\": \"https://batchshipyardexamples.blob.core.windows.net/code/ConvNet_CIFAR10.py\", \n",
      "                            \"file_mode\": \"0777\", \n",
      "                            \"file_path\": \"ConvNet_CIFAR10.py\"\n",
      "                        }\n",
      "                    ]\n",
      "                }\n",
      "            ]\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def write_json_to_file(json_dict, filename):\n",
    "    \"\"\" Simple function to write JSON dictionaries to files\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as outfile:\n",
    "        json.dump(json_dict, outfile)\n",
    "\n",
    "write_json_to_file(jobs, os.path.join('config', 'jobs.json'))\n",
    "print(json.dumps(jobs, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit job\n",
    "Check that everything is ok with our pool before we submit our jobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-07-12 13:37:17,721 DEBUG - listing nodes for pool gpupool\n",
      "2017-07-12 13:37:17,979 INFO - node_id=tvm-1392786932_1-20170712t132611z [state=ComputeNodeState.idle start_task_exit_code=0 scheduling_state=SchedulingState.enabled ip_address=10.0.0.6 vm_size=standard_nc6 dedicated=True total_tasks_run=0 running_tasks_count=0 total_tasks_succeeded=0]\n",
      "2017-07-12 13:37:17,979 INFO - node_id=tvm-1392786932_2-20170712t132611z [state=ComputeNodeState.idle start_task_exit_code=0 scheduling_state=SchedulingState.enabled ip_address=10.0.0.5 vm_size=standard_nc6 dedicated=True total_tasks_run=0 running_tasks_count=0 total_tasks_succeeded=0]\n",
      "2017-07-12 13:37:17,979 INFO - node_id=tvm-1392786932_3-20170712t132611z [state=ComputeNodeState.idle start_task_exit_code=0 scheduling_state=SchedulingState.enabled ip_address=10.0.0.4 vm_size=standard_nc6 dedicated=True total_tasks_run=0 running_tasks_count=0 total_tasks_succeeded=0]\n"
     ]
    }
   ],
   "source": [
    "shipyard pool listnodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have confirmed everything is working we can execute our job using the command below. The tail switch at the end will stream stdout from the node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-07-12 13:37:21,013 INFO - Adding job cntk-training-job to pool gpupool\n",
      "2017-07-12 13:37:21,520 INFO - uploading file /tmp/tmpfH3qcR as u'shipyardtaskrf-cntk-training-job/run_cifar10.shipyard.envlist'\n",
      "2017-07-12 13:37:21,761 DEBUG - submitting 1 tasks (0 -> 0) to job cntk-training-job\n",
      "2017-07-12 13:37:22,022 INFO - submitted all 1 tasks to job cntk-training-job\n",
      "2017-07-12 13:37:22,022 DEBUG - attempting to stream file stdout.txt from job=cntk-training-job task=run_cifar10\n",
      "\n",
      "************************************************************\n",
      "CNTK is activated.\n",
      "\n",
      "Please checkout tutorials and examples here:\n",
      "  /cntk/Tutorials\n",
      "  /cntk/Examples\n",
      "\n",
      "To deactivate the environment run\n",
      "\n",
      "  source /root/anaconda3/bin/deactivate\n",
      "\n",
      "************************************************************\n",
      "Creating NN model\n",
      "Learning rate per sample: 0.0015625\n",
      "Momentum per sample: 0.0\n",
      "Finished Epoch[1 of 20]: [Training] loss = 2.110387 * 50048, metric = 80.21% * 50048 17.387s (2878.5 samples/s);\n",
      "Finished Epoch[2 of 20]: [Training] loss = 1.856963 * 49984, metric = 70.04% * 49984 15.930s (3137.7 samples/s);\n",
      "Finished Epoch[3 of 20]: [Training] loss = 1.717015 * 49984, metric = 63.58% * 49984 15.758s (3172.0 samples/s);\n",
      "Finished Epoch[4 of 20]: [Training] loss = 1.592324 * 49984, metric = 58.27% * 49984 15.736s (3176.4 samples/s);\n",
      "Finished Epoch[5 of 20]: [Training] loss = 1.479462 * 50048, metric = 53.48% * 50048 15.814s (3164.8 samples/s);\n",
      "Finished Epoch[6 of 20]: [Training] loss = 1.382365 * 49984, metric = 49.45% * 49984 15.873s (3149.0 samples/s);\n",
      "Finished Epoch[7 of 20]: [Training] loss = 1.285505 * 49984, metric = 45.53% * 49984 15.955s (3132.8 samples/s);\n",
      "Finished Epoch[8 of 20]: [Training] loss = 1.216698 * 49984, metric = 42.62% * 49984 15.975s (3128.9 samples/s);\n",
      "Finished Epoch[9 of 20]: [Training] loss = 1.150520 * 50048, metric = 39.79% * 50048 15.905s (3146.7 samples/s);\n",
      "Finished Epoch[10 of 20]: [Training] loss = 1.099242 * 49984, metric = 37.95% * 49984 15.802s (3163.1 samples/s);\n",
      "Learning rate per sample: 0.00046875\n",
      "Finished Epoch[11 of 20]: [Training] loss = 0.910610 * 49984, metric = 31.37% * 49984 15.879s (3147.8 samples/s);\n",
      "Finished Epoch[12 of 20]: [Training] loss = 0.876847 * 49984, metric = 30.09% * 49984 15.894s (3144.8 samples/s);\n",
      "Finished Epoch[13 of 20]: [Training] loss = 0.858968 * 50048, metric = 29.17% * 50048 15.914s (3144.9 samples/s);\n",
      "Finished Epoch[14 of 20]: [Training] loss = 0.849888 * 49984, metric = 28.93% * 49984 16.038s (3116.6 samples/s);\n",
      "Finished Epoch[15 of 20]: [Training] loss = 0.832129 * 49984, metric = 28.28% * 49984 15.854s (3152.8 samples/s);\n",
      "Finished Epoch[16 of 20]: [Training] loss = 0.822232 * 49984, metric = 27.87% * 49984 15.831s (3157.3 samples/s);\n",
      "Finished Epoch[17 of 20]: [Training] loss = 0.808333 * 50048, metric = 27.20% * 50048 15.833s (3161.0 samples/s);\n",
      "Finished Epoch[18 of 20]: [Training] loss = 0.788930 * 49984, metric = 26.88% * 49984 15.835s (3156.6 samples/s);\n",
      "Finished Epoch[19 of 20]: [Training] loss = 0.784160 * 49984, metric = 26.47% * 49984 15.888s (3146.0 samples/s);\n",
      "Finished Epoch[20 of 20]: [Training] loss = 0.773588 * 49984, metric = 26.03% * 49984 16.076s (3109.2 samples/s);\n",
      "Finished Evaluation [1]: Minibatch[1-625]: metric = 29.74% * 10000;\n",
      "=====================================\n",
      " azure blobxfer parameters [v0.12.1]\n",
      "=====================================\n",
      "             platform: Linux-4.4.0-81-generic-x86_64-with\n",
      "   python interpreter: CPython 3.5.2\n",
      "     package versions: az.common=1.1.4 az.sml=0.20.5 az.stor=0.33.0 crypt=1.6 req=2.12.3\n",
      "      subscription id: None\n",
      "      management cert: None\n",
      "   transfer direction: local->Azure\n",
      "       local resource: .\n",
      "      include pattern: None\n",
      "      remote resource: None\n",
      "   max num of workers: 18\n",
      "              timeout: None\n",
      "      storage account: batch0e43a94est\n",
      "              use SAS: True\n",
      "  upload as page blob: False\n",
      "  auto vhd->page blob: False\n",
      " upload to file share: False\n",
      " container/share name: output\n",
      "  container/share URI: https://batch0e43a94est.blob.core.windows.net/output\n",
      "    compute block MD5: False\n",
      "     compute file MD5: True\n",
      "    skip on MD5 match: True\n",
      "   chunk size (bytes): 4194304\n",
      "     create container: False\n",
      "  keep mismatched MD5: False\n",
      "     recursive if dir: True\n",
      "component strip on up: 1\n",
      "        remote delete: False\n",
      "           collate to: disabled\n",
      "      local overwrite: True\n",
      "      encryption mode: disabled\n",
      "         RSA key file: disabled\n",
      "         RSA key type: disabled\n",
      "=======================================\n",
      "\n",
      "script start time: 2017-07-12 13:42:47\n",
      "computing file md5 on: ./ConvNet_CIFAR10_model.dnn\n",
      "  >> md5: ePg04BedGdHho7H0gX1kIQ==\n",
      "computing file md5 on: ./ConvNet_CIFAR10_model.dnn.ckp\n",
      "  >> md5: sTFX92jN2wJrg7Sheu2+Iw==\n",
      "computing file md5 on: ./801cbd93_run_cifar10_cntk-training-job/model_results.json\n",
      "  >> md5: H7Xl2MLIyVGp2G+aG1hAJg==\n",
      "computing file md5 on: ./801cbd93_run_cifar10_cntk-training-job/ConvNet_CIFAR10_model.dnn\n",
      "  >> md5: phwZo2WWpTd3+P4OI6zDLw==\n",
      "detected 0 empty files to upload\n",
      "performing 7 put blocks/blobs and 4 put block lists\n",
      "spawning 7 worker threads\n",
      "\n",
      "\n",
      "13.727113723754883 MiB transfered, elapsed 0.573434591293335 sec. Throughput = 191.50729910163943 Mbit/sec\n",
      "\n",
      "\n",
      "script elapsed time: 0.7547557353973389 sec\n",
      "script end time: 2017-07-12 13:42:48\n",
      "\n"
     ]
    }
   ],
   "source": [
    "shipyard jobs add --tail stdout.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also retrieve this `stdout.txt` data independently of `--tail` above by using the `data stream` command. Note that when you delete the job all this information is also deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-07-12 13:51:59,954 DEBUG - attempting to stream file stdout.txt from job=cntk-training-job task=run_cifar10\n",
      "\n",
      "************************************************************\n",
      "CNTK is activated.\n",
      "\n",
      "Please checkout tutorials and examples here:\n",
      "  /cntk/Tutorials\n",
      "  /cntk/Examples\n",
      "\n",
      "To deactivate the environment run\n",
      "\n",
      "  source /root/anaconda3/bin/deactivate\n",
      "\n",
      "************************************************************\n",
      "Creating NN model\n",
      "Learning rate per sample: 0.0015625\n",
      "Momentum per sample: 0.0\n",
      "Finished Epoch[1 of 20]: [Training] loss = 2.110387 * 50048, metric = 80.21% * 50048 17.387s (2878.5 samples/s);\n",
      "Finished Epoch[2 of 20]: [Training] loss = 1.856963 * 49984, metric = 70.04% * 49984 15.930s (3137.7 samples/s);\n",
      "Finished Epoch[3 of 20]: [Training] loss = 1.717015 * 49984, metric = 63.58% * 49984 15.758s (3172.0 samples/s);\n",
      "Finished Epoch[4 of 20]: [Training] loss = 1.592324 * 49984, metric = 58.27% * 49984 15.736s (3176.4 samples/s);\n",
      "Finished Epoch[5 of 20]: [Training] loss = 1.479462 * 50048, metric = 53.48% * 50048 15.814s (3164.8 samples/s);\n",
      "Finished Epoch[6 of 20]: [Training] loss = 1.382365 * 49984, metric = 49.45% * 49984 15.873s (3149.0 samples/s);\n",
      "Finished Epoch[7 of 20]: [Training] loss = 1.285505 * 49984, metric = 45.53% * 49984 15.955s (3132.8 samples/s);\n",
      "Finished Epoch[8 of 20]: [Training] loss = 1.216698 * 49984, metric = 42.62% * 49984 15.975s (3128.9 samples/s);\n",
      "Finished Epoch[9 of 20]: [Training] loss = 1.150520 * 50048, metric = 39.79% * 50048 15.905s (3146.7 samples/s);\n",
      "Finished Epoch[10 of 20]: [Training] loss = 1.099242 * 49984, metric = 37.95% * 49984 15.802s (3163.1 samples/s);\n",
      "Learning rate per sample: 0.00046875\n",
      "Finished Epoch[11 of 20]: [Training] loss = 0.910610 * 49984, metric = 31.37% * 49984 15.879s (3147.8 samples/s);\n",
      "Finished Epoch[12 of 20]: [Training] loss = 0.876847 * 49984, metric = 30.09% * 49984 15.894s (3144.8 samples/s);\n",
      "Finished Epoch[13 of 20]: [Training] loss = 0.858968 * 50048, metric = 29.17% * 50048 15.914s (3144.9 samples/s);\n",
      "Finished Epoch[14 of 20]: [Training] loss = 0.849888 * 49984, metric = 28.93% * 49984 16.038s (3116.6 samples/s);\n",
      "Finished Epoch[15 of 20]: [Training] loss = 0.832129 * 49984, metric = 28.28% * 49984 15.854s (3152.8 samples/s);\n",
      "Finished Epoch[16 of 20]: [Training] loss = 0.822232 * 49984, metric = 27.87% * 49984 15.831s (3157.3 samples/s);\n",
      "Finished Epoch[17 of 20]: [Training] loss = 0.808333 * 50048, metric = 27.20% * 50048 15.833s (3161.0 samples/s);\n",
      "Finished Epoch[18 of 20]: [Training] loss = 0.788930 * 49984, metric = 26.88% * 49984 15.835s (3156.6 samples/s);\n",
      "Finished Epoch[19 of 20]: [Training] loss = 0.784160 * 49984, metric = 26.47% * 49984 15.888s (3146.0 samples/s);\n",
      "Finished Epoch[20 of 20]: [Training] loss = 0.773588 * 49984, metric = 26.03% * 49984 16.076s (3109.2 samples/s);\n",
      "Finished Evaluation [1]: Minibatch[1-625]: metric = 29.74% * 10000;\n",
      "=====================================\n",
      " azure blobxfer parameters [v0.12.1]\n",
      "=====================================\n",
      "             platform: Linux-4.4.0-81-generic-x86_64-with\n",
      "   python interpreter: CPython 3.5.2\n",
      "     package versions: az.common=1.1.4 az.sml=0.20.5 az.stor=0.33.0 crypt=1.6 req=2.12.3\n",
      "      subscription id: None\n",
      "      management cert: None\n",
      "   transfer direction: local->Azure\n",
      "       local resource: .\n",
      "      include pattern: None\n",
      "      remote resource: None\n",
      "   max num of workers: 18\n",
      "              timeout: None\n",
      "      storage account: batch0e43a94est\n",
      "              use SAS: True\n",
      "  upload as page blob: False\n",
      "  auto vhd->page blob: False\n",
      " upload to file share: False\n",
      " container/share name: output\n",
      "  container/share URI: https://batch0e43a94est.blob.core.windows.net/output\n",
      "    compute block MD5: False\n",
      "     compute file MD5: True\n",
      "    skip on MD5 match: True\n",
      "   chunk size (bytes): 4194304\n",
      "     create container: False\n",
      "  keep mismatched MD5: False\n",
      "     recursive if dir: True\n",
      "component strip on up: 1\n",
      "        remote delete: False\n",
      "           collate to: disabled\n",
      "      local overwrite: True\n",
      "      encryption mode: disabled\n",
      "         RSA key file: disabled\n",
      "         RSA key type: disabled\n",
      "=======================================\n",
      "\n",
      "script start time: 2017-07-12 13:42:47\n",
      "computing file md5 on: ./ConvNet_CIFAR10_model.dnn\n",
      "  >> md5: ePg04BedGdHho7H0gX1kIQ==\n",
      "computing file md5 on: ./ConvNet_CIFAR10_model.dnn.ckp\n",
      "  >> md5: sTFX92jN2wJrg7Sheu2+Iw==\n",
      "computing file md5 on: ./801cbd93_run_cifar10_cntk-training-job/model_results.json\n",
      "  >> md5: H7Xl2MLIyVGp2G+aG1hAJg==\n",
      "computing file md5 on: ./801cbd93_run_cifar10_cntk-training-job/ConvNet_CIFAR10_model.dnn\n",
      "  >> md5: phwZo2WWpTd3+P4OI6zDLw==\n",
      "detected 0 empty files to upload\n",
      "performing 7 put blocks/blobs and 4 put block lists\n",
      "spawning 7 worker threads\n",
      "\n",
      "\n",
      "13.727113723754883 MiB transfered, elapsed 0.573434591293335 sec. Throughput = 191.50729910163943 Mbit/sec\n",
      "\n",
      "\n",
      "script elapsed time: 0.7547557353973389 sec\n",
      "script end time: 2017-07-12 13:42:48\n",
      "\n"
     ]
    }
   ],
   "source": [
    "shipyard data stream --filespec $JOB_ID,$TASK_ID,stdout.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If something goes wrong you can run the following command to get the stderr output from the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-07-12 13:52:04,470 DEBUG - attempting to stream file stderr.txt from job=cntk-training-job task=run_cifar10\n",
      "INFO:__main__:Processing /mnt/batch/tasks/shared/data/train_map.txt...\n",
      "INFO:__main__:Processing /mnt/batch/tasks/shared/data/test_map.txt...\n",
      "INFO:__main__:Running network with: \n",
      "                2 convolution layers\n",
      "                64  minibatch size\n",
      "                for 20 epochs\n",
      "Selected GPU[0] Tesla K80 as the process wide default device.\n",
      "ping [requestnodes (before change)]: 1 nodes pinging each other\n",
      "ping [requestnodes (after change)]: 1 nodes pinging each other\n",
      "requestnodes [MPIWrapperMpi]: using 1 out of 1 MPI nodes on a single host (1 requested); we (0) are in (participating)\n",
      "ping [mpihelper]: 1 nodes pinging each other\n",
      "-------------------------------------------------------------------\n",
      "Build info: \n",
      "\n",
      "\t\tBuilt time: May 31 2017 17:14:18\n",
      "\t\tLast modified date: Sun May 21 16:00:04 2017\n",
      "\t\tBuild type: release\n",
      "\t\tBuild target: GPU\n",
      "\t\tWith 1bit-SGD: no\n",
      "\t\tWith ASGD: yes\n",
      "\t\tMath lib: mkl\n",
      "\t\tCUDA_PATH: /usr/local/cuda-8.0\n",
      "\t\tCUB_PATH: /usr/local/cub-1.4.1\n",
      "\t\tCUDNN_PATH: /usr/local/cudnn-5.1\n",
      "\t\tBuild Branch: HEAD\n",
      "\t\tBuild SHA1: ade8bc05d30e61160da729aee078e22f8bd4fced\n",
      "\t\tBuilt by Source/CNTK/buildinfo.h$$0 on 2a77e259aee8\n",
      "\t\tBuild Path: /home/philly/jenkins/workspace/CNTK-Build-Linux\n",
      "\t\tMPI distribution: Open MPI\n",
      "\t\tMPI version: 1.10.3\n",
      "-------------------------------------------------------------------\n",
      "INFO:__main__:Saving results {'test_metric': 0.2974, 'parameters': {'minibatch_size': 64, 'num_convolution_layers': 2, 'max_epochs': 20}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "shipyard data stream --filespec $JOB_ID,$TASK_ID,stderr.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To delete the job use the command below. Just be aware that this will get rid of all the files created by the job and tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-07-12 13:52:08,691 INFO - Deleting job: cntk-training-job\n",
      "2017-07-12 13:52:08,691 DEBUG - disabling job cntk-training-job first due to task termination\n",
      "2017-07-12 13:52:09,564 DEBUG - Skipping termination of completed task run_cifar10 on job cntk-training-job\n",
      "2017-07-12 13:52:10,050 DEBUG - waiting for job cntk-training-job to delete\n",
      "2017-07-12 13:52:42,000 INFO - job cntk-training-job does not exist\n"
     ]
    }
   ],
   "source": [
    "shipyard jobs del -y --termtasks --wait"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Next notebook: Scoring](03_Scoring_model.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
