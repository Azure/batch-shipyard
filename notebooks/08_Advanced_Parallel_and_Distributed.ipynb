{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIII - Parallel and Distributed Execution\n",
    "In this notebook, we will execute training across multiple nodes (or in parallel across a single node over multiple GPUs). We will train an image classification model with Resnet20 on the CIFAR-10 data set across multiple nodes in this notebook.\n",
    "\n",
    "Azure Batch and Batch Shipyard have the ability to perform \"gang scheduling\" or scheduling multiple nodes for a single task. This is most commonly used for Message Passing Interface (MPI) jobs.\n",
    "\n",
    "* [Setup](#section1)\n",
    "* [Configure and Submit MPI Job and Submit](#section2)\n",
    "* [Delete Multi-Instance Job](#section3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a simple alias for Batch Shipyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%alias shipyard SHIPYARD_CONFIGDIR=config python $HOME/batch-shipyard/shipyard.py %l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that everything is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: shipyard.py [OPTIONS] COMMAND [ARGS]...\r\n",
      "\r\n",
      "  Batch Shipyard: Provision and Execute Docker Workloads on Azure Batch\r\n",
      "\r\n",
      "Options:\r\n",
      "  --version   Show the version and exit.\r\n",
      "  -h, --help  Show this message and exit.\r\n",
      "\r\n",
      "Commands:\r\n",
      "  cert      Certificate actions\r\n",
      "  data      Data actions\r\n",
      "  fs        Filesystem in Azure actions\r\n",
      "  jobs      Jobs actions\r\n",
      "  keyvault  KeyVault actions\r\n",
      "  misc      Miscellaneous actions\r\n",
      "  pool      Pool actions\r\n",
      "  storage   Storage actions\r\n"
     ]
    }
   ],
   "source": [
    "shipyard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first delete the pool used in the non-advanced notebooks and wait for it to be removed so we can free up our core quota. We need to create a new pool with different settings and Docker image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-21 15:30:39,661 INFO - Deleting pool: gpupool\n",
      "2017-06-21 15:30:39,902 DEBUG - clearing table (pk=batcha3dc41fdba$gpupool): shipyardregistry\n",
      "2017-06-21 15:30:40,249 DEBUG - clearing table (pk=batcha3dc41fdba$gpupool): shipyardgr\n",
      "2017-06-21 15:30:40,347 DEBUG - clearing table (pk=batcha3dc41fdba$gpupool): shipyardperf\n",
      "2017-06-21 15:30:40,398 DEBUG - clearing table (pk=batcha3dc41fdba$gpupool): shipyarddht\n",
      "2017-06-21 15:30:40,451 DEBUG - clearing table (pk=batcha3dc41fdba$gpupool): shipyardimages\n",
      "2017-06-21 15:30:40,552 DEBUG - clearing table (pk=batcha3dc41fdba$gpupool): shipyardtorrentinfo\n",
      "2017-06-21 15:30:40,604 DEBUG - deleting queue: shipyardgr-batcha3dc41fdba-gpupool\n",
      "2017-06-21 15:30:40,825 DEBUG - deleting container: shipyardtor-batcha3dc41fdba-gpupool\n",
      "2017-06-21 15:30:40,995 DEBUG - deleting container: shipyardrf-batcha3dc41fdba-gpupool\n",
      "2017-06-21 15:30:41,036 DEBUG - waiting for pool gpupool to delete\n"
     ]
    }
   ],
   "source": [
    "shipyard pool del -y --wait"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the account information we saved earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def read_json(filename):\n",
    "    with open(filename, 'r') as infile:\n",
    "        return json.load(infile)\n",
    "    \n",
    "def write_json_to_file(json_dict, filename):\n",
    "    \"\"\" Simple function to write JSON dictionaries to files\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as outfile:\n",
    "        json.dump(json_dict, outfile)\n",
    "\n",
    "account_info = read_json('account_information.json')\n",
    "\n",
    "storage_account_key = account_info['storage_account_key']\n",
    "storage_account_name = account_info['storage_account_name']\n",
    "STORAGE_ALIAS = account_info['STORAGE_ALIAS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the `resource_files` to randomly download train and test data for CNTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "IMAGE_NAME = 'alfpark/cntk:2.0.rc2-gpu-1bit-sgd-python3.5-cuda8.0-cudnn5.1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data set conversion scripts to be uploaded. On real production runs, we would already have this data pre-converted instead of converting at the time of node startup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we'll create an MPI helper script for executing the MPI job. This helper script does the following:\n",
    "1. Ensures that there are GPUs available to execute the task.\n",
    "2. Parses the `$AZ_BATCH_HOST_LIST` for all of the hosts participating in the MPI job and creates a `hostfile` from it\n",
    "3. Computes the total number of slots (processors)\n",
    "4. Sets the proper CNTK training directory, script and options\n",
    "5. Executes the MPI job via `mpirun`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing run_cifar10.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_cifar10.sh\n",
    "#!/usr/bin/env bash\n",
    "\n",
    "set -e\n",
    "set -o pipefail\n",
    "\n",
    "# get number of GPUs on machine\n",
    "ngpus=$(nvidia-smi -L | wc -l)\n",
    "echo \"num gpus: $ngpus\"\n",
    "\n",
    "if [ $ngpus -eq 0 ]; then\n",
    "    echo \"No GPUs detected.\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# get number of nodes\n",
    "IFS=',' read -ra HOSTS <<< \"$AZ_BATCH_HOST_LIST\"\n",
    "nodes=${#HOSTS[@]}\n",
    "\n",
    "# create hostfile\n",
    "touch hostfile\n",
    ">| hostfile\n",
    "for node in \"${HOSTS[@]}\"\n",
    "do\n",
    "    echo $node slots=$ngpus max-slots=$ngpus >> hostfile\n",
    "done\n",
    "\n",
    "# compute number of processors\n",
    "np=$(($nodes * $ngpus))\n",
    "\n",
    "# print configuration\n",
    "echo \"num nodes: $nodes\"\n",
    "echo \"hosts: ${HOSTS[@]}\"\n",
    "echo \"num mpi processes: $np\"\n",
    "\n",
    "# set cntk related vars\n",
    "trainscript=$AZ_BATCH_TASK_SHARED_DIR/TrainResNet_CIFAR10.py\n",
    "\n",
    "# set training options\n",
    "trainopts=\"--datadir $AZ_BATCH_NODE_SHARED_DIR/data --modeldir $AZ_BATCH_TASK_WORKING_DIR/output --network resnet20 --distributed True -q 1 -a 0\"\n",
    "\n",
    "# execute mpi job\n",
    "/root/openmpi/bin/mpirun --allow-run-as-root --mca btl_tcp_if_exclude docker0 \\\n",
    "    -np $np --hostfile hostfile -x LD_LIBRARY_PATH\\\n",
    "    /bin/bash -c \"source /cntk/activate-cntk; python -u $trainscript $trainopts $*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move the files into a directory to be uploaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 12\r\n",
      "drwxr-xr-x  2 nbuser nbuser 4096 Jun 21 15:35 ./\r\n",
      "drwx------ 19 nbuser nbuser 4096 Jun 21 15:35 ../\r\n",
      "-rw-r--r--  1 nbuser nbuser 1080 Jun 21 15:35 run_cifar10.sh\r\n"
     ]
    }
   ],
   "source": [
    "INPUT_CONTAINER = 'input-dist'\n",
    "UPLOAD_DIR = 'dist_upload'\n",
    "\n",
    "!rm -rf $UPLOAD_DIR\n",
    "!mkdir -p $UPLOAD_DIR\n",
    "!mv run_cifar10.sh $UPLOAD_DIR\n",
    "!ls -alF $UPLOAD_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create the config structure to directly reference these files to ingress into Azure Storage. This obviates the need to call `blobxfer` as it will be done for us during pool creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"batch_shipyard\": {\n",
    "        \"storage_account_settings\": STORAGE_ALIAS\n",
    "    },\n",
    "    \"global_resources\": {\n",
    "        \"docker_images\": [\n",
    "            IMAGE_NAME\n",
    "        ],\n",
    "        \"files\": [\n",
    "            {\n",
    "                \"source\": {\n",
    "                    \"path\": UPLOAD_DIR\n",
    "                },\n",
    "                \"destination\": {\n",
    "                    \"storage_account_settings\": STORAGE_ALIAS,\n",
    "                    \"data_transfer\": {\n",
    "                        \"container\": INPUT_CONTAINER\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create the pool specification with a few modifications for this particular execution:\n",
    "- `inter_node_communication_enabled` will ensure nodes are allocated such that they can communicate with each other (e.g., send and receive network packets)\n",
    "- `input_data` specifies the scripts we created above to be downloaded into `$AZ_BATCH_NODE_SHARED_DIR/cifar10_data`\n",
    "- `transfer_files_on_pool_creation` will transfer the `files` specified in `global_resources` to be transferred during pool creation (i.e., `pool add`)\n",
    "- `resource_files` are the CNTK train and test data files\n",
    "- `additional_node_prep_commands` are commands to execute for node preparation of all compute nodes. Our additional node prep command is to execute the conversion script we created in an earlier step above\n",
    "\n",
    "**Note:** Most often it is better to scale up the execution first, prior to scale out. Due to our default core quota of just 20 cores, we are using 3 `STANDARD_NC6` nodes. In real production runs, we'd most likely scale up to multiple GPUs within a single node (parallel execution) such as `STANDARD_NC12` or `STANDARD_NC24` prior to scaling out to multiple NC nodes (parallel and distributed execution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "POOL_ID = 'gpupool-multi-instance'\n",
    "\n",
    "pool = {\n",
    "    \"pool_specification\": {\n",
    "        \"id\": POOL_ID,\n",
    "        \"vm_size\": \"STANDARD_NC6\",\n",
    "        \"vm_count\": {\n",
    "            \"dedicated\": 3\n",
    "        },\n",
    "        \"publisher\": \"Canonical\",\n",
    "        \"offer\": \"UbuntuServer\",\n",
    "        \"sku\": \"16.04-LTS\",\n",
    "        \"ssh\": {\n",
    "            \"username\": \"docker\"\n",
    "        },\n",
    "        \"inter_node_communication_enabled\": True,\n",
    "        \"reboot_on_start_task_failed\": False,\n",
    "        \"block_until_all_global_resources_loaded\": True,\n",
    "        \"transfer_files_on_pool_creation\": True,\n",
    "        \"input_data\": {\n",
    "            \"azure_storage\": [\n",
    "                {\n",
    "                    \"storage_account_settings\": STORAGE_ALIAS,\n",
    "                    \"container\": INPUT_CONTAINER,\n",
    "                    \"destination\": \"$AZ_BATCH_NODE_SHARED_DIR\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        \"resource_files\": [\n",
    "            {\n",
    "                \"file_path\": \"cifar_data_processing.py\",\n",
    "                \"blob_source\": \"https://batchshipyardexamples.blob.core.windows.net/code/cifar_data_processing.py\",\n",
    "                \"file_mode\":'0777'\n",
    "            },\n",
    "            {\n",
    "                \"file_path\": \"convert_cifar10.sh\",\n",
    "                \"blob_source\": \"https://batchshipyardexamples.blob.core.windows.net/code/convert_cifar10.sh\",\n",
    "                \"file_mode\":'0777'\n",
    "            }\n",
    "        ],\n",
    "         \"additional_node_prep_commands\": [\n",
    "            \"/bin/bash convert_cifar10.sh {} $AZ_BATCH_NODE_SHARED_DIR/data\".format(IMAGE_NAME),\n",
    "             \"chmod 777 $AZ_BATCH_NODE_SHARED_DIR/run_cifar10.sh\"\n",
    "        ]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory 'config': File exists\n",
      "{\n",
      "    \"batch_shipyard\": {\n",
      "        \"storage_account_settings\": \"mystorageaccount\"\n",
      "    }, \n",
      "    \"global_resources\": {\n",
      "        \"docker_images\": [\n",
      "            \"alfpark/cntk:2.0.rc2-gpu-1bit-sgd-python3.5-cuda8.0-cudnn5.1\"\n",
      "        ], \n",
      "        \"files\": [\n",
      "            {\n",
      "                \"destination\": {\n",
      "                    \"data_transfer\": {\n",
      "                        \"container\": \"input-dist\"\n",
      "                    }, \n",
      "                    \"storage_account_settings\": \"mystorageaccount\"\n",
      "                }, \n",
      "                \"source\": {\n",
      "                    \"path\": \"dist_upload\"\n",
      "                }\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"pool_specification\": {\n",
      "        \"additional_node_prep_commands\": [\n",
      "            \"/bin/bash convert_cifar10.sh alfpark/cntk:2.0.rc2-gpu-1bit-sgd-python3.5-cuda8.0-cudnn5.1 $AZ_BATCH_NODE_SHARED_DIR/data\", \n",
      "            \"chmod 777 $AZ_BATCH_NODE_SHARED_DIR/run_cifar10.sh\"\n",
      "        ], \n",
      "        \"block_until_all_global_resources_loaded\": true, \n",
      "        \"id\": \"gpupool-multi-instance\", \n",
      "        \"input_data\": {\n",
      "            \"azure_storage\": [\n",
      "                {\n",
      "                    \"container\": \"input-dist\", \n",
      "                    \"destination\": \"$AZ_BATCH_NODE_SHARED_DIR\", \n",
      "                    \"storage_account_settings\": \"mystorageaccount\"\n",
      "                }\n",
      "            ]\n",
      "        }, \n",
      "        \"inter_node_communication_enabled\": true, \n",
      "        \"offer\": \"UbuntuServer\", \n",
      "        \"publisher\": \"Canonical\", \n",
      "        \"reboot_on_start_task_failed\": false, \n",
      "        \"resource_files\": [\n",
      "            {\n",
      "                \"blob_source\": \"https://batchshipyardexamples.blob.core.windows.net/code/cifar_data_processing.py\", \n",
      "                \"file_mode\": \"0777\", \n",
      "                \"file_path\": \"cifar_data_processing.py\"\n",
      "            }, \n",
      "            {\n",
      "                \"blob_source\": \"https://batchshipyardexamples.blob.core.windows.net/code/convert_cifar10.sh\", \n",
      "                \"file_mode\": \"0777\", \n",
      "                \"file_path\": \"convert_cifar10.sh\"\n",
      "            }\n",
      "        ], \n",
      "        \"sku\": \"16.04-LTS\", \n",
      "        \"ssh\": {\n",
      "            \"username\": \"docker\"\n",
      "        }, \n",
      "        \"transfer_files_on_pool_creation\": true, \n",
      "        \"vm_count\": {\n",
      "            \"dedicated\": 3\n",
      "        }, \n",
      "        \"vm_size\": \"STANDARD_NC6\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!mkdir config\n",
    "write_json_to_file(config, os.path.join('config', 'config.json'))\n",
    "write_json_to_file(pool, os.path.join('config', 'pool.json'))\n",
    "print(json.dumps(config, indent=4, sort_keys=True))\n",
    "print(json.dumps(pool, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the pool, please be patient while the compute nodes are allocated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-21 15:37:26,928 WARNING - DEPRECATION WARNING: pool_specification:publisher/offer/sku is set instead of a vm_configuration complex property. This configuration will not be supported in future releases. Please update your configuration to include a vm_configuration property.\n",
      "2017-06-21 15:37:27,136 INFO - creating table: shipyardregistry\n",
      "2017-06-21 15:37:27,338 INFO - creating container: shipyardremotefs\n",
      "2017-06-21 15:37:27,555 INFO - creating table: shipyardgr\n",
      "2017-06-21 15:37:27,605 INFO - creating table: shipyarddht\n",
      "2017-06-21 15:37:27,662 INFO - creating table: shipyardimages\n",
      "2017-06-21 15:37:27,713 INFO - creating queue: shipyardgr-batcha3dc41fdba-gpupool-multi-instance\n",
      "2017-06-21 15:37:27,965 INFO - creating container: shipyardtor-batcha3dc41fdba-gpupool-multi-instance\n",
      "2017-06-21 15:37:28,015 INFO - creating table: shipyardtorrentinfo\n",
      "2017-06-21 15:37:28,062 INFO - creating container: shipyardrf-batcha3dc41fdba-gpupool-multi-instance\n",
      "2017-06-21 15:37:28,117 DEBUG - clearing table (pk=batcha3dc41fdba$gpupool-multi-instance): shipyardregistry\n",
      "2017-06-21 15:37:28,271 DEBUG - clearing table (pk=batcha3dc41fdba$gpupool-multi-instance): shipyardgr\n",
      "2017-06-21 15:37:28,317 DEBUG - clearing table (pk=batcha3dc41fdba$gpupool-multi-instance): shipyardperf\n",
      "2017-06-21 15:37:28,363 DEBUG - clearing table (pk=batcha3dc41fdba$gpupool-multi-instance): shipyarddht\n",
      "2017-06-21 15:37:28,407 DEBUG - clearing table (pk=batcha3dc41fdba$gpupool-multi-instance): shipyardimages\n",
      "2017-06-21 15:37:28,452 INFO - clearing queue: shipyardgr-batcha3dc41fdba-gpupool-multi-instance\n",
      "2017-06-21 15:37:28,576 INFO - deleting blobs: shipyardtor-batcha3dc41fdba-gpupool-multi-instance\n",
      "2017-06-21 15:37:28,621 DEBUG - clearing table (pk=batcha3dc41fdba$gpupool-multi-instance): shipyardtorrentinfo\n",
      "2017-06-21 15:37:28,666 INFO - deleting blobs: shipyardrf-batcha3dc41fdba-gpupool-multi-instance\n",
      "2017-06-21 15:37:28,710 WARNING - disabling block until all global resources loaded with transfer files on pool creation enabled\n",
      "2017-06-21 15:37:28,758 INFO - adding global resource: docker:alfpark/cntk:2.0.rc2-gpu-1bit-sgd-python3.5-cuda8.0-cudnn5.1\n",
      "2017-06-21 15:37:28,851 DEBUG - no virtual network settings specified\n",
      "2017-06-21 15:37:28,852 INFO - begin ingressing data from dist_upload to container input-dist\n",
      "2017-06-21 15:37:29,414 INFO - uploading file /home/nbuser/batch-shipyard/scripts/docker_jp_block.sh as u'docker_jp_block.sh'\n",
      "2017-06-21 15:37:29,480 INFO - =====================================\n",
      " azure blobxfer parameters [v0.12.1]\n",
      "=====================================\n",
      "             platform: Linux-4.4.0-75-generic-x86_64-with-debian-stretch-sid\n",
      "   python interpreter: CPython 2.7.11\n",
      "     package versions: az.common=1.1.5 az.sml=0.20.5 az.stor=0.34.2 crypt=1.9 req=2.18.1\n",
      "      subscription id: None\n",
      "      management cert: None\n",
      "   transfer direction: local->Azure\n",
      "       local resource: dist_upload\n",
      "      include pattern: None\n",
      "      remote resource: None\n",
      "   max num of workers: 24\n",
      "              timeout: None\n",
      "      storage account: batcha3dc41fdst\n",
      "              use SAS: False\n",
      "  upload as page blob: False\n",
      "  auto vhd->page blob: False\n",
      " upload to file share: False\n",
      " container/share name: input-dist\n",
      "  container/share URI: https://batcha3dc41fdst.blob.core.windows.net/input-dist\n",
      "    compute block MD5: False\n",
      "     compute file MD5: True\n",
      "    skip on MD5 match: True\n",
      "   chunk size (bytes): 4194304\n",
      "     create container: True\n",
      "  keep mismatched MD5: False\n",
      "     recursive if dir: True\n",
      "component strip on up: 1\n",
      "        remote delete: False\n",
      "           collate to: disabled\n",
      "      local overwrite: True\n",
      "      encryption mode: disabled\n",
      "         RSA key file: disabled\n",
      "         RSA key type: disabled\n",
      "=======================================\n",
      "\n",
      "script start time: 2017-06-21 15:37:29\n",
      "computing file md5 on: dist_upload/run_cifar10.sh\n",
      "  >> md5: oAjxPhIr6gNE1v68IgaiMQ==\n",
      "creating container, if needed: input-dist\n",
      "detected 0 empty files to upload\n",
      "performing 1 put blocks/blobs and 1 put block lists\n",
      "spawning 1 worker threads\n",
      "\n",
      "\n",
      "0.00102996826172 MiB transfered, elapsed 0.105484008789 sec. Throughput = 0.0781136988283 Mbit/sec\n",
      "\n",
      "\n",
      "script elapsed time: 0.341809034348 sec\n",
      "script end time: 2017-06-21 15:37:29\n",
      "\n",
      "2017-06-21 15:37:29,506 INFO - uploading file /home/nbuser/batch-shipyard/scripts/shipyard_blobxfer.sh as u'shipyard_blobxfer.sh'\n",
      "2017-06-21 15:37:29,597 INFO - uploading file /home/nbuser/batch-shipyard/resources/nvidia-driver.run as 'nvidia-driver.run'\n",
      "2017-06-21 15:37:31,774 INFO - uploading file /home/nbuser/batch-shipyard/resources/nvidia-docker.deb as 'nvidia-docker.deb'\n",
      "2017-06-21 15:37:32,422 INFO - uploading file /home/nbuser/batch-shipyard/scripts/shipyard_nodeprep.sh as u'shipyard_nodeprep.sh'\n",
      "2017-06-21 15:37:32,475 INFO - Attempting to create pool: gpupool-multi-instance\n",
      "2017-06-21 15:37:32,745 INFO - Created pool: gpupool-multi-instance\n",
      "2017-06-21 15:37:32,746 INFO - waiting for all nodes in pool gpupool-multi-instance to reach one of: frozenset([<ComputeNodeState.idle: 'idle'>, <ComputeNodeState.preempted: 'preempted'>, <ComputeNodeState.start_task_failed: 'startTaskFailed'>, <ComputeNodeState.unusable: 'unusable'>])\n",
      "2017-06-21 15:37:53,965 DEBUG - waiting for 3 dedicated nodes and 0 low priority nodes to reach desired state\n",
      "2017-06-21 15:38:25,142 DEBUG - waiting for 3 dedicated nodes and 0 low priority nodes to reach desired state\n",
      "2017-06-21 15:38:56,292 DEBUG - waiting for 3 dedicated nodes and 0 low priority nodes to reach desired state\n",
      "2017-06-21 15:39:27,501 DEBUG - waiting for 3 dedicated nodes and 0 low priority nodes to reach desired state\n",
      "2017-06-21 15:39:58,757 DEBUG - waiting for 3 dedicated nodes and 0 low priority nodes to reach desired state\n",
      "2017-06-21 15:39:58,757 DEBUG - tvm-1392786932_1-20170621t153952z: ComputeNodeState.starting\n",
      "2017-06-21 15:39:58,757 DEBUG - tvm-1392786932_2-20170621t153952z: ComputeNodeState.starting\n",
      "2017-06-21 15:39:58,757 DEBUG - tvm-1392786932_3-20170621t153952z: ComputeNodeState.starting\n",
      "2017-06-21 15:40:30,220 DEBUG - waiting for 3 dedicated nodes and 0 low priority nodes to reach desired state\n",
      "2017-06-21 15:40:30,220 DEBUG - tvm-1392786932_1-20170621t153952z: ComputeNodeState.starting\n",
      "2017-06-21 15:40:30,220 DEBUG - tvm-1392786932_2-20170621t153952z: ComputeNodeState.starting\n",
      "2017-06-21 15:40:30,220 DEBUG - tvm-1392786932_3-20170621t153952z: ComputeNodeState.starting\n",
      "2017-06-21 15:41:01,534 DEBUG - waiting for 3 dedicated nodes and 0 low priority nodes to reach desired state\n",
      "2017-06-21 15:41:01,534 DEBUG - tvm-1392786932_1-20170621t153952z: ComputeNodeState.waiting_for_start_task\n",
      "2017-06-21 15:41:01,534 DEBUG - tvm-1392786932_2-20170621t153952z: ComputeNodeState.waiting_for_start_task\n",
      "2017-06-21 15:41:01,534 DEBUG - tvm-1392786932_3-20170621t153952z: ComputeNodeState.starting\n",
      "2017-06-21 15:41:32,754 DEBUG - waiting for 3 dedicated nodes and 0 low priority nodes to reach desired state\n",
      "2017-06-21 15:41:32,755 DEBUG - tvm-1392786932_1-20170621t153952z: ComputeNodeState.waiting_for_start_task\n",
      "2017-06-21 15:41:32,755 DEBUG - tvm-1392786932_2-20170621t153952z: ComputeNodeState.waiting_for_start_task\n",
      "2017-06-21 15:41:32,755 DEBUG - tvm-1392786932_3-20170621t153952z: ComputeNodeState.starting\n",
      "2017-06-21 15:42:04,158 DEBUG - waiting for 3 dedicated nodes and 0 low priority nodes to reach desired state\n",
      "2017-06-21 15:42:04,158 DEBUG - tvm-1392786932_1-20170621t153952z: ComputeNodeState.waiting_for_start_task\n",
      "2017-06-21 15:42:04,158 DEBUG - tvm-1392786932_2-20170621t153952z: ComputeNodeState.waiting_for_start_task\n",
      "2017-06-21 15:42:04,158 DEBUG - tvm-1392786932_3-20170621t153952z: ComputeNodeState.starting\n",
      "2017-06-21 15:42:35,717 DEBUG - waiting for 3 dedicated nodes and 0 low priority nodes to reach desired state\n",
      "2017-06-21 15:42:35,717 DEBUG - tvm-1392786932_1-20170621t153952z: ComputeNodeState.waiting_for_start_task\n",
      "2017-06-21 15:42:35,717 DEBUG - tvm-1392786932_2-20170621t153952z: ComputeNodeState.waiting_for_start_task\n",
      "2017-06-21 15:42:35,717 DEBUG - tvm-1392786932_3-20170621t153952z: ComputeNodeState.waiting_for_start_task\n",
      "2017-06-21 15:43:07,066 DEBUG - waiting for 3 dedicated nodes and 0 low priority nodes to reach desired state\n",
      "2017-06-21 15:43:07,066 DEBUG - tvm-1392786932_1-20170621t153952z: ComputeNodeState.waiting_for_start_task\n",
      "2017-06-21 15:43:07,066 DEBUG - tvm-1392786932_2-20170621t153952z: ComputeNodeState.waiting_for_start_task\n",
      "2017-06-21 15:43:07,067 DEBUG - tvm-1392786932_3-20170621t153952z: ComputeNodeState.waiting_for_start_task\n",
      "2017-06-21 15:43:38,421 DEBUG - waiting for 3 dedicated nodes and 0 low priority nodes to reach desired state\n",
      "2017-06-21 15:43:38,421 DEBUG - tvm-1392786932_1-20170621t153952z: ComputeNodeState.waiting_for_start_task\n",
      "2017-06-21 15:43:38,421 DEBUG - tvm-1392786932_2-20170621t153952z: ComputeNodeState.waiting_for_start_task\n",
      "2017-06-21 15:43:38,422 DEBUG - tvm-1392786932_3-20170621t153952z: ComputeNodeState.waiting_for_start_task\n",
      "2017-06-21 15:44:09,813 DEBUG - waiting for 3 dedicated nodes and 0 low priority nodes to reach desired state\n",
      "2017-06-21 15:44:09,813 DEBUG - tvm-1392786932_1-20170621t153952z: ComputeNodeState.waiting_for_start_task\n",
      "2017-06-21 15:44:09,813 DEBUG - tvm-1392786932_2-20170621t153952z: ComputeNodeState.waiting_for_start_task\n",
      "2017-06-21 15:44:09,813 DEBUG - tvm-1392786932_3-20170621t153952z: ComputeNodeState.waiting_for_start_task\n",
      "2017-06-21 15:45:12,562 DEBUG - waiting for 3 dedicated nodes and 0 low priority nodes to reach desired state\n",
      "2017-06-21 15:45:12,562 DEBUG - tvm-1392786932_1-20170621t153952z: ComputeNodeState.waiting_for_start_task\n",
      "2017-06-21 15:45:12,562 DEBUG - tvm-1392786932_2-20170621t153952z: ComputeNodeState.waiting_for_start_task\n",
      "2017-06-21 15:45:12,562 DEBUG - tvm-1392786932_3-20170621t153952z: ComputeNodeState.waiting_for_start_task\n",
      "2017-06-21 15:45:44,014 DEBUG - waiting for 3 dedicated nodes and 0 low priority nodes to reach desired state\n",
      "2017-06-21 15:45:44,014 DEBUG - tvm-1392786932_1-20170621t153952z: ComputeNodeState.waiting_for_start_task\n",
      "2017-06-21 15:45:44,014 DEBUG - tvm-1392786932_2-20170621t153952z: ComputeNodeState.waiting_for_start_task\n",
      "2017-06-21 15:45:44,015 DEBUG - tvm-1392786932_3-20170621t153952z: ComputeNodeState.waiting_for_start_task\n",
      "2017-06-21 15:46:15,596 DEBUG - waiting for 3 dedicated nodes and 0 low priority nodes to reach desired state\n",
      "2017-06-21 15:46:15,597 DEBUG - tvm-1392786932_1-20170621t153952z: ComputeNodeState.waiting_for_start_task\n",
      "2017-06-21 15:46:15,597 DEBUG - tvm-1392786932_2-20170621t153952z: ComputeNodeState.waiting_for_start_task\n",
      "2017-06-21 15:46:15,597 DEBUG - tvm-1392786932_3-20170621t153952z: ComputeNodeState.waiting_for_start_task\n",
      "2017-06-21 15:46:47,405 DEBUG - waiting for 3 dedicated nodes and 0 low priority nodes to reach desired state\n",
      "2017-06-21 15:46:47,405 DEBUG - tvm-1392786932_1-20170621t153952z: ComputeNodeState.waiting_for_start_task\n",
      "2017-06-21 15:46:47,405 DEBUG - tvm-1392786932_2-20170621t153952z: ComputeNodeState.waiting_for_start_task\n",
      "2017-06-21 15:46:47,405 DEBUG - tvm-1392786932_3-20170621t153952z: ComputeNodeState.waiting_for_start_task\n",
      "2017-06-21 15:47:18,853 DEBUG - waiting for 3 dedicated nodes and 0 low priority nodes to reach desired state\n",
      "2017-06-21 15:47:18,853 DEBUG - tvm-1392786932_1-20170621t153952z: ComputeNodeState.waiting_for_start_task\n",
      "2017-06-21 15:47:18,853 DEBUG - tvm-1392786932_2-20170621t153952z: ComputeNodeState.waiting_for_start_task\n",
      "2017-06-21 15:47:18,853 DEBUG - tvm-1392786932_3-20170621t153952z: ComputeNodeState.waiting_for_start_task\n",
      "2017-06-21 15:47:50,290 DEBUG - waiting for 3 dedicated nodes and 0 low priority nodes to reach desired state\n",
      "2017-06-21 15:47:50,290 DEBUG - tvm-1392786932_1-20170621t153952z: ComputeNodeState.waiting_for_start_task\n",
      "2017-06-21 15:47:50,290 DEBUG - tvm-1392786932_2-20170621t153952z: ComputeNodeState.waiting_for_start_task\n",
      "2017-06-21 15:47:50,291 DEBUG - tvm-1392786932_3-20170621t153952z: ComputeNodeState.waiting_for_start_task\n",
      "2017-06-21 15:48:21,710 DEBUG - waiting for 3 dedicated nodes and 0 low priority nodes to reach desired state\n",
      "2017-06-21 15:48:21,710 DEBUG - tvm-1392786932_1-20170621t153952z: ComputeNodeState.waiting_for_start_task\n",
      "2017-06-21 15:48:21,710 DEBUG - tvm-1392786932_2-20170621t153952z: ComputeNodeState.waiting_for_start_task\n",
      "2017-06-21 15:48:21,711 DEBUG - tvm-1392786932_3-20170621t153952z: ComputeNodeState.waiting_for_start_task\n",
      "2017-06-21 15:48:53,120 DEBUG - waiting for 3 dedicated nodes and 0 low priority nodes to reach desired state\n",
      "2017-06-21 15:48:53,120 DEBUG - tvm-1392786932_1-20170621t153952z: ComputeNodeState.idle\n",
      "2017-06-21 15:48:53,120 DEBUG - tvm-1392786932_2-20170621t153952z: ComputeNodeState.idle\n",
      "2017-06-21 15:48:53,120 DEBUG - tvm-1392786932_3-20170621t153952z: ComputeNodeState.waiting_for_start_task\n",
      "2017-06-21 15:49:14,226 DEBUG - listing nodes for pool gpupool-multi-instance\n",
      "2017-06-21 15:49:14,226 INFO - node_id=tvm-1392786932_1-20170621t153952z [state=ComputeNodeState.idle start_task_exit_code=0 scheduling_state=SchedulingState.enabled ip_address=10.0.0.4 vm_size=standard_nc6 dedicated=True total_tasks_run=0 running_tasks_count=0 total_tasks_succeeded=0]\n",
      "2017-06-21 15:49:14,226 INFO - node_id=tvm-1392786932_2-20170621t153952z [state=ComputeNodeState.idle start_task_exit_code=0 scheduling_state=SchedulingState.enabled ip_address=10.0.0.5 vm_size=standard_nc6 dedicated=True total_tasks_run=0 running_tasks_count=0 total_tasks_succeeded=0]\n",
      "2017-06-21 15:49:14,226 INFO - node_id=tvm-1392786932_3-20170621t153952z [state=ComputeNodeState.idle start_task_exit_code=0 scheduling_state=SchedulingState.enabled ip_address=10.0.0.6 vm_size=standard_nc6 dedicated=True total_tasks_run=0 running_tasks_count=0 total_tasks_succeeded=0]\n",
      "2017-06-21 15:49:14,227 INFO - generating ssh key pair to path: .\n",
      "Generating public/private rsa key pair.\n",
      "Your identification has been saved in id_rsa_shipyard.\n",
      "Your public key has been saved in id_rsa_shipyard.pub.\n",
      "The key fingerprint is:\n",
      "SHA256:Ks3lR+N2PnXTrZjIbja+GtVdj1zk5xifi1t5Ie2AUu4 nbuser@nbserver\n",
      "The key's randomart image is:\n",
      "+---[RSA 2048]----+\n",
      "|                .|\n",
      "|               o |\n",
      "|           .  . =|\n",
      "|          o..o.O+|\n",
      "|        S.+o.o=+*|\n",
      "|     o + +o.  *.B|\n",
      "|    . + o.+E.= Bo|\n",
      "|     .   +*o+ + .|\n",
      "|        .*+o.o   |\n",
      "+----[SHA256]-----+\n",
      "2017-06-21 15:49:14,501 INFO - adding user docker to node tvm-1392786932_1-20170621t153952z in pool gpupool-multi-instance, expiry=2017-07-21 15:49:14.501540\n",
      "No handlers could be found for logger \"msrest.serialization\"\n",
      "2017-06-21 15:49:15,252 INFO - adding user docker to node tvm-1392786932_2-20170621t153952z in pool gpupool-multi-instance, expiry=2017-07-21 15:49:15.252075\n",
      "2017-06-21 15:49:21,127 INFO - adding user docker to node tvm-1392786932_3-20170621t153952z in pool gpupool-multi-instance, expiry=2017-07-21 15:49:21.127471\n",
      "2017-06-21 15:49:21,930 INFO - node tvm-1392786932_1-20170621t153952z: ip 104.41.155.139 port 50000\n",
      "2017-06-21 15:49:22,129 INFO - node tvm-1392786932_2-20170621t153952z: ip 104.41.155.139 port 50001\n",
      "2017-06-21 15:49:22,331 INFO - node tvm-1392786932_3-20170621t153952z: ip 104.41.155.139 port 50002\n",
      "2017-06-21 15:49:22,530 WARNING - skipping data ingress from dist_upload for pool as ingress to Azure Blob/File Storage not specified\n",
      "2017-06-21 15:49:22,530 INFO - Azure Blob/File Storage transfer completed\n"
     ]
    }
   ],
   "source": [
    "shipyard pool add -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure that all compute nodes are `idle` and ready to accept tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-21 10:47:05,726 WARNING - DEPRECATION WARNING: pool_specification:publisher/offer/sku is set instead of a vm_configuration complex property. This configuration will not be supported in future releases. Please update your configuration to include a vm_configuration property.\n",
      "2017-06-21 10:47:05,737 DEBUG - listing nodes for pool gpupool-multi-instance\n",
      "2017-06-21 10:47:06,061 INFO - node_id=tvm-4283973576_1-20170621t103038z [state=ComputeNodeState.idle start_task_exit_code=0 scheduling_state=SchedulingState.enabled ip_address=10.0.0.5 vm_size=standard_nc6 dedicated=True total_tasks_run=0 running_tasks_count=0 total_tasks_succeeded=0]\n",
      "2017-06-21 10:47:06,061 INFO - node_id=tvm-4283973576_2-20170621t103038z [state=ComputeNodeState.idle start_task_exit_code=0 scheduling_state=SchedulingState.enabled ip_address=10.0.0.4 vm_size=standard_nc6 dedicated=True total_tasks_run=0 running_tasks_count=0 total_tasks_succeeded=0]\n",
      "2017-06-21 10:47:06,061 INFO - node_id=tvm-4283973576_3-20170621t103038z [state=ComputeNodeState.idle start_task_exit_code=0 scheduling_state=SchedulingState.enabled ip_address=10.0.0.6 vm_size=standard_nc6 dedicated=True total_tasks_run=0 running_tasks_count=0 total_tasks_succeeded=0]\n"
     ]
    }
   ],
   "source": [
    "shipyard pool listnodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure MPI Job and Submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MPI jobs in Batch require execution as a multi-instance task. Essentially this allows multiple compute nodes to be used for a single task.\n",
    "\n",
    "A few things to note in this jobs configuration:\n",
    "- The `COMMAND` executes the `run_cifar10.sh` script that was uploaded earlier as part of the node preparation task.\n",
    "- `auto_complete` is being set to `True` which forces the job to move from `active` to `completed` state once all tasks complete. Note that once a job has moved to `completed` state, no new tasks can be added to it.\n",
    "- `multi_instance` property is populated which enables multiple nodes, e.g., `num_instances` to participate in the execution of this task. The `coordination_command` is the command that is run on all nodes prior to the `command`. Here, we are simply executing the Docker image to run the SSH server for the MPI daemon (e.g., orted, hydra, etc.) to initialize all of the nodes prior to running the application command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "JOB_ID = 'cntk-mpi-job'\n",
    "\n",
    "# reduce the nubmer of epochs to 20 for purposes of this notebook\n",
    "COMMAND = '$AZ_BATCH_NODE_SHARED_DIR/run_cifar10.sh -e 20'\n",
    "jobs = {\n",
    "    \"job_specifications\": [\n",
    "        {\n",
    "            \"id\": JOB_ID,\n",
    "            \"auto_complete\": True,\n",
    "            \"tasks\": [\n",
    "                {\n",
    "                    \"image\": IMAGE_NAME,\n",
    "                    \"remove_container_after_exit\": True,\n",
    "                    \"command\": COMMAND,\n",
    "                    \"gpu\": True,\n",
    "                    \"multi_instance\": {\n",
    "                        \"num_instances\": \"pool_current_dedicated\",\n",
    "                        \"coordination_command\": \"/usr/sbin/sshd -D -p 23\",\n",
    "                        \"resource_files\": [\n",
    "                            {\n",
    "                                \"file_path\": \"resnet_models.py\",\n",
    "                                \"blob_source\": \"https://batchshipyardexamples.blob.core.windows.net/code/resnet_models.py\",\n",
    "                                \"file_mode\":'0777'\n",
    "                            },\n",
    "                            {\n",
    "                                \"file_path\": \"TrainResNet_CIFAR10.py\",\n",
    "                                \"blob_source\": \"https://batchshipyardexamples.blob.core.windows.net/code/TrainResNet_CIFAR10.py\",\n",
    "                                \"file_mode\":'0777'\n",
    "                            }\n",
    "                        ],\n",
    "                    },\n",
    "                }\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"job_specifications\": [\n",
      "        {\n",
      "            \"auto_complete\": true, \n",
      "            \"id\": \"cntk-mpi-job\", \n",
      "            \"tasks\": [\n",
      "                {\n",
      "                    \"command\": \"$AZ_BATCH_NODE_SHARED_DIR/run_cifar10.sh -e 20\", \n",
      "                    \"gpu\": true, \n",
      "                    \"image\": \"alfpark/cntk:2.0.rc2-gpu-1bit-sgd-python3.5-cuda8.0-cudnn5.1\", \n",
      "                    \"multi_instance\": {\n",
      "                        \"coordination_command\": \"/usr/sbin/sshd -D -p 23\", \n",
      "                        \"num_instances\": \"pool_current_dedicated\", \n",
      "                        \"resource_files\": [\n",
      "                            {\n",
      "                                \"blob_source\": \"https://batchshipyardexamples.blob.core.windows.net/code/resnet_models.py\", \n",
      "                                \"file_mode\": \"0777\", \n",
      "                                \"file_path\": \"resnet_models.py\"\n",
      "                            }, \n",
      "                            {\n",
      "                                \"blob_source\": \"https://batchshipyardexamples.blob.core.windows.net/code/TrainResNet_CIFAR10.py\", \n",
      "                                \"file_mode\": \"0777\", \n",
      "                                \"file_path\": \"TrainResNet_CIFAR10.py\"\n",
      "                            }\n",
      "                        ]\n",
      "                    }, \n",
      "                    \"remove_container_after_exit\": true\n",
      "                }\n",
      "            ]\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "write_json_to_file(jobs, os.path.join('config', 'jobs.json'))\n",
    "print(json.dumps(jobs, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit the job and tail `stdout.txt`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-21 10:47:15,697 WARNING - DEPRECATION WARNING: pool_specification:publisher/offer/sku is set instead of a vm_configuration complex property. This configuration will not be supported in future releases. Please update your configuration to include a vm_configuration property.\n",
      "2017-06-21 10:47:16,151 INFO - Adding job cntk-mpi-job to pool gpupool-multi-instance\n",
      "2017-06-21 10:47:16,743 INFO - uploading file /tmp/tmpsJeQ7e as u'shipyardtaskrf-cntk-mpi-job/dockertask-00000.shipyard.envlist'\n",
      "2017-06-21 10:47:16,796 DEBUG - submitting 1 tasks (0 -> 0) to job cntk-mpi-job\n",
      "2017-06-21 10:47:17,058 INFO - submitted all 1 tasks to job cntk-mpi-job\n",
      "2017-06-21 10:47:17,298 DEBUG - attempting to stream file stdout.txt from job=cntk-mpi-job task=dockertask-00000\n",
      "d8deefa2b42de464e3cf5857358f334ef51c97ffc26304487ffd9c1872f62d67\n",
      "num gpus: 1\n",
      "num nodes: 3\n",
      "hosts: 10.0.0.4 10.0.0.5 10.0.0.6\n",
      "num mpi processes: 3\n",
      "\n",
      "************************************************************\n",
      "CNTK is activated.\n",
      "\n",
      "Please checkout tutorials and examples here:\n",
      "  /cntk/Tutorials\n",
      "  /cntk/Examples\n",
      "\n",
      "To deactivate the environment run\n",
      "\n",
      "  source /root/anaconda3/bin/deactivate\n",
      "\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "CNTK is activated.\n",
      "\n",
      "Please checkout tutorials and examples here:\n",
      "  /cntk/Tutorials\n",
      "  /cntk/Examples\n",
      "\n",
      "To deactivate the environment run\n",
      "\n",
      "  source /root/anaconda3/bin/deactivate\n",
      "\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "CNTK is activated.\n",
      "\n",
      "Please checkout tutorials and examples here:\n",
      "  /cntk/Tutorials\n",
      "  /cntk/Examples\n",
      "\n",
      "To deactivate the environment run\n",
      "\n",
      "  source /root/anaconda3/bin/deactivate\n",
      "\n",
      "************************************************************\n",
      "Finished Epoch[1 of 20]: [Training] loss = 1.732289 * 50048, metric = 64.29% * 50048 23.690s (2112.6 samples/s);\n",
      "Finished Epoch[1 of 20]: [Training] loss = 1.732289 * 50048, metric = 64.29% * 50048 22.822s (2193.0 samples/s);\n",
      "Finished Epoch[1 of 20]: [Training] loss = 1.732289 * 50048, metric = 64.29% * 50048 23.217s (2155.7 samples/s);\n",
      "Finished Epoch[2 of 20]: [Training] loss = 1.238567 * 50048, metric = 44.96% * 50048 20.696s (2418.2 samples/s);\n",
      "Finished Epoch[2 of 20]: [Training] loss = 1.238567 * 50048, metric = 44.96% * 50048 20.696s (2418.2 samples/s);\n",
      "Finished Epoch[2 of 20]: [Training] loss = 1.238567 * 50048, metric = 44.96% * 50048 20.695s (2418.4 samples/s);\n",
      "Finished Epoch[3 of 20]: [Training] loss = 1.005196 * 49920, metric = 36.08% * 49920 20.849s (2394.4 samples/s);\n",
      "Finished Epoch[3 of 20]: [Training] loss = 1.005196 * 49920, metric = 36.08% * 49920 20.824s (2397.2 samples/s);\n",
      "Finished Epoch[3 of 20]: [Training] loss = 1.005196 * 49920, metric = 36.08% * 49920 20.849s (2394.4 samples/s);\n",
      "Finished Epoch[4 of 20]: [Training] loss = 0.892360 * 50048, metric = 31.33% * 50048 20.652s (2423.4 samples/s);\n",
      "Finished Epoch[4 of 20]: [Training] loss = 0.892360 * 50048, metric = 31.33% * 50048 20.652s (2423.4 samples/s);\n",
      "Finished Epoch[4 of 20]: [Training] loss = 0.892360 * 50048, metric = 31.33% * 50048 20.625s (2426.6 samples/s);\n",
      "Finished Epoch[5 of 20]: [Training] loss = 0.775418 * 50048, metric = 27.12% * 50048 20.766s (2410.1 samples/s);\n",
      "Finished Epoch[5 of 20]: [Training] loss = 0.775418 * 50048, metric = 27.12% * 50048 20.767s (2410.0 samples/s);\n",
      "Finished Epoch[5 of 20]: [Training] loss = 0.775418 * 50048, metric = 27.12% * 50048 20.763s (2410.4 samples/s);\n",
      "Finished Epoch[6 of 20]: [Training] loss = 0.718712 * 49920, metric = 24.81% * 49920 20.965s (2381.1 samples/s);\n",
      "Finished Epoch[6 of 20]: [Training] loss = 0.718712 * 49920, metric = 24.81% * 49920 20.967s (2380.9 samples/s);\n",
      "Finished Epoch[6 of 20]: [Training] loss = 0.718712 * 49920, metric = 24.81% * 49920 20.966s (2381.0 samples/s);\n",
      "Finished Epoch[7 of 20]: [Training] loss = 0.669271 * 50048, metric = 23.18% * 50048 20.848s (2400.6 samples/s);\n",
      "Finished Epoch[7 of 20]: [Training] loss = 0.669271 * 50048, metric = 23.18% * 50048 20.873s (2397.7 samples/s);\n",
      "Finished Epoch[7 of 20]: [Training] loss = 0.669271 * 50048, metric = 23.18% * 50048 20.875s (2397.5 samples/s);\n",
      "Finished Epoch[8 of 20]: [Training] loss = 0.625383 * 49920, metric = 21.50% * 49920 20.809s (2399.0 samples/s);\n",
      "Finished Epoch[8 of 20]: [Training] loss = 0.625383 * 49920, metric = 21.50% * 49920 20.808s (2399.1 samples/s);\n",
      "Finished Epoch[8 of 20]: [Training] loss = 0.625383 * 49920, metric = 21.50% * 49920 20.809s (2399.0 samples/s);\n",
      "Finished Epoch[9 of 20]: [Training] loss = 0.587982 * 50048, metric = 20.38% * 50048 21.050s (2377.6 samples/s);\n",
      "Finished Epoch[9 of 20]: [Training] loss = 0.587982 * 50048, metric = 20.38% * 50048 21.029s (2380.0 samples/s);\n",
      "Finished Epoch[9 of 20]: [Training] loss = 0.587982 * 50048, metric = 20.38% * 50048 21.055s (2377.0 samples/s);\n",
      "Finished Epoch[10 of 20]: [Training] loss = 0.563451 * 50048, metric = 19.45% * 50048 20.993s (2384.0 samples/s);\n",
      "Finished Epoch[10 of 20]: [Training] loss = 0.563451 * 50048, metric = 19.45% * 50048 21.022s (2380.7 samples/s);\n",
      "Finished Epoch[10 of 20]: [Training] loss = 0.563451 * 50048, metric = 19.45% * 50048 21.022s (2380.7 samples/s);\n",
      "Finished Epoch[11 of 20]: [Training] loss = 0.545140 * 49920, metric = 18.81% * 49920 20.874s (2391.5 samples/s);\n",
      "Finished Epoch[11 of 20]: [Training] loss = 0.545140 * 49920, metric = 18.81% * 49920 20.873s (2391.6 samples/s);\n",
      "Finished Epoch[11 of 20]: [Training] loss = 0.545140 * 49920, metric = 18.81% * 49920 20.874s (2391.5 samples/s);\n",
      "Finished Epoch[12 of 20]: [Training] loss = 0.517026 * 50048, metric = 17.98% * 50048 21.081s (2374.1 samples/s);\n",
      "Finished Epoch[12 of 20]: [Training] loss = 0.517026 * 50048, metric = 17.98% * 50048 21.107s (2371.2 samples/s);\n",
      "Finished Epoch[12 of 20]: [Training] loss = 0.517026 * 50048, metric = 17.98% * 50048 21.109s (2370.9 samples/s);\n",
      "Finished Epoch[13 of 20]: [Training] loss = 0.494963 * 50048, metric = 17.16% * 50048 21.030s (2379.8 samples/s);\n",
      "Finished Epoch[13 of 20]: [Training] loss = 0.494963 * 50048, metric = 17.16% * 50048 21.033s (2379.5 samples/s);\n",
      "Finished Epoch[13 of 20]: [Training] loss = 0.494963 * 50048, metric = 17.16% * 50048 21.030s (2379.8 samples/s);\n",
      "Finished Epoch[14 of 20]: [Training] loss = 0.497472 * 49920, metric = 17.21% * 49920 21.128s (2362.7 samples/s);\n",
      "Finished Epoch[14 of 20]: [Training] loss = 0.497472 * 49920, metric = 17.21% * 49920 21.102s (2365.7 samples/s);\n",
      "Finished Epoch[14 of 20]: [Training] loss = 0.497472 * 49920, metric = 17.21% * 49920 21.128s (2362.7 samples/s);\n",
      "Finished Epoch[15 of 20]: [Training] loss = 0.469450 * 50048, metric = 16.21% * 50048 21.017s (2381.3 samples/s);\n",
      "Finished Epoch[15 of 20]: [Training] loss = 0.469450 * 50048, metric = 16.21% * 50048 21.018s (2381.2 samples/s);\n",
      "Finished Epoch[15 of 20]: [Training] loss = 0.469450 * 50048, metric = 16.21% * 50048 20.987s (2384.7 samples/s);\n",
      "Finished Epoch[16 of 20]: [Training] loss = 0.457611 * 49920, metric = 15.77% * 49920 21.012s (2375.8 samples/s);\n",
      "Finished Epoch[16 of 20]: [Training] loss = 0.457611 * 49920, metric = 15.77% * 49920 21.013s (2375.7 samples/s);\n",
      "Finished Epoch[16 of 20]: [Training] loss = 0.457611 * 49920, metric = 15.77% * 49920 21.013s (2375.7 samples/s);\n",
      "Finished Epoch[17 of 20]: [Training] loss = 0.451254 * 50048, metric = 15.58% * 50048 21.109s (2370.9 samples/s);\n",
      "Finished Epoch[17 of 20]: [Training] loss = 0.451254 * 50048, metric = 15.58% * 50048 21.137s (2367.8 samples/s);\n",
      "Finished Epoch[17 of 20]: [Training] loss = 0.451254 * 50048, metric = 15.58% * 50048 21.134s (2368.1 samples/s);\n",
      "Finished Epoch[18 of 20]: [Training] loss = 0.437370 * 50048, metric = 15.03% * 50048 21.243s (2356.0 samples/s);\n",
      "Finished Epoch[18 of 20]: [Training] loss = 0.437370 * 50048, metric = 15.03% * 50048 21.217s (2358.9 samples/s);\n",
      "Finished Epoch[18 of 20]: [Training] loss = 0.437370 * 50048, metric = 15.03% * 50048 21.244s (2355.9 samples/s);\n",
      "Finished Epoch[19 of 20]: [Training] loss = 0.427794 * 49920, metric = 14.95% * 49920 20.843s (2395.0 samples/s);\n",
      "Finished Epoch[19 of 20]: [Training] loss = 0.427794 * 49920, metric = 14.95% * 49920 20.843s (2395.0 samples/s);\n",
      "Finished Epoch[19 of 20]: [Training] loss = 0.427794 * 49920, metric = 14.95% * 49920 20.842s (2395.2 samples/s);\n",
      "Finished Epoch[20 of 20]: [Training] loss = 0.434546 * 49984, metric = 15.07% * 49984 27.583s (1812.1 samples/s);\n",
      "Finished Epoch[20 of 20]: [Training] loss = 0.434546 * 49984, metric = 15.07% * 49984 27.554s (1814.0 samples/s);\n",
      "Finished Epoch[20 of 20]: [Training] loss = 0.434546 * 49984, metric = 15.07% * 49984 27.563s (1813.4 samples/s);\n",
      "Finished Evaluation [1]: Minibatch[1-667]: metric = 33.93% * 10000;\n",
      "Finished Evaluation [1]: Minibatch[1-667]: metric = 33.93% * 10000;\n",
      "Finished Evaluation [1]: Minibatch[1-667]: metric = 33.93% * 10000;\n",
      "\n"
     ]
    }
   ],
   "source": [
    "shipyard jobs add --tail stdout.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the command below we can check the status of our jobs. Once all jobs have an exit code we can continue. You can also view the **heatmap** of this pool on [Azure Portal](https://portal.azure.com) to monitor the progress of this job on the compute nodes under your Batch account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Multi-instance Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleting multi-instance jobs running as Docker containers requires a little more care. We will need to first ensure that the job has entered `completed` state. In the above `jobs` configuration, we set `auto_complete` to `True` enabling the Batch service to automatically complete the job when all tasks finish. This also allows automatic cleanup of the running Docker containers used for executing the MPI job.\n",
    "\n",
    "Special logic is required to cleanup MPI jobs since the `coordination_command` that runs actually detaches an SSH server. The job auto completion logic Batch Shipyard injects ensures that these containers are killed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-21 10:56:30,592 WARNING - DEPRECATION WARNING: pool_specification:publisher/offer/sku is set instead of a vm_configuration complex property. This configuration will not be supported in future releases. Please update your configuration to include a vm_configuration property.\n",
      "2017-06-21 10:56:31,116 INFO - job_id=cntk-mpi-job task_id=dockertask-00000 [state=TaskState.completed max_retries=0 retention_time=10675199 days, 2:48:05.477581 pool_id=gpupool-multi-instance node_id=tvm-4283973576_2-20170621t103038z start_time=2017-06-21 10:47:18.399507+00:00 end_time=2017-06-21 10:54:33.882116+00:00 duration=0:07:15.482609 exit_code=0]\n"
     ]
    }
   ],
   "source": [
    "shipyard jobs listtasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we are sure that the job is completed, then we issue the standard delete command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-21 15:49:24,206 WARNING - DEPRECATION WARNING: pool_specification:publisher/offer/sku is set instead of a vm_configuration complex property. This configuration will not be supported in future releases. Please update your configuration to include a vm_configuration property.\n",
      "2017-06-21 15:49:24,216 INFO - Deleting job: cntk-ps-as-job\n",
      "2017-06-21 15:49:24,216 DEBUG - disabling job cntk-ps-as-job first due to task termination\n",
      "2017-06-21 15:49:24,645 ERROR - cntk-ps-as-job job does not exist\n"
     ]
    }
   ],
   "source": [
    "shipyard jobs del -y --termtasks --wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-21 15:49:26,317 WARNING - DEPRECATION WARNING: pool_specification:publisher/offer/sku is set instead of a vm_configuration complex property. This configuration will not be supported in future releases. Please update your configuration to include a vm_configuration property.\n",
      "2017-06-21 15:49:26,330 INFO - Deleting pool: gpupool-multi-instance\n",
      "2017-06-21 15:49:26,541 DEBUG - clearing table (pk=batcha3dc41fdba$gpupool-multi-instance): shipyardregistry\n",
      "2017-06-21 15:49:26,886 DEBUG - clearing table (pk=batcha3dc41fdba$gpupool-multi-instance): shipyardgr\n",
      "2017-06-21 15:49:26,990 DEBUG - clearing table (pk=batcha3dc41fdba$gpupool-multi-instance): shipyardperf\n",
      "2017-06-21 15:49:27,046 DEBUG - clearing table (pk=batcha3dc41fdba$gpupool-multi-instance): shipyarddht\n",
      "2017-06-21 15:49:27,098 DEBUG - clearing table (pk=batcha3dc41fdba$gpupool-multi-instance): shipyardimages\n",
      "2017-06-21 15:49:27,204 DEBUG - clearing table (pk=batcha3dc41fdba$gpupool-multi-instance): shipyardtorrentinfo\n",
      "2017-06-21 15:49:27,256 DEBUG - deleting queue: shipyardgr-batcha3dc41fdba-gpupool-multi-instance\n",
      "2017-06-21 15:49:27,483 DEBUG - deleting container: shipyardtor-batcha3dc41fdba-gpupool-multi-instance\n",
      "2017-06-21 15:49:28,064 DEBUG - deleting container: shipyardrf-batcha3dc41fdba-gpupool-multi-instance\n",
      "2017-06-21 15:49:28,110 DEBUG - waiting for pool gpupool-multi-instance to delete\n"
     ]
    }
   ],
   "source": [
    "shipyard pool del -y --wait"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "You can proceed to the [Notebook: Clean Up](05_Clean_Up.ipynb) if you are done for now, or proceed to one of the following additional Notebooks:\n",
    "* [Notebook: Automatic Model Selection](06_Advanced_Auto_Model_Selection.ipynb)\n",
    "* [Notebook: Tensorboard Visualization](07_Advanced_Tensorboard.ipynb) - note this requires running this notebook on your own machine\n",
    "* [Notebook: Keras with TensorFlow](09_Keras_Single_GPU_Training_With_Tensorflow.ipynb)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}